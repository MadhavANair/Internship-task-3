{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d70ecc47-eb82-449e-a549-124bcf981da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ STARTING CLEANUP: Conda & Pip Caches\n",
      "============================================================\n",
      "\n",
      "üì¶ Running Conda Cleanup...\n",
      "‚úÖ Conda clean successful.\n",
      "There are no unused tarball(s) to remove.\n",
      "There are no index cache(s) to remove.\n",
      "There are no unused package(s) to remove.\n",
      "There are no tempfile(s) to remove.\n",
      "There are no logfile(s) to remove....\n",
      "\n",
      "üêç Running Pip Cleanup...\n",
      "‚úÖ Pip cache purge successful: Files removed: 0 (0 bytes)\n",
      "\n",
      "üìì Cleaning Jupyter Checkpoints (.ipynb_checkpoints)...\n",
      "‚úÖ Removed 2 checkpoint folders (~61.39 MB)\n",
      "\n",
      "============================================================\n",
      "‚ú® CLEANUP COMPLETE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_conda_pip_cache():\n",
    "    print(\"üßπ STARTING CLEANUP: Conda & Pip Caches\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. CONDA CLEANUP\n",
    "    # Using 'conda clean --all' is the standard way.\n",
    "    # We use subprocess to run the command line tool.\n",
    "    print(\"\\nüì¶ Running Conda Cleanup...\")\n",
    "    try:\n",
    "        # -a: all (index cache, lock files, unused cache packages, tarballs)\n",
    "        # -y: yes (do not ask for confirmation)\n",
    "        result = subprocess.run([\"conda\", \"clean\", \"-a\", \"-y\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Conda clean successful.\")\n",
    "            # Print a summary of what was removed (first few lines usually contain the size)\n",
    "            print(\"\\n\".join(result.stdout.split('\\n')[:5]) + \"...\") \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Conda clean warning:\\n{result.stderr}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running conda clean: {e}\")\n",
    "\n",
    "    # 2. PIP CLEANUP\n",
    "    # Pip cache is usually at ~/.cache/pip or %LocalAppData%\\pip\\Cache\n",
    "    print(\"\\nüêç Running Pip Cleanup...\")\n",
    "    \n",
    "    # Attempt to use pip's internal cache purge command\n",
    "    try:\n",
    "        result = subprocess.run([\"pip\", \"cache\", \"purge\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Pip cache purge successful: {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Pip purge warning:\\n{result.stderr}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ÑπÔ∏è 'pip' command not found in path. Trying module execution...\")\n",
    "        # Fallback: python -m pip cache purge\n",
    "        subprocess.run([\"python\", \"-m\", \"pip\", \"cache\", \"purge\"])\n",
    "\n",
    "    # 3. JUPYTER CHECKPOINTS CLEANUP (Optional but recommended)\n",
    "    print(\"\\nüìì Cleaning Jupyter Checkpoints (.ipynb_checkpoints)...\")\n",
    "    cwd = Path(os.getcwd())\n",
    "    count = 0\n",
    "    size_cleared = 0\n",
    "    \n",
    "    for p in cwd.rglob(\".ipynb_checkpoints\"):\n",
    "        if p.is_dir():\n",
    "            try:\n",
    "                # Calculate size before deleting\n",
    "                current_size = sum(f.stat().st_size for f in p.rglob('*') if f.is_file())\n",
    "                shutil.rmtree(p)\n",
    "                count += 1\n",
    "                size_cleared += current_size\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Could not remove {p}: {e}\")\n",
    "    \n",
    "    mb_cleared = size_cleared / (1024 * 1024)\n",
    "    print(f\"‚úÖ Removed {count} checkpoint folders (~{mb_cleared:.2f} MB)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ú® CLEANUP COMPLETE\")\n",
    "\n",
    "# Run the function\n",
    "clean_conda_pip_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "774de55b-0e71-4cad-8887-39787746f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Worker module loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b713e8a72b54bc3a6b97fb50e5948ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>üï∫ Stale Page Detection: Party Bouncer (Multiprocessing)</h2>'), Text(value='', ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import concurrent.futures\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import csv\n",
    "import sys\n",
    "import zlib\n",
    "from io import BytesIO\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. AUTO-CREATE WORKER FILE (Fixes ModuleNotFoundError)\n",
    "# ============================================================================\n",
    "worker_code = \"\"\"\n",
    "import requests\n",
    "import zlib\n",
    "from lxml import etree\n",
    "from io import BytesIO\n",
    "\n",
    "def parse_gz_sitemap_worker(gz_url):\n",
    "    urls_data = []\n",
    "    try:\n",
    "        with requests.get(gz_url, stream=True, timeout=60) as response:\n",
    "            if response.status_code != 200:\n",
    "                return []\n",
    "            d = zlib.decompressobj(16 + zlib.MAX_WBITS)\n",
    "            def stream_generator():\n",
    "                for chunk in response.iter_content(chunk_size=65536):\n",
    "                    if chunk:\n",
    "                        yield d.decompress(chunk)\n",
    "            context = etree.iterparse(\n",
    "                BytesIO(b\"\".join(stream_generator())), \n",
    "                events=('end',), \n",
    "                tag='{http://www.sitemaps.org/schemas/sitemap/0.9}url'\n",
    "            )\n",
    "            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9', 'image': 'http://www.google.com/schemas/sitemap-image/1.1'}\n",
    "            for event, elem in context:\n",
    "                try:\n",
    "                    loc = elem.find('sm:loc', ns)\n",
    "                    url_text = loc.text if loc is not None else None\n",
    "                    if url_text:\n",
    "                        lastmod = elem.find('sm:lastmod', ns)\n",
    "                        image_elem = elem.find('image:image', ns)\n",
    "                        image_caption = \"\"\n",
    "                        if image_elem is not None:\n",
    "                            cap_node = image_elem.find('image:caption', ns)\n",
    "                            image_caption = cap_node.text if cap_node is not None else \"\"\n",
    "                            if not image_caption:\n",
    "                                title_node = image_elem.find('image:title', ns)\n",
    "                                image_caption = title_node.text if title_node is not None else \"\"\n",
    "                        if not image_caption:\n",
    "                            image_caption = url_text.split('/')[-1].replace('-', ' ').replace('.html', '')\n",
    "                        urls_data.append({'url': url_text, 'last_modified': lastmod.text if lastmod is not None else None, 'image_caption': image_caption})\n",
    "                except: pass\n",
    "                finally:\n",
    "                    elem.clear()\n",
    "                    while elem.getprevious() is not None:\n",
    "                        del elem.getparent()[0]\n",
    "            del context\n",
    "            return urls_data\n",
    "    except: return []\n",
    "\"\"\"\n",
    "\n",
    "# Write the file to current directory\n",
    "with open('sitemap_worker.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(worker_code)\n",
    "\n",
    "# Ensure current dir is in path and import\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "try:\n",
    "    from sitemap_worker import parse_gz_sitemap_worker\n",
    "    print(\"‚úÖ Worker module loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to load worker module: {e}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MAIN LOGIC (MULTIPROCESSING)\n",
    "# ============================================================================\n",
    "def detect_stale_pages_bouncer(log_folder_path, sitemap_list_path, output_dir='stale_pages_output', \n",
    "                               max_records_per_file=500000, max_gz_per_index=None,\n",
    "                               log_batch_size=5, workers=None):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Auto-detect optimal workers if not specified\n",
    "    if workers is None:\n",
    "        workers = max(1, os.cpu_count() - 2)\n",
    "    \n",
    "    # --- Check Sitemap List File ---\n",
    "    if not os.path.exists(sitemap_list_path):\n",
    "        print(f\"‚ùå Sitemap list file not found: {sitemap_list_path}\")\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        with open(sitemap_list_path, 'r', encoding='utf-8') as f:\n",
    "            sitemap_urls = [line.strip() for line in f if line.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading sitemap list file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if not sitemap_urls:\n",
    "        print(\"‚ùå Sitemap list file is empty\")\n",
    "        return None\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üï∫ STALE PAGE DETECTION: THE PARTY BOUNCER (MULTIPROCESSING MODE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Log folder: {log_folder_path}\")\n",
    "    print(f\"üìÑ Sitemap list: {sitemap_list_path}\")\n",
    "    print(f\"‚öôÔ∏è  Workers: {workers} (ProcessPoolExecutor)\")\n",
    "    print(f\"üó∫Ô∏è  Processing {len(sitemap_urls)} sitemap indexes from list\")\n",
    "    \n",
    "    # --- Step 1: Processing Logs ---\n",
    "    print(\"\\nüìñ Step 1: Building the Guest List (Processing Logs)...\")\n",
    "    \n",
    "    if not os.path.exists(log_folder_path):\n",
    "        print(f\"‚ùå Folder not found: {log_folder_path}\")\n",
    "        return None\n",
    "\n",
    "    log_files = [f for f in os.listdir(log_folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not log_files:\n",
    "        print(\"‚ùå No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    all_log_stats = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(log_files), log_batch_size), desc=\"   Processing Logs\"):\n",
    "        batch_files = log_files[i:i+log_batch_size]\n",
    "        dfs = []\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(log_folder_path, file), \n",
    "                               usecols=['request_uri', 'http_user_agent', 'time_iso8601', 'status'], \n",
    "                               encoding='utf-8-sig', low_memory=False)\n",
    "                dfs.append(df)\n",
    "            except: continue\n",
    "            \n",
    "        if dfs:\n",
    "            batch_df = pd.concat(dfs)\n",
    "            batch_df['url'] = 'https://www.alamy.com' + batch_df['request_uri'].str.split('?').str[0].fillna('')\n",
    "            batch_df['timestamp'] = pd.to_datetime(batch_df['time_iso8601'], errors='coerce')\n",
    "            \n",
    "            min_date = batch_df['timestamp'].min()\n",
    "            max_date = batch_df['timestamp'].max()\n",
    "            days = (max_date - min_date).days + 1 if pd.notnull(max_date) else 1\n",
    "            \n",
    "            batch_stats = batch_df.groupby('url').agg({\n",
    "                'request_uri': 'count',\n",
    "                'http_user_agent': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "                'status': lambda x: x.mode()[0] if len(x.mode()) > 0 else 200\n",
    "            }).reset_index()\n",
    "            batch_stats.columns = ['url', 'crawl_count', 'user_agent', 'status_code']\n",
    "            batch_stats['days_active'] = days\n",
    "            \n",
    "            all_log_stats.append(batch_stats)\n",
    "            del batch_df, dfs\n",
    "            gc.collect()\n",
    "\n",
    "    if not all_log_stats:\n",
    "        print(\"‚ùå No log data processed.\")\n",
    "        return None\n",
    "\n",
    "    print(\"   ‚îú‚îÄ Finalizing Guest List...\")\n",
    "    full_log_stats = pd.concat(all_log_stats).groupby('url').agg({\n",
    "        'crawl_count': 'sum',\n",
    "        'user_agent': 'first',\n",
    "        'status_code': 'first',\n",
    "        'days_active': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    full_log_stats['crawl_frequency'] = full_log_stats['crawl_count'] / full_log_stats['days_active']\n",
    "    full_log_stats['authority_score'] = (full_log_stats['crawl_count'] * 0.7 + \n",
    "                                         full_log_stats['crawl_frequency'] * full_log_stats['days_active'] * 0.3)\n",
    "    \n",
    "    threshold = full_log_stats['crawl_count'].quantile(0.10)\n",
    "    print(f\"   ‚îú‚îÄ Bottom 10% Threshold: <= {threshold} crawls\")\n",
    "    \n",
    "    invited_guests = set(full_log_stats['url'])\n",
    "    wallflowers_df = full_log_stats[full_log_stats['crawl_count'] <= threshold]\n",
    "    wallflowers = set(wallflowers_df['url'])\n",
    "    \n",
    "    print(f\"‚úÖ Guest List Ready: {len(invited_guests):,} total, {len(wallflowers):,} low activity\")\n",
    "\n",
    "    # --- Step 2: Sitemaps ---\n",
    "    print(\"\\nüì• Step 2: Checking Sitemaps (Multiprocessing)...\")\n",
    "    \n",
    "    def parse_sitemap_index(index_url):\n",
    "        try:\n",
    "            response = requests.get(index_url, timeout=60)\n",
    "            root = ET.fromstring(response.content)\n",
    "            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            return [loc.text for loc in root.findall('sm:sitemap/sm:loc', ns)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    stale_party_list = []\n",
    "    total_processed_urls = 0\n",
    "    \n",
    "    def check_at_gate(batch_df):\n",
    "        batch_df['is_invited'] = batch_df['url'].isin(invited_guests)\n",
    "        batch_df['is_wallflower'] = batch_df['url'].isin(wallflowers)\n",
    "        \n",
    "        orphans = batch_df[~batch_df['is_invited']].copy()\n",
    "        orphans['page_type'] = 'Orphan'\n",
    "        orphans['crawl_count'] = 0\n",
    "        orphans['authority_score'] = 0.0\n",
    "        orphans['crawl_frequency'] = 0.0\n",
    "        orphans['user_agent'] = 'Not Crawled'\n",
    "        orphans['status_code'] = 0  \n",
    "        \n",
    "        low_act = batch_df[batch_df['is_wallflower']].copy()\n",
    "        low_act['page_type'] = 'Low Activity'\n",
    "        \n",
    "        if not low_act.empty:\n",
    "            low_act = low_act.merge(\n",
    "                full_log_stats[['url', 'crawl_count', 'authority_score', 'crawl_frequency', 'user_agent', 'status_code']], \n",
    "                on='url', how='left'\n",
    "            )\n",
    "            low_act['status_code'] = low_act['status_code'].fillna(200)\n",
    "        \n",
    "        return pd.concat([orphans, low_act])\n",
    "\n",
    "    for idx_num, index_url in enumerate(sitemap_urls, 1):\n",
    "        print(f\"\\n   ‚îú‚îÄ Index {idx_num}/{len(sitemap_urls)}: {index_url.split('/')[-1]}\")\n",
    "        gz_urls = parse_sitemap_index(index_url)\n",
    "        if not gz_urls: continue\n",
    "        if max_gz_per_index: gz_urls = gz_urls[:max_gz_per_index]\n",
    "            \n",
    "        print(f\"   ‚îÇ  ‚îú‚îÄ Spawning {workers} PROCESSES for {len(gz_urls)} files...\")\n",
    "        \n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = {executor.submit(parse_gz_sitemap_worker, url): url for url in gz_urls}\n",
    "            batch_results = []\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(gz_urls), desc=\"   ‚îÇ  ‚îî‚îÄ Parsing\", leave=False):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result: batch_results.extend(result)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                if len(batch_results) > 200000:\n",
    "                    batch_df = pd.DataFrame(batch_results)\n",
    "                    total_processed_urls += len(batch_df)\n",
    "                    processed_batch = check_at_gate(batch_df)\n",
    "                    if not processed_batch.empty:\n",
    "                        stale_party_list.append(processed_batch)\n",
    "                    batch_results = []\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "            \n",
    "            if batch_results:\n",
    "                batch_df = pd.DataFrame(batch_results)\n",
    "                total_processed_urls += len(batch_df)\n",
    "                processed_batch = check_at_gate(batch_df)\n",
    "                if not processed_batch.empty:\n",
    "                    stale_party_list.append(processed_batch)\n",
    "                del batch_df\n",
    "                gc.collect()\n",
    "\n",
    "    # --- Step 3: Saving ---\n",
    "    print(\"\\nüíæ Step 3: Consolidating and Saving...\")\n",
    "    \n",
    "    if not stale_party_list:\n",
    "        print(\"‚ùå No stale pages found.\")\n",
    "        return None\n",
    "        \n",
    "    final_df = pd.concat(stale_party_list, ignore_index=True)\n",
    "    \n",
    "    current_date = pd.Timestamp.now()\n",
    "    final_df['last_modified'] = pd.to_datetime(final_df['last_modified'], errors='coerce')\n",
    "    final_df['days_since_modified'] = (current_date - final_df['last_modified']).dt.days\n",
    "    \n",
    "    final_df['priority_score'] = 0\n",
    "    final_df.loc[final_df['days_since_modified'] > 180, 'priority_score'] = 100\n",
    "    final_df.loc[(final_df['days_since_modified'] > 90) & (final_df['days_since_modified'] <= 180), 'priority_score'] = 70\n",
    "    final_df.loc[final_df['days_since_modified'] <= 90, 'priority_score'] = 40\n",
    "    \n",
    "    final_df = final_df.sort_values(['page_type', 'priority_score'], ascending=[False, False])\n",
    "    \n",
    "    output_cols = ['url', 'crawl_count', 'authority_score', 'crawl_frequency', 'user_agent', \n",
    "                   'status_code', 'page_type', 'priority_score', 'days_since_modified', 'last_modified',\n",
    "                   'image_caption']\n",
    "    \n",
    "    final_cols = [c for c in output_cols if c in final_df.columns]\n",
    "    final_df = final_df[final_cols]\n",
    "    \n",
    "    if len(final_df) <= max_records_per_file:\n",
    "        output_path = os.path.join(output_dir, 'stale_pages.csv')\n",
    "        final_df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "        print(f\"   ‚îî‚îÄ Saved: {output_path}\")\n",
    "    else:\n",
    "        num_parts = (len(final_df) // max_records_per_file) + 1\n",
    "        for i in range(num_parts):\n",
    "            start_idx = i * max_records_per_file\n",
    "            end_idx = min((i + 1) * max_records_per_file, len(final_df))\n",
    "            part_df = final_df.iloc[start_idx:end_idx]\n",
    "            output_path = os.path.join(output_dir, f'stale_pages_part{i+1}.csv')\n",
    "            part_df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "        print(f\"   ‚îî‚îÄ Saved {num_parts} files\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL STATS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Total Analyzed: {total_processed_urls:,}\")\n",
    "    print(f\"‚úÖ Stale Found: {len(final_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Orphans: {len(final_df[final_df['page_type']=='Orphan']):,}\")\n",
    "    print(f\"   ‚Ä¢ Low Activity: {len(final_df[final_df['page_type']=='Low Activity']):,}\")\n",
    "    print(f\"üìÅ Output Location: {os.path.abspath(output_dir)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ============================================================================\n",
    "# 3. UI WIDGETS\n",
    "# ============================================================================\n",
    "log_folder_input = widgets.Text(\n",
    "    value='', \n",
    "    placeholder='D:\\\\path\\\\to\\\\log\\\\files', \n",
    "    description='Log Folder:', \n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "sitemap_file_input = widgets.Text(\n",
    "    value='sitemap_list.txt',\n",
    "    placeholder='Enter path to sitemap.txt file', \n",
    "    description='Sitemap List:', \n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "test_mode_checkbox = widgets.Checkbox(value=True, description='Test Mode')\n",
    "log_batch_input = widgets.IntText(value=5, description='Log Batch:')\n",
    "gz_batch_input = widgets.IntText(value=max(1, os.cpu_count() - 2), description='Workers (CPU):')\n",
    "\n",
    "run_button = widgets.Button(description='üöÄ Run Party Bouncer', button_style='success', icon='check', layout=widgets.Layout(width='300px'))\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        max_gz = 10 if test_mode_checkbox.value else None\n",
    "        detect_stale_pages_bouncer(\n",
    "            log_folder_input.value, \n",
    "            sitemap_file_input.value,\n",
    "            max_gz_per_index=max_gz, \n",
    "            log_batch_size=log_batch_input.value, \n",
    "            workers=gz_batch_input.value\n",
    "        )\n",
    "\n",
    "run_button.on_click(on_run_clicked)\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>üï∫ Stale Page Detection: Party Bouncer (Multiprocessing)</h2>\"), \n",
    "    log_folder_input, \n",
    "    sitemap_file_input, \n",
    "    widgets.HBox([log_batch_input, gz_batch_input]), \n",
    "    test_mode_checkbox, \n",
    "    run_button, \n",
    "    output_area\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2374d-ac9a-471b-8fbf-ad8d470ee175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ffdc4-4800-4402-9311-a6f4b78ad47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
