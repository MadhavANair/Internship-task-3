{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cb2a9-00f6-4847-878b-d6a94cad5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_conda_cache():\n",
    "    \"\"\"Comprehensive Conda cache cleanup\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üßπ CONDA CACHE CLEANUP\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Run conda clean --all\n",
    "    print(\"\\n1Ô∏è‚É£ Running 'conda clean --all'...\")\n",
    "    try:\n",
    "        result = subprocess.run(['conda', 'clean', '--all', '-y'], \n",
    "                              capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        print(\"‚úÖ Conda clean completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error running conda clean: {e}\")\n",
    "    \n",
    "    # 2. Find and show conda directories\n",
    "    print(\"\\n2Ô∏è‚É£ Locating Conda directories...\")\n",
    "    \n",
    "    try:\n",
    "        # Get conda info\n",
    "        result = subprocess.run(['conda', 'info', '--base'], \n",
    "                              capture_output=True, text=True)\n",
    "        conda_base = result.stdout.strip()\n",
    "        print(f\"   Conda base: {conda_base}\")\n",
    "        \n",
    "        # Common cache locations\n",
    "        cache_locations = [\n",
    "            os.path.join(conda_base, 'pkgs'),\n",
    "            os.path.join(conda_base, 'conda-meta'),\n",
    "            os.path.expanduser('~/.conda/pkgs'),\n",
    "            os.path.expanduser('~/anaconda3/pkgs'),\n",
    "            os.path.expanduser('~/miniconda3/pkgs'),\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n3Ô∏è‚É£ Cache locations:\")\n",
    "        for location in cache_locations:\n",
    "            if os.path.exists(location):\n",
    "                size = sum(f.stat().st_size for f in Path(location).rglob('*') if f.is_file())\n",
    "                size_mb = size / (1024**2)\n",
    "                print(f\"   üìÅ {location} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "    \n",
    "    # 3. Clear pip cache\n",
    "    print(\"\\n4Ô∏è‚É£ Clearing pip cache...\")\n",
    "    try:\n",
    "        subprocess.run(['pip', 'cache', 'purge'], capture_output=True)\n",
    "        print(\"‚úÖ Pip cache cleared\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ CLEANUP COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clear_conda_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0f4c4-30ed-48fe-b1ef-c661051fdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_phase2(input_folder=None, input_file=None, output_folder=\"googlebot_final_only\"):\n",
    "    \"\"\"\n",
    "    Phase 2: Log file verification and processing\n",
    "    IN-MEMORY 3-PHASE GOOGLEBOT EXTRACTION\n",
    "    Only outputs final Phase 3 files with search-API filtering\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_folder : str, optional\n",
    "        Path to folder containing .log or .log.gz files\n",
    "    input_file : str, optional\n",
    "        Path to single .log or .log.gz file\n",
    "    output_folder : str\n",
    "        Directory where output CSV files will be saved\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            'status': 'success' or 'error',\n",
    "            'output_folder': absolute path to output folder,\n",
    "            'phase3_files': list of output CSV files,\n",
    "            'final_records': total records after filtering,\n",
    "            'search_api_filtered': count of search-API URLs filtered,\n",
    "            'match_rate': percentage match with GSC,\n",
    "            'rejection_stats': breakdown of verification stats\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import csv\n",
    "    import gzip\n",
    "    import socket\n",
    "    import time\n",
    "    import zipfile\n",
    "    import ipaddress\n",
    "    import requests\n",
    "    import dns.resolver\n",
    "    import dns.reversename\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    from pathlib import Path\n",
    "    from tqdm import tqdm\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CONFIGURATION\n",
    "    # ============================================================================\n",
    "    \n",
    "    MAX_DNS_WORKERS = 150\n",
    "    DNS_TIMEOUT = 2\n",
    "    IP_RANGES_CACHE_FILE = \"google_ip_ranges_cache.json\"\n",
    "    CACHE_EXPIRY_HOURS = 24\n",
    "    GOOGLE_DOMAINS = ['.googlebot.com', '.google.com', '.googleusercontent.com']\n",
    "    MAX_RECORDS_PER_FILE = 500000\n",
    "    \n",
    "    # NEW: Lenient mode and FcrDNS fallback\n",
    "    LENIENT_MODE = True  # Accept records with UA but missing/invalid IP\n",
    "    ENABLE_FCRDNS_FALLBACK = True  # Verify non-CIDR IPs with FcrDNS\n",
    "    \n",
    "    GOOGLE_IP_RANGE_URLS = {\n",
    "        'googlebot': 'https://developers.google.com/static/search/apis/ipranges/googlebot.json',\n",
    "        'special_crawlers': 'https://developers.google.com/static/search/apis/ipranges/special-crawlers.json',\n",
    "        'user_triggered_fetchers': 'https://developers.google.com/static/search/apis/ipranges/user-triggered-fetchers.json',\n",
    "        'user_triggered_fetchers_google': 'https://developers.google.com/static/search/apis/ipranges/user-triggered-fetchers-google.json',\n",
    "    }\n",
    "    \n",
    "    # ============================================================================\n",
    "    # FETCH DYNAMIC IP RANGES\n",
    "    # ============================================================================\n",
    "    \n",
    "    def fetch_google_ip_ranges():\n",
    "        \"\"\"Fetch Google's official IP ranges (IPv4 + IPv6) from JSON files\"\"\"\n",
    "        print(\"\\nüåê Fetching Google IP ranges from official sources...\")\n",
    "        \n",
    "        all_prefixes = []\n",
    "        \n",
    "        for source_name, url in GOOGLE_IP_RANGE_URLS.items():\n",
    "            try:\n",
    "                print(f\"   ‚Ä¢ Fetching {source_name}...\")\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                \n",
    "                if 'prefixes' in data:\n",
    "                    for prefix in data['prefixes']:\n",
    "                        if 'ipv4Prefix' in prefix:\n",
    "                            all_prefixes.append(prefix['ipv4Prefix'])\n",
    "                        if 'ipv6Prefix' in prefix:\n",
    "                            all_prefixes.append(prefix['ipv6Prefix'])\n",
    "                \n",
    "                print(f\"     ‚úÖ Retrieved {len(data.get('prefixes', []))} prefixes\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ö†Ô∏è Warning: Failed to fetch {source_name}: {str(e)}\")\n",
    "        \n",
    "        ip_networks = []\n",
    "        ipv4_count = 0\n",
    "        ipv6_count = 0\n",
    "        \n",
    "        for prefix in all_prefixes:\n",
    "            try:\n",
    "                network = ipaddress.ip_network(prefix)\n",
    "                ip_networks.append(network)\n",
    "                \n",
    "                if network.version == 4:\n",
    "                    ipv4_count += 1\n",
    "                else:\n",
    "                    ipv6_count += 1\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\n‚úÖ Total IP ranges loaded: {len(ip_networks)} CIDR blocks\")\n",
    "        print(f\"   ‚Ä¢ IPv4 ranges: {ipv4_count}\")\n",
    "        print(f\"   ‚Ä¢ IPv6 ranges: {ipv6_count}\")\n",
    "        return ip_networks\n",
    "    \n",
    "    def load_or_fetch_ip_ranges(cache_file=IP_RANGES_CACHE_FILE):\n",
    "        \"\"\"Load IP ranges from cache or fetch from Google if expired\"\"\"\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'r') as f:\n",
    "                    cache_data = json.load(f)\n",
    "                \n",
    "                cache_time = cache_data.get('timestamp', 0)\n",
    "                cache_age_hours = (time.time() - cache_time) / 3600\n",
    "                \n",
    "                if cache_age_hours < CACHE_EXPIRY_HOURS:\n",
    "                    print(f\"\\n‚úÖ Using cached IP ranges (age: {cache_age_hours:.1f} hours)\")\n",
    "                    ip_networks = [ipaddress.ip_network(cidr) for cidr in cache_data['prefixes']]\n",
    "                    \n",
    "                    ipv4_count = sum(1 for n in ip_networks if n.version == 4)\n",
    "                    ipv6_count = sum(1 for n in ip_networks if n.version == 6)\n",
    "                    print(f\"   ‚Ä¢ IPv4 ranges: {ipv4_count}\")\n",
    "                    print(f\"   ‚Ä¢ IPv6 ranges: {ipv6_count}\")\n",
    "                    return ip_networks\n",
    "                else:\n",
    "                    print(f\"\\n‚è∞ Cache expired, fetching fresh data...\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ö†Ô∏è Cache error, fetching fresh data...\")\n",
    "        \n",
    "        ip_networks = fetch_google_ip_ranges()\n",
    "        \n",
    "        try:\n",
    "            cache_data = {\n",
    "                'timestamp': time.time(),\n",
    "                'prefixes': [str(network) for network in ip_networks]\n",
    "            }\n",
    "            with open(cache_file, 'w') as f:\n",
    "                json.dump(cache_data, f, indent=2)\n",
    "            print(f\"üíæ IP ranges cached\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save cache\")\n",
    "        \n",
    "        return ip_networks\n",
    "    \n",
    "    # ============================================================================\n",
    "    # UTILITY FUNCTIONS\n",
    "    # ============================================================================\n",
    "    \n",
    "    def extract_full_timestamp_from_filename(filename):\n",
    "        \"\"\"Extract unique timestamp from filename\"\"\"\n",
    "        base_name = filename\n",
    "        if base_name.endswith('.gz'):\n",
    "            base_name = base_name[:-3]\n",
    "        if base_name.endswith('.log'):\n",
    "            base_name = base_name[:-4]\n",
    "        if base_name.startswith('nginx-'):\n",
    "            base_name = base_name[6:]\n",
    "        return base_name if base_name else None\n",
    "    \n",
    "    def extract_first_ip(ip_string):\n",
    "        \"\"\"Extract first IP (IPv4 or IPv6) from X-Forwarded-For field\"\"\"\n",
    "        if not ip_string:\n",
    "            return None\n",
    "        \n",
    "        ip = str(ip_string).split(',')[0].strip()\n",
    "        \n",
    "        if ip.startswith('[') and ip.endswith(']'):\n",
    "            ip = ip[1:-1]\n",
    "        \n",
    "        return ip if ip else None\n",
    "    \n",
    "    def is_googlebot(user_agent_str):\n",
    "        \"\"\"UA validation - Check if contains googlebot patterns\"\"\"\n",
    "        if not user_agent_str:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            ua_lower = str(user_agent_str).lower()\n",
    "        except:\n",
    "            return False\n",
    "        \n",
    "        googlebot_patterns = [\n",
    "            'googlebot', 'google-inspectiontool', 'googlebot-image', 'googlebot-news',\n",
    "            'googlebot-video', 'adsbot-google', 'mediapartners-google', 'apis-google',\n",
    "            'google favicon', 'feedfetcher-google', 'google-read-aloud', 'duplichecker',\n",
    "            'google web preview', 'google-site-verification', 'google-smartphone'\n",
    "        ]\n",
    "        \n",
    "        for pattern in googlebot_patterns:\n",
    "            if pattern in ua_lower:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_search_api_url(url):\n",
    "        \"\"\"Check if URL contains search-api pattern\"\"\"\n",
    "        if not url:\n",
    "            return False\n",
    "        return '/search-api/v1/search/' in str(url).lower()\n",
    "    \n",
    "    def create_zip_archive(file_list, output_folder, archive_name=\"googlebot_data\"):\n",
    "        \"\"\"Create ZIP archive\"\"\"\n",
    "        zip_filename = os.path.join(output_folder, f\"{archive_name}_{time.strftime('%Y%m%d_%H%M%S')}.zip\")\n",
    "        \n",
    "        print(f\"\\nüì¶ Creating ZIP archive...\")\n",
    "        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for file_path in tqdm(file_list, desc=\"Archiving\", unit=\" files\"):\n",
    "                zipf.write(file_path, arcname=os.path.basename(file_path))\n",
    "        \n",
    "        zip_size_mb = os.path.getsize(zip_filename) / (1024**2)\n",
    "        print(f\"‚úÖ ZIP created: {os.path.basename(zip_filename)} ({zip_size_mb:.2f} MB)\")\n",
    "        return zip_filename\n",
    "    \n",
    "    # ============================================================================\n",
    "    # VERIFICATION METHODS\n",
    "    # ============================================================================\n",
    "    \n",
    "    def verify_ip_in_range(ip, ip_networks):\n",
    "        \"\"\"Check if IP is in Google's CIDR ranges\"\"\"\n",
    "        try:\n",
    "            ip_addr = ipaddress.ip_address(ip)\n",
    "            for network in ip_networks:\n",
    "                if ip_addr in network:\n",
    "                    return True\n",
    "            return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "    \n",
    "    def verify_ip_method2_dns_fast(ip):\n",
    "        \"\"\"FcrDNS Verification - ULTRA FAST using dnspython\"\"\"\n",
    "        try:\n",
    "            rev_name = dns.reversename.from_address(ip)\n",
    "            \n",
    "            resolver = dns.resolver.Resolver()\n",
    "            resolver.timeout = DNS_TIMEOUT\n",
    "            resolver.lifetime = DNS_TIMEOUT\n",
    "            \n",
    "            try:\n",
    "                reverse_answers = resolver.resolve(rev_name, 'PTR')\n",
    "                hostname = str(reverse_answers[0]).rstrip('.')\n",
    "                hostname_lower = hostname.lower()\n",
    "            except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.Timeout, dns.exception.DNSException):\n",
    "                return False\n",
    "            \n",
    "            is_google_domain = any(hostname_lower.endswith(domain) for domain in GOOGLE_DOMAINS)\n",
    "            \n",
    "            if not is_google_domain:\n",
    "                return False\n",
    "            \n",
    "            try:\n",
    "                forward_ips = []\n",
    "                \n",
    "                try:\n",
    "                    forward_answers = resolver.resolve(hostname, 'A')\n",
    "                    forward_ips.extend([str(rdata) for rdata in forward_answers])\n",
    "                except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    forward_answers = resolver.resolve(hostname, 'AAAA')\n",
    "                    forward_ips.extend([str(rdata) for rdata in forward_answers])\n",
    "                except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n",
    "                    pass\n",
    "                \n",
    "                if not forward_ips:\n",
    "                    return False\n",
    "                \n",
    "                if ip in forward_ips:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            \n",
    "            except (dns.resolver.Timeout, dns.exception.DNSException):\n",
    "                return False\n",
    "        \n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def verify_ips_parallel_dns(ips_to_verify, desc=\"DNS Verification\"):\n",
    "        \"\"\"Verify IPs using FcrDNS in parallel\"\"\"\n",
    "        verified_ips = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=MAX_DNS_WORKERS) as executor:\n",
    "            future_to_ip = {\n",
    "                executor.submit(verify_ip_method2_dns_fast, ip): ip \n",
    "                for ip in ips_to_verify\n",
    "            }\n",
    "            \n",
    "            with tqdm(total=len(ips_to_verify), desc=desc, unit=\" IPs\") as pbar:\n",
    "                for future in as_completed(future_to_ip):\n",
    "                    ip = future_to_ip[future]\n",
    "                    try:\n",
    "                        is_verified = future.result(timeout=DNS_TIMEOUT + 1)\n",
    "                        verified_ips[ip] = is_verified\n",
    "                    except Exception:\n",
    "                        verified_ips[ip] = False\n",
    "                    finally:\n",
    "                        pbar.update(1)\n",
    "        \n",
    "        return verified_ips\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MAIN PROCESSING\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéØ IN-MEMORY 3-PHASE GOOGLEBOT EXTRACTION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"   Lenient Mode: {'ENABLED' if LENIENT_MODE else 'DISABLED'}\")\n",
    "    print(f\"   FcrDNS Fallback: {'ENABLED' if ENABLE_FCRDNS_FALLBACK else 'DISABLED'}\")\n",
    "    print(f\"   Search-API Filter: ENABLED\")\n",
    "    \n",
    "    # Load IP ranges\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"LOADING GOOGLE IP RANGES\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    try:\n",
    "        google_ip_networks = load_or_fetch_ip_ranges()\n",
    "        print(f\"‚úÖ Ready with {len(google_ip_networks)} official IP ranges\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load IP ranges: {str(e)}\")\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to load IP ranges: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # Get input files\n",
    "    log_files = []\n",
    "    if input_file:\n",
    "        if os.path.exists(input_file):\n",
    "            log_files = [input_file]\n",
    "        else:\n",
    "            print(f\"‚ùå File not found: {input_file}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': f'File not found: {input_file}'\n",
    "            }\n",
    "    elif input_folder:\n",
    "        if os.path.exists(input_folder):\n",
    "            log_files = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) \n",
    "                        if f.endswith('.log') or f.endswith('.log.gz')])\n",
    "            if not log_files:\n",
    "                print(f\"‚ùå No .log files found in: {input_folder}\")\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'error': f'No .log files found in: {input_folder}'\n",
    "                }\n",
    "        else:\n",
    "            print(f\"‚ùå Folder not found: {input_folder}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': f'Folder not found: {input_folder}'\n",
    "            }\n",
    "    else:\n",
    "        print(\"‚ùå Please provide input_folder or input_file\")\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': 'Please provide input_folder or input_file'\n",
    "        }\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    abs_output_folder = os.path.abspath(output_folder)\n",
    "    \n",
    "    print(f\"\\nüìÅ Input files: {len(log_files)}\")\n",
    "    \n",
    "    print(f\"\\nüîç IN-MEMORY 3-PHASE STRATEGY:\")\n",
    "    print(f\"   Phase 1: Filter by Googlebot UA ‚Üí Store in memory\")\n",
    "    print(f\"   Phase 2: Check IPs in CIDR ‚Üí Store in memory\")\n",
    "    print(f\"   Phase 3: FcrDNS verification ‚Üí Write final output (no intermediate files)\")\n",
    "    print(f\"   Phase 4: Search-API filter ‚Üí Final clean dataset\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 1: IN-MEMORY UA FILTERING\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 1: FILTER BY GOOGLEBOT UA (IN-MEMORY)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    googlebot_records = []\n",
    "    unique_ips_phase1 = set()\n",
    "    googlebot_variants = {}\n",
    "    \n",
    "    stats_phase1 = {\n",
    "        'total_processed': 0,\n",
    "        'has_googlebot_ua': 0,\n",
    "    }\n",
    "    \n",
    "    start_phase1 = time.time()\n",
    "    \n",
    "    for file_idx, log_file in enumerate(log_files, 1):\n",
    "        filename = os.path.basename(log_file)\n",
    "        file_size_mb = os.path.getsize(log_file) / (1024**2)\n",
    "        print(f\"\\nüìÇ [{file_idx}/{len(log_files)}] {filename} ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        file_accepted = 0\n",
    "        \n",
    "        try:\n",
    "            if log_file.endswith('.gz'):\n",
    "                file_handle = gzip.open(log_file, 'rt', encoding='utf-8', errors='ignore')\n",
    "            else:\n",
    "                file_handle = open(log_file, 'r', encoding='utf-8', errors='ignore')\n",
    "            \n",
    "            with file_handle as log_reader:\n",
    "                pbar = tqdm(log_reader, desc=\"   Filtering UA\", unit=\" recs\", ncols=100, mininterval=0.5)\n",
    "                \n",
    "                for line in pbar:\n",
    "                    try:\n",
    "                        if not line.strip():\n",
    "                            continue\n",
    "                        \n",
    "                        log_entry = json.loads(line.strip())\n",
    "                        \n",
    "                        stats_phase1['total_processed'] += 1\n",
    "                        \n",
    "                        user_agent = log_entry.get('http_user_agent', '')\n",
    "                        if user_agent:\n",
    "                            user_agent = str(user_agent).strip()\n",
    "                        \n",
    "                        if not is_googlebot(user_agent):\n",
    "                            continue\n",
    "                        \n",
    "                        stats_phase1['has_googlebot_ua'] += 1\n",
    "                        \n",
    "                        # Track variant\n",
    "                        ua_lower = user_agent.lower()\n",
    "                        for variant in ['googlebot-image', 'googlebot-news', 'googlebot-video', \n",
    "                                       'google-inspectiontool', 'adsbot-google', 'mediapartners-google']:\n",
    "                            if variant in ua_lower:\n",
    "                                key = variant.replace('-', ' ').title().replace(' ', '-')\n",
    "                                googlebot_variants[key] = googlebot_variants.get(key, 0) + 1\n",
    "                                break\n",
    "                        else:\n",
    "                            if 'googlebot' in ua_lower:\n",
    "                                googlebot_variants['Googlebot (standard)'] = googlebot_variants.get('Googlebot (standard)', 0) + 1\n",
    "                        \n",
    "                        # Extract all required fields\n",
    "                        ip = extract_first_ip(log_entry.get('http_x_forwarded_for', ''))\n",
    "                        if ip:\n",
    "                            unique_ips_phase1.add(ip)\n",
    "                        \n",
    "                        # Store record with all fields\n",
    "                        record = {\n",
    "                            'time_iso8601': str(log_entry.get('time_iso8601', '')).strip(),\n",
    "                            'request_uri': str(log_entry.get('request_uri', '')).strip(),\n",
    "                            'status': str(log_entry.get('status', '')).strip(),\n",
    "                            'http_user_agent': user_agent,\n",
    "                            'http_x_forwarded_for': ip if ip else '',\n",
    "                            'geoip_country_code': str(log_entry.get('geoip_country_code', '')).strip(),\n",
    "                            'upstream_response_time': str(log_entry.get('upstream_response_time', '')).strip(),\n",
    "                            'bytes_sent': str(log_entry.get('bytes_sent', log_entry.get('body_bytes_sent', ''))).strip(),\n",
    "                            'source_file': filename\n",
    "                        }\n",
    "                        \n",
    "                        googlebot_records.append(record)\n",
    "                        file_accepted += 1\n",
    "                    \n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                pbar.close()\n",
    "            \n",
    "            print(f\"   ‚úÖ Accepted: {file_accepted:,} records with Googlebot UA\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    elapsed_phase1 = time.time() - start_phase1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 1 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Total processed: {stats_phase1['total_processed']:,}\")\n",
    "    print(f\"   Googlebot UA filtered: {len(googlebot_records):,}\")\n",
    "    print(f\"   Unique IPs: {len(unique_ips_phase1):,}\")\n",
    "    print(f\"   Time: {elapsed_phase1:.1f}s ({elapsed_phase1/60:.1f} min)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 2: CIDR CHECK (IN-MEMORY)\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 2: CHECK IPS AGAINST CIDR RANGES (IN-MEMORY)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\nüîç Checking {len(unique_ips_phase1):,} IPs against CIDR ranges...\")\n",
    "    verified_ips_cidr = {}\n",
    "    for ip in tqdm(unique_ips_phase1, desc=\"Checking IPs\", unit=\" IPs\"):\n",
    "        verified_ips_cidr[ip] = verify_ip_in_range(ip, google_ip_networks)\n",
    "    \n",
    "    ips_in_range = sum(1 for v in verified_ips_cidr.values() if v)\n",
    "    unique_ips_outside_cidr = {ip for ip, in_range in verified_ips_cidr.items() if not in_range}\n",
    "    \n",
    "    print(f\"   ‚úÖ IPs in CIDR ranges: {ips_in_range:,}/{len(unique_ips_phase1):,}\")\n",
    "    print(f\"   ‚ö†Ô∏è IPs outside ranges: {len(unique_ips_outside_cidr):,} (will check with FcrDNS in Phase 3)\")\n",
    "    \n",
    "    elapsed_phase2 = time.time() - start_phase1 - elapsed_phase1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 2 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Time: {elapsed_phase2:.1f}s ({elapsed_phase2/60:.1f} min)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 3: FcrDNS + WRITE FINAL OUTPUT\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 3: FcrDNS VERIFICATION + WRITE FINAL OUTPUT\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    dns_verified_ips = {}\n",
    "    \n",
    "    if ENABLE_FCRDNS_FALLBACK and len(unique_ips_outside_cidr) > 0:\n",
    "        print(f\"\\nüåê Starting FcrDNS verification for {len(unique_ips_outside_cidr):,} IPs outside CIDR ranges...\")\n",
    "        print(f\"   Using {MAX_DNS_WORKERS} workers\")\n",
    "        \n",
    "        start_dns = time.time()\n",
    "        \n",
    "        dns_verified_ips = verify_ips_parallel_dns(unique_ips_outside_cidr, desc=\"FcrDNS Verification\")\n",
    "        \n",
    "        dns_pass = sum(1 for v in dns_verified_ips.values() if v)\n",
    "        elapsed_dns = time.time() - start_dns\n",
    "        \n",
    "        print(f\"\\n‚úÖ FcrDNS Verification Results:\")\n",
    "        print(f\"   Passed: {dns_pass:,}/{len(unique_ips_outside_cidr):,}\")\n",
    "        print(f\"   Time: {elapsed_dns:.1f}s ({elapsed_dns/60:.1f} min)\")\n",
    "        if len(unique_ips_outside_cidr) > 0:\n",
    "            print(f\"   Speed: {len(unique_ips_outside_cidr)/elapsed_dns:.0f} IPs/sec\")\n",
    "    \n",
    "    print(f\"\\nüìù Writing final verified records (with search-API filtering)...\")\n",
    "    \n",
    "    phase3_files = []\n",
    "    total_phase3_before_filter = 0\n",
    "    total_phase3_after_filter = 0\n",
    "    search_api_filtered = 0\n",
    "    \n",
    "    rejection_stats = {\n",
    "        'in_cidr': 0,\n",
    "        'missing_ip_accepted': 0,\n",
    "        'invalid_ip_accepted': 0,\n",
    "        'fcrdns_passed': 0,\n",
    "        'fcrdns_failed_rejected': 0\n",
    "    }\n",
    "    \n",
    "    # Group records by source file\n",
    "    records_by_file = {}\n",
    "    for record in googlebot_records:\n",
    "        source_file = record['source_file']\n",
    "        if source_file not in records_by_file:\n",
    "            records_by_file[source_file] = []\n",
    "        records_by_file[source_file].append(record)\n",
    "    \n",
    "    start_write = time.time()\n",
    "    \n",
    "    for source_file, records in tqdm(sorted(records_by_file.items()), desc=\"Writing files\", unit=\" files\"):\n",
    "        file_timestamp = extract_full_timestamp_from_filename(source_file)\n",
    "        \n",
    "        if file_timestamp:\n",
    "            base_filename = f\"googlebot_{file_timestamp}\"\n",
    "        else:\n",
    "            base_filename = f\"googlebot_file\"\n",
    "        \n",
    "        current_file_index = 1\n",
    "        current_file_records = 0\n",
    "        current_writer = None\n",
    "        current_csvfile = None\n",
    "        \n",
    "        def create_output_file(file_index):\n",
    "            if file_index == 1:\n",
    "                csv_filename = os.path.join(output_folder, f\"{base_filename}.csv\")\n",
    "            else:\n",
    "                csv_filename = os.path.join(output_folder, f\"{base_filename}_part{file_index}.csv\")\n",
    "            \n",
    "            csvfile = open(csv_filename, 'w', newline='', encoding='utf-8-sig')\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                'time_iso8601', 'request_uri', 'status', 'http_user_agent',\n",
    "                'http_x_forwarded_for', 'geoip_country_code', \n",
    "                'upstream_response_time', 'bytes_sent'\n",
    "            ], quoting=csv.QUOTE_ALL)\n",
    "            writer.writeheader()\n",
    "            phase3_files.append(csv_filename)\n",
    "            return csvfile, writer\n",
    "        \n",
    "        current_csvfile, current_writer = create_output_file(current_file_index)\n",
    "        \n",
    "        for record in records:\n",
    "            ip = record['http_x_forwarded_for'].strip()\n",
    "            \n",
    "            # Determine if record should be accepted\n",
    "            accept_record = False\n",
    "            \n",
    "            if not ip or ip == '' or ip.lower() in ['none', 'null', '-', 'unknown']:\n",
    "                if LENIENT_MODE:\n",
    "                    accept_record = True\n",
    "                    rejection_stats['missing_ip_accepted'] += 1\n",
    "            elif ip in verified_ips_cidr:\n",
    "                if verified_ips_cidr[ip]:\n",
    "                    accept_record = True\n",
    "                    rejection_stats['in_cidr'] += 1\n",
    "                else:\n",
    "                    if dns_verified_ips.get(ip, False):\n",
    "                        accept_record = True\n",
    "                        rejection_stats['fcrdns_passed'] += 1\n",
    "                    else:\n",
    "                        rejection_stats['fcrdns_failed_rejected'] += 1\n",
    "            else:\n",
    "                if LENIENT_MODE:\n",
    "                    accept_record = True\n",
    "                    rejection_stats['invalid_ip_accepted'] += 1\n",
    "            \n",
    "            if accept_record:\n",
    "                total_phase3_before_filter += 1\n",
    "                \n",
    "                # Check for search-API URL\n",
    "                if is_search_api_url(record['request_uri']):\n",
    "                    search_api_filtered += 1\n",
    "                    continue\n",
    "                \n",
    "                total_phase3_after_filter += 1\n",
    "                \n",
    "                if current_file_records >= MAX_RECORDS_PER_FILE:\n",
    "                    current_csvfile.close()\n",
    "                    current_file_index += 1\n",
    "                    current_csvfile, current_writer = create_output_file(current_file_index)\n",
    "                    current_file_records = 0\n",
    "                \n",
    "                current_writer.writerow({\n",
    "                    'time_iso8601': record['time_iso8601'],\n",
    "                    'request_uri': record['request_uri'],\n",
    "                    'status': record['status'],\n",
    "                    'http_user_agent': record['http_user_agent'],\n",
    "                    'http_x_forwarded_for': record['http_x_forwarded_for'],\n",
    "                    'geoip_country_code': record['geoip_country_code'],\n",
    "                    'upstream_response_time': record['upstream_response_time'],\n",
    "                    'bytes_sent': record['bytes_sent']\n",
    "                })\n",
    "                current_file_records += 1\n",
    "        \n",
    "        if current_csvfile and not current_csvfile.closed:\n",
    "            current_csvfile.close()\n",
    "    \n",
    "    elapsed_write = time.time() - start_write\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 3 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Before search-API filter: {total_phase3_before_filter:,}\")\n",
    "    print(f\"   Search-API URLs filtered: {search_api_filtered:,}\")\n",
    "    print(f\"   After search-API filter: {total_phase3_after_filter:,}\")\n",
    "    print(f\"   Breakdown:\")\n",
    "    print(f\"   - CIDR verified: {rejection_stats['in_cidr']:,}\")\n",
    "    print(f\"   - Missing IP (lenient): {rejection_stats['missing_ip_accepted']:,}\")\n",
    "    print(f\"   - Invalid IP (lenient): {rejection_stats['invalid_ip_accepted']:,}\")\n",
    "    print(f\"   - FcrDNS passed: {rejection_stats['fcrdns_passed']:,}\")\n",
    "    print(f\"   - FcrDNS failed (rejected): {rejection_stats['fcrdns_failed_rejected']:,}\")\n",
    "    print(f\"   Output files: {len(phase3_files)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SUMMARY\n",
    "    # ========================================================================\n",
    "    total_time = time.time() - start_phase1\n",
    "    \n",
    "    gsc_expected = 14913024\n",
    "    match_rate = (total_phase3_after_filter / gsc_expected * 100) if total_phase3_after_filter > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ COMPLETE: IN-MEMORY 3-PHASE EXTRACTION\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  PERFORMANCE:\")\n",
    "    print(f\"   Phase 1 (UA Filter): {elapsed_phase1:.1f}s ({elapsed_phase1/60:.1f} min)\")\n",
    "    print(f\"   Phase 2 (CIDR Check): {elapsed_phase2:.1f}s ({elapsed_phase2/60:.1f} min)\")\n",
    "    print(f\"   Phase 3 (FcrDNS + Write): {elapsed_write:.1f}s ({elapsed_write/60:.1f} min)\")\n",
    "    print(f\"   TOTAL: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS:\")\n",
    "    print(f\"   Phase 1: {len(googlebot_records):,} (Googlebot UA)\")\n",
    "    print(f\"   Phase 2: {len(unique_ips_phase1):,} unique IPs checked\")\n",
    "    print(f\"   Phase 3 (before search-API): {total_phase3_before_filter:,}\")\n",
    "    print(f\"   Phase 3 (after search-API): {total_phase3_after_filter:,} ‚úÖ\")\n",
    "    \n",
    "    print(f\"\\nü§ñ GOOGLEBOT VARIANTS:\")\n",
    "    for variant, count in sorted(googlebot_variants.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   ‚Ä¢ {variant}: {count:,}\")\n",
    "    \n",
    "    print(f\"\\nüéØ GSC COMPARISON:\")\n",
    "    print(f\"   GSC: {gsc_expected:,}\")\n",
    "    print(f\"   Output: {total_phase3_after_filter:,}\")\n",
    "    print(f\"   Difference: {abs(gsc_expected - total_phase3_after_filter):,}\")\n",
    "    print(f\"   Match: {match_rate:.1f}%\")\n",
    "    \n",
    "    if match_rate >= 98:\n",
    "        print(f\"   üèÜ NEAR-PERFECT!\")\n",
    "    elif match_rate >= 95:\n",
    "        print(f\"   ‚úÖ EXCELLENT MATCH!\")\n",
    "    elif match_rate >= 90:\n",
    "        print(f\"   ‚úÖ VERY GOOD!\")\n",
    "    \n",
    "    # Create ZIP\n",
    "    create_zip = True\n",
    "    if create_zip:\n",
    "        zip_filename = create_zip_archive(phase3_files, output_folder, \"googlebot_final\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'output_folder': abs_output_folder,\n",
    "        'phase3_files': phase3_files,\n",
    "        'final_records': total_phase3_after_filter,\n",
    "        'search_api_filtered': search_api_filtered,\n",
    "        'match_rate': match_rate,\n",
    "        'rejection_stats': rejection_stats\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e619246-8471-4741-909b-8123e38528b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_phase3_2(input_folder, output_folder, crawl_threshold=10):\n",
    "    \"\"\"\n",
    "    Phase 3.2: Host Page Counting\n",
    "    Identifies URLs crawled at least once every N days with host eligibility criteria\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_folder : str\n",
    "        Path to folder containing Phase 2 output CSV files\n",
    "    output_folder : str\n",
    "        Directory where host_pages.csv will be saved\n",
    "    crawl_threshold : int\n",
    "        Maximum gap in days between crawls (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            'status': 'success' or 'error',\n",
    "            'host_pages': count of host pages identified,\n",
    "            'output_file': path to output file,\n",
    "            'dataframe': host_pages DataFrame (optional),\n",
    "            'total_pages': total unique pages analyzed\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from datetime import timedelta\n",
    "    import matplotlib.pyplot as plt\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    from tqdm import tqdm\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    max_records_per_file = 500000\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(\"üìñ Loading crawl data...\")\n",
    "    dfs = []\n",
    "    \n",
    "    if os.path.isdir(input_folder):\n",
    "        files = [f for f in os.listdir(input_folder) \n",
    "                if f.endswith(('.csv', '.xlsx', '.xls')) \n",
    "                and not f.startswith('~$')\n",
    "                and not f.startswith('.')]\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"‚ùå No CSV/Excel files found in folder: {input_folder}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': f'No CSV/Excel files found in folder: {input_folder}'\n",
    "            }\n",
    "            \n",
    "        print(f\"üìÅ Found {len(files)} file(s) in folder\")\n",
    "        \n",
    "        for file in tqdm(files, desc=\"Reading files\"):\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "            try:\n",
    "                if file.endswith('.csv'):\n",
    "                    temp_df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "                else:\n",
    "                    temp_df = pd.read_excel(file_path)\n",
    "                dfs.append(temp_df)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error reading {file}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"‚ùå No data loaded from folder\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': 'No data loaded from folder'\n",
    "            }\n",
    "            \n",
    "        print(\"   ‚îú‚îÄ Concatenating DataFrames...\")\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} records from {len(dfs)} file(s)\")\n",
    "        \n",
    "    else:\n",
    "        if not os.path.exists(input_folder):\n",
    "            print(f\"‚ùå File/folder not found: {input_folder}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': f'File/folder not found: {input_folder}'\n",
    "            }\n",
    "            \n",
    "        df = pd.read_excel(input_folder) if input_folder.endswith('.xlsx') else pd.read_csv(input_folder)\n",
    "        print(f\"‚úÖ Loaded {len(df):,} records from single file\")\n",
    "    \n",
    "    # Memory optimization\n",
    "    print(\"‚ö° Optimizing memory...\")\n",
    "    progress_mem = widgets.IntProgress(value=0, min=0, max=100, description='Memory:', bar_style='info')\n",
    "    display(progress_mem)\n",
    "    \n",
    "    progress_mem.value = 50\n",
    "    df = df[['request_uri', 'time_iso8601', 'http_user_agent', 'status']].copy()\n",
    "    progress_mem.value = 100\n",
    "    \n",
    "    # Vectorized URL cleaning\n",
    "    print(\"‚ö° Processing URLs...\")\n",
    "    progress_url = widgets.IntProgress(value=0, min=0, max=100, description='URLs:', bar_style='info')\n",
    "    display(progress_url)\n",
    "    \n",
    "    progress_url.value = 33\n",
    "    df['timestamp'] = pd.to_datetime(df['time_iso8601'])\n",
    "    progress_url.value = 66\n",
    "    df['url_clean'] = df['request_uri'].str.split('?').str[0]\n",
    "    df['url'] = 'https://www.alamy.com' + df['url_clean'].fillna('')\n",
    "    progress_url.value = 100\n",
    "    \n",
    "    # Drop intermediate columns\n",
    "    df = df[['url', 'timestamp', 'http_user_agent', 'status']].copy()\n",
    "    \n",
    "    min_date, max_date = df['timestamp'].min(), df['timestamp'].max()\n",
    "    period_days = (max_date - min_date).days + 1\n",
    "    total_pages = df['url'].nunique()\n",
    "    \n",
    "    print(f\"üìÖ Period: {min_date.date()} to {max_date.date()} ({period_days} days)\")\n",
    "    print(f\"üìä Total unique pages: {total_pages:,}\")\n",
    "    \n",
    "    print(\"\\n‚öôÔ∏è Calculating page statistics...\")\n",
    "    \n",
    "    # Step 1: First groupby - counts only\n",
    "    print(\"   ‚îú‚îÄ Step 1/6: Aggregating crawl counts (lightweight)...\")\n",
    "    progress_group1 = widgets.IntProgress(value=0, min=0, max=100, description='Counts:', bar_style='info')\n",
    "    display(progress_group1)\n",
    "    \n",
    "    progress_group1.value = 30\n",
    "    url_stats = df.groupby('url', sort=False).agg({\n",
    "        'timestamp': ['count', 'min', 'max']\n",
    "    }).reset_index()\n",
    "    url_stats.columns = ['url', 'crawl_count', 'first_crawl', 'last_crawl']\n",
    "    progress_group1.value = 100\n",
    "    print(f\"   ‚îÇ     ‚îî‚îÄ ‚úÖ Aggregated {len(url_stats):,} unique URLs\")\n",
    "    \n",
    "    # Step 2: Filter to only URLs with ‚â•3 crawls\n",
    "    print(\"   ‚îú‚îÄ Step 2/6: Pre-filtering URLs (‚â•3 crawls)...\")\n",
    "    progress_filter = widgets.IntProgress(value=0, min=0, max=100, description='Filtering:', bar_style='info')\n",
    "    display(progress_filter)\n",
    "    \n",
    "    progress_filter.value = 50\n",
    "    candidate_urls = url_stats[url_stats['crawl_count'] >= 3]['url'].tolist()\n",
    "    df_candidates = df[df['url'].isin(candidate_urls)].copy()\n",
    "    progress_filter.value = 100\n",
    "    print(f\"   ‚îÇ     ‚îî‚îÄ ‚úÖ Filtered to {len(candidate_urls):,} candidates ({len(df_candidates):,} records)\")\n",
    "    \n",
    "    # Step 3: Create timestamp lists\n",
    "    print(\"   ‚îú‚îÄ Step 3/6: Grouping timestamps (filtered set)...\")\n",
    "    progress_group2 = widgets.IntProgress(value=0, min=0, max=100, description='Timestamps:', bar_style='info')\n",
    "    display(progress_group2)\n",
    "    \n",
    "    progress_group2.value = 30\n",
    "    timestamp_groups = df_candidates.groupby('url', sort=False)['timestamp'].apply(list).reset_index()\n",
    "    timestamp_groups.columns = ['url', 'timestamps']\n",
    "    progress_group2.value = 100\n",
    "    print(f\"   ‚îÇ     ‚îî‚îÄ ‚úÖ Created timestamp lists for {len(timestamp_groups):,} URLs\")\n",
    "    \n",
    "    # Step 4: Get user agents\n",
    "    print(\"   ‚îú‚îÄ Step 4/6: Extracting user agents...\")\n",
    "    progress_ua = widgets.IntProgress(value=0, min=0, max=100, description='User Agents:', bar_style='info')\n",
    "    display(progress_ua)\n",
    "    \n",
    "    progress_ua.value = 50\n",
    "    user_agent_groups = df_candidates.groupby('url', sort=False)['http_user_agent'].apply(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    ).reset_index()\n",
    "    user_agent_groups.columns = ['url', 'user_agent']\n",
    "    progress_ua.value = 100\n",
    "    print(f\"   ‚îÇ     ‚îî‚îÄ ‚úÖ Extracted user agents for {len(user_agent_groups):,} URLs\")\n",
    "    \n",
    "    # Step 5: Extract status codes\n",
    "    print(\"   ‚îú‚îÄ Step 5/6: Extracting status codes...\")\n",
    "    progress_status = widgets.IntProgress(value=0, min=0, max=100, description='Status Codes:', bar_style='info')\n",
    "    display(progress_status)\n",
    "    \n",
    "    progress_status.value = 50\n",
    "    status_groups = df_candidates.groupby('url', sort=False)['status'].apply(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else 200\n",
    "    ).reset_index()\n",
    "    status_groups.columns = ['url', 'status_code']\n",
    "    progress_status.value = 100\n",
    "    print(f\"   ‚îÇ     ‚îî‚îÄ ‚úÖ Extracted status codes for {len(status_groups):,} URLs\")\n",
    "    \n",
    "    # Merge all stats\n",
    "    url_groups = url_stats.merge(timestamp_groups, on='url', how='left')\n",
    "    url_groups = url_groups.merge(user_agent_groups, on='url', how='left')\n",
    "    url_groups = url_groups.merge(status_groups, on='url', how='left')\n",
    "    url_groups['status_code'] = url_groups['status_code'].fillna(200)\n",
    "    \n",
    "    url_groups['crawl_frequency'] = url_groups['crawl_count'] / period_days\n",
    "    \n",
    "    # Step 6: Host eligibility check\n",
    "    print(f\"   ‚îî‚îÄ Step 6/6: Checking host eligibility (‚â•3 crawls + max {crawl_threshold}-day gap)...\")\n",
    "    \n",
    "    def check_host_eligibility_fast(timestamps, min_crawls=3, max_gap_days=crawl_threshold):\n",
    "        if timestamps is None or not isinstance(timestamps, list) or len(timestamps) < min_crawls:\n",
    "            return False\n",
    "        sorted_ts = sorted(timestamps)\n",
    "        gaps = [(sorted_ts[i+1] - sorted_ts[i]).days for i in range(len(sorted_ts) - 1)]\n",
    "        return all(gap <= max_gap_days for gap in gaps)\n",
    "    \n",
    "    tqdm.pandas(desc=\"   ‚îÇ     ‚îî‚îÄ Validating intervals\")\n",
    "    url_groups['meets_interval'] = url_groups['timestamps'].progress_apply(check_host_eligibility_fast)\n",
    "    \n",
    "    # Classification\n",
    "    print(\"\\n‚úÖ Classifying host pages...\")\n",
    "    url_groups['is_host'] = (url_groups['crawl_count'] >= 3) & (url_groups['meets_interval'])\n",
    "    \n",
    "    url_groups['authority_score'] = url_groups['crawl_count']\n",
    "    \n",
    "    host_pages = url_groups[url_groups['is_host']].copy()\n",
    "    host_pages = host_pages.sort_values('authority_score', ascending=False)\n",
    "    \n",
    "    print(f\"   ‚îî‚îÄ ‚úÖ Identified {len(host_pages):,} host pages ({len(host_pages)/total_pages*100:.2f}% of total)\")\n",
    "    \n",
    "    # Analyze Googlebot types\n",
    "    print(\"\\nü§ñ Analyzing Googlebot types...\")\n",
    "    \n",
    "    def extract_bot_type(user_agent):\n",
    "        if pd.isna(user_agent):\n",
    "            return 'Unknown'\n",
    "        \n",
    "        ua_str = str(user_agent).lower()\n",
    "        \n",
    "        if 'smartphone' in ua_str and 'googlebot' in ua_str:\n",
    "            return 'Googlebot-Smartphone'\n",
    "        elif 'googlebot-image' in ua_str:\n",
    "            return 'Googlebot-Image'\n",
    "        elif 'googlebot-video' in ua_str:\n",
    "            return 'Googlebot-Video'\n",
    "        elif 'googlebot-news' in ua_str:\n",
    "            return 'Googlebot-News'\n",
    "        elif 'adsbot' in ua_str:\n",
    "            return 'AdsBot-Google'\n",
    "        elif 'mediapartners' in ua_str:\n",
    "            return 'Mediapartners-Google'\n",
    "        elif 'mobile' in ua_str and 'googlebot' in ua_str:\n",
    "            return 'Googlebot-Mobile-Legacy' \n",
    "        elif 'googlebot' in ua_str:\n",
    "            return 'Googlebot-Desktop'\n",
    "        elif 'google' in ua_str:\n",
    "            return 'Other-Google-Bot'\n",
    "        else:\n",
    "            return 'Non-Google'\n",
    "    \n",
    "    df['bot_type'] = df['http_user_agent'].apply(extract_bot_type)\n",
    "    bot_stats = df.groupby('bot_type')['url'].nunique().reset_index()\n",
    "    bot_stats.columns = ['bot_type', 'unique_urls']\n",
    "    bot_stats = bot_stats.sort_values('unique_urls', ascending=False)\n",
    "    \n",
    "    print(f\"   ‚îî‚îÄ ‚úÖ Analyzed {len(bot_stats)} bot types\")\n",
    "    \n",
    "    # Visualizations\n",
    "    print(\"\\nüìä Generating visualizations...\")\n",
    "    progress_viz = widgets.IntProgress(value=0, min=0, max=100, description='Visuals:', bar_style='warning')\n",
    "    display(progress_viz)\n",
    "    \n",
    "    progress_viz.value = 20\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    host_urls = set(host_pages['url'].tolist())\n",
    "    df['is_host_crawl'] = df['url'].isin(host_urls)\n",
    "    \n",
    "    progress_viz.value = 40\n",
    "    daily_stats = df.groupby('date').agg({\n",
    "        'url': 'nunique',\n",
    "        'is_host_crawl': 'sum'\n",
    "    }).reset_index()\n",
    "    daily_stats.columns = ['date', 'total_pages_crawled', 'host_pages_crawled']\n",
    "    progress_viz.value = 60\n",
    "    \n",
    "    output1 = widgets.Output()\n",
    "    \n",
    "    with output1:\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        ax[0].plot(daily_stats['date'], daily_stats['total_pages_crawled'], \n",
    "                   label='Total Pages Crawled', linewidth=2, color='steelblue', alpha=0.7)\n",
    "        ax[0].plot(daily_stats['date'], daily_stats['host_pages_crawled'], \n",
    "                   label='Host Pages Crawled', linewidth=2, color='orangered')\n",
    "        ax[0].set_xlabel('Date', fontsize=12)\n",
    "        ax[0].set_ylabel('Page Count', fontsize=12)\n",
    "        ax[0].set_title('Host Pages vs Total Pages Crawled Over Time', fontsize=14, fontweight='bold')\n",
    "        ax[0].legend(loc='upper left', fontsize=10)\n",
    "        ax[0].grid(True, alpha=0.3)\n",
    "        plt.setp(ax[0].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        daily_stats['host_pct'] = (daily_stats['host_pages_crawled'] / daily_stats['total_pages_crawled']) * 100\n",
    "        ax[1].fill_between(daily_stats['date'], daily_stats['host_pct'], \n",
    "                           color='mediumseagreen', alpha=0.5, label='Host Page %')\n",
    "        ax[1].plot(daily_stats['date'], daily_stats['host_pct'], \n",
    "                   color='darkgreen', linewidth=2)\n",
    "        ax[1].axhline(y=daily_stats['host_pct'].mean(), color='red', \n",
    "                      linestyle='--', label=f'Avg: {daily_stats[\"host_pct\"].mean():.1f}%')\n",
    "        ax[1].set_xlabel('Date', fontsize=12)\n",
    "        ax[1].set_ylabel('Host Page %', fontsize=12)\n",
    "        ax[1].set_title('Host Page Crawl Percentage Over Time', fontsize=14, fontweight='bold')\n",
    "        ax[1].legend(loc='upper left', fontsize=10)\n",
    "        ax[1].grid(True, alpha=0.3)\n",
    "        plt.setp(ax[1].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    output2 = widgets.Output()\n",
    "    \n",
    "    with output2:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        top_10 = host_pages.head(10)\n",
    "        colors_gradient = plt.cm.Reds(np.linspace(0.4, 0.9, len(top_10)))\n",
    "        \n",
    "        ax[0].barh(range(len(top_10)), top_10['crawl_count'].values, color=colors_gradient)\n",
    "        ax[0].set_yticks(range(len(top_10)))\n",
    "        ax[0].set_yticklabels([f\"{i+1}. {url.split('/')[-1][:30]}...\" for i, url in enumerate(top_10['url'].values)], fontsize=9)\n",
    "        ax[0].set_xlabel('Crawl Count', fontsize=12, fontweight='bold')\n",
    "        ax[0].set_title('Top 10 Host Pages by Crawl Count', fontsize=14, fontweight='bold')\n",
    "        ax[0].invert_yaxis()\n",
    "        ax[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(top_10['crawl_count'].values):\n",
    "            ax[0].text(v + max(top_10['crawl_count'])*0.01, i, f'{v:.0f}', \n",
    "                      va='center', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        colors_bot = plt.cm.Blues(np.linspace(0.4, 0.9, len(bot_stats)))\n",
    "        \n",
    "        ax[1].bar(range(len(bot_stats)), bot_stats['unique_urls'].values, color=colors_bot, edgecolor='black', linewidth=1)\n",
    "        ax[1].set_xticks(range(len(bot_stats)))\n",
    "        ax[1].set_xticklabels(bot_stats['bot_type'].values, rotation=45, ha='right', fontsize=9)\n",
    "        ax[1].set_ylabel('Unique URLs Crawled', fontsize=12, fontweight='bold')\n",
    "        ax[1].set_title('Googlebot Types - URL Coverage', fontsize=14, fontweight='bold')\n",
    "        ax[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(bot_stats['unique_urls'].values):\n",
    "            ax[1].text(i, v + max(bot_stats['unique_urls'])*0.01, f'{v:,}', \n",
    "                      ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    progress_viz.value = 80\n",
    "    \n",
    "    output3 = widgets.Output()\n",
    "    \n",
    "    with output3:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "        categories = ['Host Pages', 'Regular Pages']\n",
    "        counts = [len(host_pages), total_pages - len(host_pages)]\n",
    "        colors = ['#ff6b6b', '#4ecdc4']\n",
    "        \n",
    "        ax.bar(categories, counts, color=colors, edgecolor='black', linewidth=1.5, width=0.6)\n",
    "        ax.set_ylabel('Page Count', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Host Pages vs Regular Pages Distribution', fontsize=14, fontweight='bold')\n",
    "        ax.text(0, counts[0] + max(counts)*0.02, f'{counts[0]:,}', \n",
    "                   ha='center', fontsize=12, fontweight='bold')\n",
    "        ax.text(1, counts[1] + max(counts)*0.02, f'{counts[1]:,}', \n",
    "                   ha='center', fontsize=12, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    progress_viz.value = 100\n",
    "    print(\"   ‚îî‚îÄ ‚úÖ Visualizations complete\")\n",
    "    \n",
    "    # Save output\n",
    "    print(\"\\nüíæ Saving output files...\")\n",
    "    progress_save = widgets.IntProgress(value=0, min=0, max=100, description='Saving:', bar_style='success')\n",
    "    display(progress_save)\n",
    "    \n",
    "    output_cols = ['url', 'crawl_count', 'crawl_frequency', 'user_agent', 'status_code']\n",
    "    \n",
    "    output_files = []\n",
    "    \n",
    "    if len(host_pages) <= max_records_per_file:\n",
    "        progress_save.value = 50\n",
    "        output_path = os.path.join(output_folder, 'host_pages.csv')\n",
    "        host_pages[output_cols].to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        output_files.append(output_path)\n",
    "        progress_save.value = 100\n",
    "        print(f\"   ‚îî‚îÄ Saved: {output_path}\")\n",
    "    else:\n",
    "        num_parts = (len(host_pages) // max_records_per_file) + 1\n",
    "        print(f\"   ‚îú‚îÄ Splitting into {num_parts} parts...\")\n",
    "        for i in tqdm(range(num_parts), desc=\"   ‚îÇ  ‚îî‚îÄ Writing files\"):\n",
    "            start_idx = i * max_records_per_file\n",
    "            end_idx = min((i + 1) * max_records_per_file, len(host_pages))\n",
    "            part_df = host_pages.iloc[start_idx:end_idx]\n",
    "            output_path = os.path.join(output_folder, f'host_pages_part{i+1}.csv')\n",
    "            part_df[output_cols].to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            output_files.append(output_path)\n",
    "            progress_save.value = int((i+1) / num_parts * 100)\n",
    "        print(f\"   ‚îî‚îÄ Saved {num_parts} files\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä PHASE 3.2 COMPLETE: HOST PAGE DETECTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Host Page Criteria: ‚â•3 crawls + max {crawl_threshold}-day gap\")\n",
    "    print(f\"‚úÖ Output: url, crawl_count, crawl_frequency, user_agent, status_code\")\n",
    "    print(f\"‚úÖ Analyzed {len(bot_stats)} different bot types\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    display(output1)\n",
    "    display(output2)\n",
    "    display(output3)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'host_pages': len(host_pages),\n",
    "        'total_pages': total_pages,\n",
    "        'output_file': output_files[0] if output_files else None,\n",
    "        'output_files': output_files,\n",
    "        'dataframe': host_pages\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a5635-ae9e-4d40-a669-8196e669354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_phase3_4(input_folder, output_folder, sitemap_urls, stale_threshold=30, \n",
    "                     percentile=10.0, workers=10, log_batch_size=5, test_mode=False):\n",
    "    \"\"\"\n",
    "    Phase 3.4: Stale Page Detection (Party Bouncer)\n",
    "    Identifies orphan pages and low-activity pages from sitemap analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_folder : str\n",
    "        Path to folder containing Phase 2 output (crawl log CSV files)\n",
    "    output_folder : str\n",
    "        Directory where stale_pages.csv will be saved\n",
    "    sitemap_urls : str or list\n",
    "        Sitemap index URL(s) - can be single URL or comma/newline separated list\n",
    "    stale_threshold : int\n",
    "        Days since last modification to consider stale (default: 30)\n",
    "    percentile : float\n",
    "        Bottom percentile for low activity detection (default: 10.0)\n",
    "    workers : int\n",
    "        Number of parallel workers for sitemap parsing (default: 10)\n",
    "    log_batch_size : int\n",
    "        Number of log files to process per batch (default: 5)\n",
    "    test_mode : bool\n",
    "        If True, limits to 10 gz files per index for testing (default: False)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            'status': 'success' or 'error',\n",
    "            'orphans': count of orphan pages,\n",
    "            'low_activity': count of low activity pages,\n",
    "            'total_stale': total stale pages,\n",
    "            'output_files': list of output file paths,\n",
    "            'dataframe': stale_pages DataFrame (optional)\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import requests\n",
    "    import xml.etree.ElementTree as ET\n",
    "    import gzip\n",
    "    from io import BytesIO\n",
    "    import os\n",
    "    import concurrent.futures\n",
    "    from tqdm.notebook import tqdm\n",
    "    import warnings\n",
    "    import gc\n",
    "    import csv\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Worker function for parallel sitemap parsing\n",
    "    def parse_gz_sitemap_worker(gz_url):\n",
    "        try:\n",
    "            response = requests.get(gz_url, timeout=60, stream=True)\n",
    "            if response.status_code != 200:\n",
    "                return []\n",
    "            with gzip.GzipFile(fileobj=BytesIO(response.content)) as gz:\n",
    "                xml_content = gz.read()\n",
    "            root = ET.fromstring(xml_content)\n",
    "            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            urls_data = []\n",
    "            for url_elem in root.findall('sm:url', ns):\n",
    "                loc = url_elem.find('sm:loc', ns)\n",
    "                lastmod = url_elem.find('sm:lastmod', ns)\n",
    "                if loc is not None:\n",
    "                    urls_data.append({\n",
    "                        'url': loc.text,\n",
    "                        'last_modified': lastmod.text if lastmod is not None else None\n",
    "                    })\n",
    "            return urls_data\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    max_records_per_file = 500000\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Parse sitemap URLs\n",
    "    if isinstance(sitemap_urls, str):\n",
    "        sitemap_list = [url.strip() for url in sitemap_urls.replace(',', '\\n').split('\\n') if url.strip()]\n",
    "    else:\n",
    "        sitemap_list = sitemap_urls\n",
    "    \n",
    "    if not sitemap_list:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': 'No sitemap URLs provided'\n",
    "        }\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üï∫ STALE PAGE DETECTION: THE PARTY BOUNCER (Parallel: {workers} workers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Log folder: {input_folder}\")\n",
    "    print(f\"üó∫Ô∏è  Sitemap indexes: {len(sitemap_list)}\")\n",
    "    print(f\"üìä Low activity threshold: Bottom {percentile}%\")\n",
    "    print(f\"‚è±Ô∏è  Stale threshold: {stale_threshold} days\")\n",
    "    \n",
    "    # STEP 1: Build guest list from logs\n",
    "    print(\"\\nüìñ Step 1: Building the Guest List (Processing Logs)...\")\n",
    "    \n",
    "    if not os.path.exists(input_folder):\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Folder not found: {input_folder}'\n",
    "        }\n",
    "\n",
    "    log_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "    \n",
    "    if not log_files:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': 'No CSV files found in input folder'\n",
    "        }\n",
    "\n",
    "    all_log_stats = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(log_files), log_batch_size), desc=\"   Processing Logs\"):\n",
    "        batch_files = log_files[i:i+log_batch_size]\n",
    "        dfs = []\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(input_folder, file), \n",
    "                               usecols=['request_uri', 'http_user_agent', 'time_iso8601', 'status'], \n",
    "                               encoding='utf-8-sig', low_memory=False)\n",
    "                dfs.append(df)\n",
    "            except: \n",
    "                continue\n",
    "            \n",
    "        if dfs:\n",
    "            batch_df = pd.concat(dfs)\n",
    "            batch_df['url'] = 'https://www.alamy.com' + batch_df['request_uri'].str.split('?').str[0].fillna('')\n",
    "            batch_df['timestamp'] = pd.to_datetime(batch_df['time_iso8601'], errors='coerce')\n",
    "            \n",
    "            min_date = batch_df['timestamp'].min()\n",
    "            max_date = batch_df['timestamp'].max()\n",
    "            days = (max_date - min_date).days + 1 if pd.notnull(max_date) else 1\n",
    "            \n",
    "            batch_stats = batch_df.groupby('url').agg({\n",
    "                'request_uri': 'count',\n",
    "                'http_user_agent': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "                'status': lambda x: x.mode()[0] if len(x.mode()) > 0 else 200\n",
    "            }).reset_index()\n",
    "            batch_stats.columns = ['url', 'crawl_count', 'user_agent', 'status_code']\n",
    "            batch_stats['days_active'] = days\n",
    "            \n",
    "            all_log_stats.append(batch_stats)\n",
    "            del batch_df, dfs\n",
    "            gc.collect()\n",
    "\n",
    "    if not all_log_stats:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': 'No log data processed'\n",
    "        }\n",
    "\n",
    "    print(\"   ‚îú‚îÄ Finalizing Guest List...\")\n",
    "    full_log_stats = pd.concat(all_log_stats).groupby('url').agg({\n",
    "        'crawl_count': 'sum',\n",
    "        'user_agent': 'first',\n",
    "        'status_code': 'first',\n",
    "        'days_active': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    full_log_stats['crawl_frequency'] = full_log_stats['crawl_count'] / full_log_stats['days_active']\n",
    "    full_log_stats['authority_score'] = (full_log_stats['crawl_count'] * 0.7 + \n",
    "                                         full_log_stats['crawl_frequency'] * full_log_stats['days_active'] * 0.3)\n",
    "    \n",
    "    threshold = full_log_stats['crawl_count'].quantile(percentile / 100.0)\n",
    "    print(f\"   ‚îú‚îÄ Bottom {percentile}% Threshold: <= {threshold} crawls\")\n",
    "    \n",
    "    invited_guests = set(full_log_stats['url'])\n",
    "    wallflowers_df = full_log_stats[full_log_stats['crawl_count'] <= threshold]\n",
    "    wallflowers = set(wallflowers_df['url'])\n",
    "    \n",
    "    print(f\"‚úÖ Guest List Ready: {len(invited_guests):,} total, {len(wallflowers):,} low activity\")\n",
    "\n",
    "    # STEP 2: Parse sitemaps\n",
    "    print(\"\\nüì• Step 2: Checking Sitemaps at the Gate (Parallel)...\")\n",
    "    \n",
    "    def parse_sitemap_index(index_url):\n",
    "        try:\n",
    "            response = requests.get(index_url, timeout=60)\n",
    "            root = ET.fromstring(response.content)\n",
    "            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            return [loc.text for loc in root.findall('sm:sitemap/sm:loc', ns)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    stale_party_list = []\n",
    "    total_processed_urls = 0\n",
    "    \n",
    "    def check_at_gate(batch_df):\n",
    "        batch_df['is_invited'] = batch_df['url'].isin(invited_guests)\n",
    "        batch_df['is_wallflower'] = batch_df['url'].isin(wallflowers)\n",
    "        \n",
    "        orphans = batch_df[~batch_df['is_invited']].copy()\n",
    "        orphans['page_type'] = 'Orphan'\n",
    "        orphans['crawl_count'] = 0\n",
    "        orphans['authority_score'] = 0.0\n",
    "        orphans['crawl_frequency'] = 0.0\n",
    "        orphans['user_agent'] = 'Not Crawled'\n",
    "        orphans['status_code'] = 0\n",
    "        \n",
    "        low_act = batch_df[batch_df['is_wallflower']].copy()\n",
    "        low_act['page_type'] = 'Low Activity'\n",
    "        \n",
    "        if not low_act.empty:\n",
    "            low_act = low_act.merge(\n",
    "                full_log_stats[['url', 'crawl_count', 'authority_score', 'crawl_frequency', 'user_agent', 'status_code']], \n",
    "                on='url', how='left'\n",
    "            )\n",
    "            low_act['status_code'] = low_act['status_code'].fillna(200)\n",
    "        \n",
    "        return pd.concat([orphans, low_act])\n",
    "\n",
    "    max_gz = 10 if test_mode else None\n",
    "    \n",
    "    for idx_num, index_url in enumerate(sitemap_list, 1):\n",
    "        print(f\"\\n   ‚îú‚îÄ Index {idx_num}/{len(sitemap_list)}: {index_url.split('/')[-1]}\")\n",
    "        gz_urls = parse_sitemap_index(index_url)\n",
    "        if not gz_urls: \n",
    "            continue\n",
    "        if max_gz: \n",
    "            gz_urls = gz_urls[:max_gz]\n",
    "            \n",
    "        print(f\"   ‚îÇ  ‚îú‚îÄ Spawning {workers} workers for {len(gz_urls)} files...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = {executor.submit(parse_gz_sitemap_worker, url): url for url in gz_urls}\n",
    "            batch_results = []\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(gz_urls), desc=\"   ‚îÇ  ‚îî‚îÄ Parsing\", leave=False):\n",
    "                result = future.result()\n",
    "                if result: \n",
    "                    batch_results.extend(result)\n",
    "                \n",
    "                if len(batch_results) > 200000:\n",
    "                    batch_df = pd.DataFrame(batch_results)\n",
    "                    total_processed_urls += len(batch_df)\n",
    "                    processed_batch = check_at_gate(batch_df)\n",
    "                    if not processed_batch.empty:\n",
    "                        stale_party_list.append(processed_batch)\n",
    "                    batch_results = []\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "            \n",
    "            if batch_results:\n",
    "                batch_df = pd.DataFrame(batch_results)\n",
    "                total_processed_urls += len(batch_df)\n",
    "                processed_batch = check_at_gate(batch_df)\n",
    "                if not processed_batch.empty:\n",
    "                    stale_party_list.append(processed_batch)\n",
    "                del batch_df\n",
    "                gc.collect()\n",
    "\n",
    "    # STEP 3: Save output\n",
    "    print(\"\\nüíæ Step 3: Consolidating and Saving...\")\n",
    "    \n",
    "    if not stale_party_list:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': 'No stale pages found'\n",
    "        }\n",
    "        \n",
    "    final_df = pd.concat(stale_party_list, ignore_index=True)\n",
    "    \n",
    "    current_date = pd.Timestamp.now()\n",
    "    final_df['last_modified'] = pd.to_datetime(final_df['last_modified'], errors='coerce')\n",
    "    final_df['days_since_modified'] = (current_date - final_df['last_modified']).dt.days\n",
    "    \n",
    "    final_df['priority_score'] = 0\n",
    "    final_df.loc[final_df['days_since_modified'] > 180, 'priority_score'] = 100\n",
    "    final_df.loc[(final_df['days_since_modified'] > 90) & (final_df['days_since_modified'] <= 180), 'priority_score'] = 70\n",
    "    final_df.loc[final_df['days_since_modified'] <= 90, 'priority_score'] = 40\n",
    "    \n",
    "    final_df = final_df.sort_values(['page_type', 'priority_score'], ascending=[False, False])\n",
    "    \n",
    "    output_cols = ['url', 'crawl_count', 'authority_score', 'crawl_frequency', 'user_agent', \n",
    "                   'status_code', 'page_type', 'priority_score', 'days_since_modified', 'last_modified']\n",
    "    \n",
    "    final_cols = [c for c in output_cols if c in final_df.columns]\n",
    "    final_df = final_df[final_cols]\n",
    "    \n",
    "    output_files = []\n",
    "    \n",
    "    if len(final_df) <= max_records_per_file:\n",
    "        output_path = os.path.join(output_folder, 'stale_pages.csv')\n",
    "        final_df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "        output_files.append(output_path)\n",
    "        print(f\"   ‚îî‚îÄ Saved: {output_path}\")\n",
    "    else:\n",
    "        num_parts = (len(final_df) // max_records_per_file) + 1\n",
    "        for i in range(num_parts):\n",
    "            start_idx = i * max_records_per_file\n",
    "            end_idx = min((i + 1) * max_records_per_file, len(final_df))\n",
    "            part_df = final_df.iloc[start_idx:end_idx]\n",
    "            output_path = os.path.join(output_folder, f'stale_pages_part{i+1}.csv')\n",
    "            part_df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "            output_files.append(output_path)\n",
    "        print(f\"   ‚îî‚îÄ Saved {num_parts} files\")\n",
    "\n",
    "    orphan_count = len(final_df[final_df['page_type']=='Orphan'])\n",
    "    low_activity_count = len(final_df[final_df['page_type']=='Low Activity'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL STATS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Total Analyzed: {total_processed_urls:,}\")\n",
    "    print(f\"‚úÖ Stale Found: {len(final_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Orphans: {orphan_count:,}\")\n",
    "    print(f\"   ‚Ä¢ Low Activity: {low_activity_count:,}\")\n",
    "    print(f\"üìÅ Output Location: {os.path.abspath(output_folder)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'orphans': orphan_count,\n",
    "        'low_activity': low_activity_count,\n",
    "        'total_stale': len(final_df),\n",
    "        'output_files': output_files,\n",
    "        'dataframe': final_df\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49138197-fb56-4681-9aa8-342b58ac2815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_phase4_1(input_folder, output_folder, model_name='all-MiniLM-L6-v2', batch_size=32):\n",
    "    \"\"\"\n",
    "    Phase 4.1: Generate Semantic Embeddings\n",
    "    Creates vector embeddings for host and stale pages for similarity computation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_folder : str\n",
    "        Path to folder containing host_pages.csv and stale_pages*.csv from Phase 3\n",
    "    output_folder : str\n",
    "        Directory where embeddings will be saved\n",
    "    model_name : str\n",
    "        SentenceTransformer model name (default: 'all-MiniLM-L6-v2')\n",
    "    batch_size : int\n",
    "        Batch size for encoding (default: 32)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            'status': 'success' or 'error',\n",
    "            'host_count': number of host embeddings,\n",
    "            'stale_count': number of stale embeddings,\n",
    "            'output_folder': path to output folder,\n",
    "            'output_files': list of generated files\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from urllib.parse import urlparse\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import glob\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Phase 4.1: Semantic Embedding Generation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 1: Load Phase 3 Data\n",
    "    # ========================================\n",
    "    print(\"\\n[1/5] Loading Phase 3 data...\")\n",
    "    \n",
    "    # Load host pages\n",
    "    host_pages_path = os.path.join(input_folder, 'host_pages.csv')\n",
    "    \n",
    "    if not os.path.exists(host_pages_path):\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Host pages file not found: {host_pages_path}'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        host_pages = pd.read_csv(host_pages_path)\n",
    "        print(f\"‚úì Loaded {len(host_pages):,} host pages from {host_pages_path}\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to load host_pages.csv: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # Load stale pages (handle single file or multiple parts)\n",
    "    stale_pages_files = []\n",
    "    \n",
    "    # Check for single file\n",
    "    single_file = os.path.join(input_folder, 'stale_pages.csv')\n",
    "    if os.path.exists(single_file):\n",
    "        stale_pages_files.append(single_file)\n",
    "    else:\n",
    "        # Check for multiple parts (stale_pages_part*.csv)\n",
    "        pattern = os.path.join(input_folder, 'stale_pages_part*.csv')\n",
    "        stale_pages_files = sorted(glob.glob(pattern))\n",
    "    \n",
    "    if not stale_pages_files:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'No stale pages files found in {input_folder}'\n",
    "        }\n",
    "    \n",
    "    # Load and concatenate all stale page files\n",
    "    print(f\"  Found {len(stale_pages_files)} stale page file(s):\")\n",
    "    stale_pages_list = []\n",
    "    for file in stale_pages_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            stale_pages_list.append(df)\n",
    "            print(f\"    - {os.path.basename(file)}: {len(df):,} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Error reading {os.path.basename(file)}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if not stale_pages_list:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': 'Failed to load any stale pages files'\n",
    "        }\n",
    "    \n",
    "    stale_pages = pd.concat(stale_pages_list, ignore_index=True)\n",
    "    print(f\"‚úì Loaded total {len(stale_pages):,} stale pages (combined)\")\n",
    "    \n",
    "    # Remove duplicates if any\n",
    "    original_count = len(stale_pages)\n",
    "    stale_pages = stale_pages.drop_duplicates(subset=['url'], keep='first')\n",
    "    if len(stale_pages) < original_count:\n",
    "        print(f\"  ‚Ñπ Removed {original_count - len(stale_pages):,} duplicate URLs\")\n",
    "    \n",
    "    print(f\"\\n‚úì Configuration complete\")\n",
    "    print(f\"  Input folder: {input_folder}\")\n",
    "    print(f\"  Output folder: {output_folder}\")\n",
    "    print(f\"  Model: {model_name}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 2: Extract Text from URLs\n",
    "    # ========================================\n",
    "    print(\"\\n[2/5] Extracting text from URLs...\")\n",
    "    \n",
    "    def extract_text_from_url(url):\n",
    "        \"\"\"\n",
    "        Extract meaningful keywords from URL path for embedding.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            parsed = urlparse(str(url))\n",
    "            path = parsed.path.strip('/')\n",
    "            \n",
    "            # Remove common file extensions\n",
    "            path = path.replace('.html', '').replace('.php', '').replace('.htm', '')\n",
    "            \n",
    "            # Replace separators with spaces\n",
    "            text = path.replace('/', ' ').replace('-', ' ').replace('_', ' ')\n",
    "            \n",
    "            # Clean whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            \n",
    "            return text if text else 'unknown page'\n",
    "        except:\n",
    "            return 'unknown page'\n",
    "    \n",
    "    # Apply to both datasets\n",
    "    host_pages['text'] = host_pages['url'].apply(extract_text_from_url)\n",
    "    stale_pages['text'] = stale_pages['url'].apply(extract_text_from_url)\n",
    "    \n",
    "    print(f\"‚úì Extracted text for {len(host_pages):,} host pages\")\n",
    "    print(f\"‚úì Extracted text for {len(stale_pages):,} stale pages\")\n",
    "    \n",
    "    print(\"\\nText extraction examples:\")\n",
    "    for i in range(min(3, len(host_pages))):\n",
    "        print(f\"  URL:  {host_pages.iloc[i]['url'][:60]}...\")\n",
    "        print(f\"  Text: {host_pages.iloc[i]['text']}\")\n",
    "        print()\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 3: Load Embedding Model\n",
    "    # ========================================\n",
    "    print(\"\\n[3/5] Loading embedding model...\")\n",
    "    \n",
    "    print(f\"  Model: {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"‚úì Model loaded\")\n",
    "        print(f\"  Embedding dimensions: {model.get_sentence_embedding_dimension()}\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to load model {model_name}: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 4: Generate Embeddings\n",
    "    # ========================================\n",
    "    print(\"\\n[4/5] Generating embeddings...\")\n",
    "    \n",
    "    # Generate host embeddings\n",
    "    print(f\"\\n  ‚Üí Computing embeddings for {len(host_pages):,} host pages...\")\n",
    "    try:\n",
    "        host_embeddings = model.encode(\n",
    "            host_pages['text'].tolist(),\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        print(f\"  ‚úì Host embeddings shape: {host_embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to generate host embeddings: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # Generate stale embeddings\n",
    "    print(f\"\\n  ‚Üí Computing embeddings for {len(stale_pages):,} stale pages...\")\n",
    "    try:\n",
    "        stale_embeddings = model.encode(\n",
    "            stale_pages['text'].tolist(),\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        print(f\"  ‚úì Stale embeddings shape: {stale_embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to generate stale embeddings: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 5: Save Embeddings & Metadata\n",
    "    # ========================================\n",
    "    print(\"\\n[5/5] Saving embeddings to disk...\")\n",
    "    \n",
    "    output_files = []\n",
    "    \n",
    "    try:\n",
    "        # Save embeddings as NumPy arrays\n",
    "        host_emb_path = os.path.join(output_folder, 'host_embeddings.npy')\n",
    "        stale_emb_path = os.path.join(output_folder, 'stale_embeddings.npy')\n",
    "        \n",
    "        np.save(host_emb_path, host_embeddings)\n",
    "        np.save(stale_emb_path, stale_embeddings)\n",
    "        \n",
    "        output_files.extend([host_emb_path, stale_emb_path])\n",
    "        \n",
    "        print(f\"‚úì Saved host_embeddings.npy ({host_embeddings.nbytes / 1024**2:.2f} MB)\")\n",
    "        print(f\"‚úì Saved stale_embeddings.npy ({stale_embeddings.nbytes / 1024**2:.2f} MB)\")\n",
    "        \n",
    "        # Save the combined stale_pages for Phase 4.2\n",
    "        stale_combined_path = os.path.join(output_folder, 'stale_pages_combined.csv')\n",
    "        stale_pages.to_csv(stale_combined_path, index=False)\n",
    "        output_files.append(stale_combined_path)\n",
    "        print(f\"‚úì Saved stale_pages_combined.csv (for Phase 4.2)\")\n",
    "        \n",
    "        # Save metadata for reproducibility\n",
    "        metadata = {\n",
    "            \"model_name\": model_name,\n",
    "            \"embedding_dimensions\": int(model.get_sentence_embedding_dimension()),\n",
    "            \"host_pages_count\": int(len(host_pages)),\n",
    "            \"stale_pages_count\": int(len(stale_pages)),\n",
    "            \"stale_pages_source_files\": [os.path.basename(f) for f in stale_pages_files],\n",
    "            \"batch_size\": batch_size,\n",
    "            \"normalized\": True,\n",
    "            \"generated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"input_files\": {\n",
    "                \"host_pages\": host_pages_path,\n",
    "                \"stale_pages\": stale_pages_files\n",
    "            },\n",
    "            \"output_files\": {\n",
    "                \"host_embeddings\": host_emb_path,\n",
    "                \"stale_embeddings\": stale_emb_path,\n",
    "                \"stale_pages_combined\": stale_combined_path\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(output_folder, 'embedding_metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        output_files.append(metadata_path)\n",
    "        print(\"‚úì Saved embedding_metadata.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to save embeddings: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 6: Verification & Summary\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Phase 4.1 Complete - Embedding Generation Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nGenerated Files:\")\n",
    "    for filepath in output_files:\n",
    "        size_mb = os.path.getsize(filepath) / 1024**2\n",
    "        print(f\"  ‚úì {os.path.basename(filepath)} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(\"\\nSanity Checks:\")\n",
    "    print(f\"  Host embeddings shape: {host_embeddings.shape}\")\n",
    "    print(f\"  Stale embeddings shape: {stale_embeddings.shape}\")\n",
    "    print(f\"  Sample embedding (first 5 dims): {host_embeddings[0][:5]}\")\n",
    "    print(f\"  Embedding norm (should be ~1.0): {np.linalg.norm(host_embeddings[0]):.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Ready for Phase 4.2 (Similarity Matrix Computation)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'host_count': len(host_pages),\n",
    "        'stale_count': len(stale_pages),\n",
    "        'output_folder': os.path.abspath(output_folder),\n",
    "        'output_files': output_files,\n",
    "        'embedding_dimensions': int(model.get_sentence_embedding_dimension())\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b57db-9def-4ac8-b027-d29962176e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_phase4_2(input_folder, output_folder, top_k=10, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Phase 4.2: Compute Top-K Similarity Matrix\n",
    "    Finds the most similar host pages for each stale page using cosine similarity\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_folder : str\n",
    "        Path to folder containing host_embeddings.npy and stale_embeddings.npy from Phase 4.1\n",
    "    output_folder : str\n",
    "        Directory where similarity results will be saved (can be same as input_folder)\n",
    "    top_k : int\n",
    "        Number of top similar host pages to return per stale page (default: 10)\n",
    "    batch_size : int\n",
    "        Batch size for similarity computation to manage memory (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            'status': 'success' or 'error',\n",
    "            'similarity_pairs': total number of similarity pairs,\n",
    "            'output_file': path to top_k_similarities.csv,\n",
    "            'avg_similarity': average similarity score,\n",
    "            'similarity_range': dict with min, max, mean, median\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    import gc\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Phase 4.2: Similarity Matrix Computation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n‚úì Configuration:\")\n",
    "    print(f\"  Input folder: {input_folder}\")\n",
    "    print(f\"  Output folder: {output_folder}\")\n",
    "    print(f\"  Top-K: {top_k}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 1: Load Embeddings & Metadata\n",
    "    # ========================================\n",
    "    print(\"\\n[1/4] Loading embeddings...\")\n",
    "    \n",
    "    host_emb_path = os.path.join(input_folder, 'host_embeddings.npy')\n",
    "    stale_emb_path = os.path.join(input_folder, 'stale_embeddings.npy')\n",
    "    metadata_path = os.path.join(input_folder, 'embedding_metadata.json')\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(host_emb_path):\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Host embeddings not found: {host_emb_path}'\n",
    "        }\n",
    "    \n",
    "    if not os.path.exists(stale_emb_path):\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Stale embeddings not found: {stale_emb_path}'\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        host_embeddings = np.load(host_emb_path)\n",
    "        stale_embeddings = np.load(stale_emb_path)\n",
    "        \n",
    "        print(f\"‚úì Host embeddings: {host_embeddings.shape}\")\n",
    "        print(f\"‚úì Stale embeddings: {stale_embeddings.shape}\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to load embeddings: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # Load metadata if exists\n",
    "    if os.path.exists(metadata_path):\n",
    "        try:\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "            print(f\"‚úì Model used: {metadata['model_name']}\")\n",
    "            print(f\"‚úì Embedding dimensions: {metadata['embedding_dimensions']}\")\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è Could not load metadata\")\n",
    "            metadata = {}\n",
    "    else:\n",
    "        metadata = {}\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 2: Compute Cosine Similarities (Batched)\n",
    "    # ========================================\n",
    "    print(f\"\\n[2/4] Computing cosine similarities (batched)...\")\n",
    "    print(f\"  Processing {len(stale_embeddings):,} stale pages in batches of {batch_size}\")\n",
    "    \n",
    "    # Storage for top-K results\n",
    "    all_stale_indices = []\n",
    "    all_host_indices = []\n",
    "    all_similarities = []\n",
    "    \n",
    "    num_stale = len(stale_embeddings)\n",
    "    num_batches = (num_stale + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"  Total batches: {num_batches}\")\n",
    "    \n",
    "    try:\n",
    "        for batch_idx in tqdm(range(num_batches), desc=\"  Computing similarities\"):\n",
    "            # Get batch of stale embeddings\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, num_stale)\n",
    "            \n",
    "            stale_batch = stale_embeddings[start_idx:end_idx]\n",
    "            \n",
    "            # Compute cosine similarity: stale_batch @ host_embeddings.T\n",
    "            # Shape: (batch_size, num_hosts)\n",
    "            similarities = np.dot(stale_batch, host_embeddings.T)\n",
    "            \n",
    "            # For each stale page, get top-K host indices\n",
    "            for i, sim_row in enumerate(similarities):\n",
    "                stale_idx = start_idx + i\n",
    "                \n",
    "                # Get top-K indices (use argpartition for efficiency)\n",
    "                if len(sim_row) <= top_k:\n",
    "                    top_k_indices = np.argsort(sim_row)[::-1]\n",
    "                else:\n",
    "                    # Faster than full sort for large arrays\n",
    "                    top_k_indices = np.argpartition(sim_row, -top_k)[-top_k:]\n",
    "                    top_k_indices = top_k_indices[np.argsort(sim_row[top_k_indices])[::-1]]\n",
    "                \n",
    "                top_k_scores = sim_row[top_k_indices]\n",
    "                \n",
    "                # Store results\n",
    "                all_stale_indices.extend([stale_idx] * top_k)\n",
    "                all_host_indices.extend(top_k_indices)\n",
    "                all_similarities.extend(top_k_scores)\n",
    "            \n",
    "            # Memory cleanup every 10 batches\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"‚úì Computed {len(all_similarities):,} similarity pairs\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to compute similarities: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 3: Create Results DataFrame\n",
    "    # ========================================\n",
    "    print(\"\\n[3/4] Creating results dataframe...\")\n",
    "    \n",
    "    try:\n",
    "        results_df = pd.DataFrame({\n",
    "            'stale_idx': all_stale_indices,\n",
    "            'host_idx': all_host_indices,\n",
    "            'similarity_score': all_similarities\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úì Results shape: {results_df.shape}\")\n",
    "        print(f\"‚úì Similarity score range: [{results_df['similarity_score'].min():.4f}, {results_df['similarity_score'].max():.4f}]\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(\"\\nSimilarity Score Distribution:\")\n",
    "        print(results_df['similarity_score'].describe())\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to create results dataframe: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 4: Save Results\n",
    "    # ========================================\n",
    "    print(\"\\n[4/4] Saving results...\")\n",
    "    \n",
    "    try:\n",
    "        output_file = os.path.join(output_folder, 'top_k_similarities.csv')\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        file_size_mb = os.path.getsize(output_file) / 1024**2\n",
    "        print(f\"‚úì Saved: {output_file} ({file_size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Save similarity computation metadata\n",
    "        similarity_metadata = {\n",
    "            \"top_k\": top_k,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_stale_pages\": int(num_stale),\n",
    "            \"num_host_pages\": int(len(host_embeddings)),\n",
    "            \"total_pairs\": int(len(results_df)),\n",
    "            \"similarity_range\": {\n",
    "                \"min\": float(results_df['similarity_score'].min()),\n",
    "                \"max\": float(results_df['similarity_score'].max()),\n",
    "                \"mean\": float(results_df['similarity_score'].mean()),\n",
    "                \"median\": float(results_df['similarity_score'].median())\n",
    "            },\n",
    "            \"input_files\": {\n",
    "                \"host_embeddings\": host_emb_path,\n",
    "                \"stale_embeddings\": stale_emb_path\n",
    "            },\n",
    "            \"output_file\": output_file\n",
    "        }\n",
    "        \n",
    "        metadata_file = os.path.join(output_folder, 'similarity_metadata.json')\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(similarity_metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úì Saved: {metadata_file}\")\n",
    "        \n",
    "        # Sample output\n",
    "        print(\"\\nSample results (first 5 rows):\")\n",
    "        print(results_df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to save results: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Phase 4.2 Complete - Similarity Matrix Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚úÖ Total similarity pairs: {len(results_df):,}\")\n",
    "    print(f\"‚úÖ Top-{top_k} matches per stale page\")\n",
    "    print(f\"‚úÖ Average similarity: {results_df['similarity_score'].mean():.4f}\")\n",
    "    print(f\"‚úÖ Output: {output_file}\")\n",
    "    print(\"\\n‚úÖ Ready for Phase 4.3 (Scoring & Ranking)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'similarity_pairs': len(results_df),\n",
    "        'output_file': output_file,\n",
    "        'avg_similarity': float(results_df['similarity_score'].mean()),\n",
    "        'similarity_range': {\n",
    "            'min': float(results_df['similarity_score'].min()),\n",
    "            'max': float(results_df['similarity_score'].max()),\n",
    "            'mean': float(results_df['similarity_score'].mean()),\n",
    "            'median': float(results_df['similarity_score'].median())\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e15f297-4994-4766-9a01-c6dc8478e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_phase4_3(phase3_folder, phase4_folder, output_folder, \n",
    "                     weight_relevance=0.55, weight_authority=0.25, \n",
    "                     weight_crawl=0.10, weight_urgency=0.10,\n",
    "                     max_rows_per_file=500000):\n",
    "    \"\"\"\n",
    "    Phase 4.3: Scoring, Ranking & Link Recommendations\n",
    "    Generates final link recommendations with scoring and placement zones\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    phase3_folder : str\n",
    "        Path to Phase 3 output folder (contains host_pages.csv)\n",
    "    phase4_folder : str\n",
    "        Path to Phase 4.1/4.2 output folder (contains embeddings and similarities)\n",
    "    output_folder : str\n",
    "        Directory where Phase 4.3 deliverables will be saved\n",
    "    weight_relevance : float\n",
    "        Weight for relevance score (default: 0.55)\n",
    "    weight_authority : float\n",
    "        Weight for authority score (default: 0.25)\n",
    "    weight_crawl : float\n",
    "        Weight for crawl frequency (default: 0.10)\n",
    "    weight_urgency : float\n",
    "        Weight for urgency score (default: 0.10)\n",
    "    max_rows_per_file : int\n",
    "        Maximum rows per CSV file for splitting (default: 500000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            'status': 'success' or 'error',\n",
    "            'total_recommendations': total link recommendations,\n",
    "            'avg_score': average final score,\n",
    "            'deliverables': list of generated files,\n",
    "            'output_folder': path to output folder\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    from tqdm import tqdm\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Phase 4.3: Scoring, Ranking & Link Recommendations\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create output folder\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Validate weights\n",
    "    total_weight = weight_relevance + weight_authority + weight_crawl + weight_urgency\n",
    "    if abs(total_weight - 1.0) > 0.01:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Weights sum to {total_weight:.2f}, normalizing to 1.0...\")\n",
    "        norm_factor = 1.0 / total_weight\n",
    "        weight_relevance *= norm_factor\n",
    "        weight_authority *= norm_factor\n",
    "        weight_crawl *= norm_factor\n",
    "        weight_urgency *= norm_factor\n",
    "    \n",
    "    print(f\"\\n‚úì Configuration:\")\n",
    "    print(f\"  Phase 3 folder: {phase3_folder}\")\n",
    "    print(f\"  Phase 4 folder: {phase4_folder}\")\n",
    "    print(f\"  Output folder: {output_folder}\")\n",
    "    print(f\"  Weights: R={weight_relevance:.2f}, A={weight_authority:.2f}, C={weight_crawl:.2f}, U={weight_urgency:.2f}\")\n",
    "    print(f\"  Max rows per file: {max_rows_per_file:,}\")\n",
    "    \n",
    "    # Check required files\n",
    "    required_files = {\n",
    "        'similarities': os.path.join(phase4_folder, 'top_k_similarities.csv'),\n",
    "        'stale_pages': os.path.join(phase4_folder, 'stale_pages_combined.csv'),\n",
    "        'host_pages': os.path.join(phase3_folder, 'host_pages.csv')\n",
    "    }\n",
    "    \n",
    "    for name, path in required_files.items():\n",
    "        if not os.path.exists(path):\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'error': f'{name} not found: {path}'\n",
    "            }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 1: Load Data\n",
    "    # ========================================\n",
    "    print(\"\\n[1/6] Loading data...\")\n",
    "    \n",
    "    try:\n",
    "        similarities = pd.read_csv(required_files['similarities'])\n",
    "        print(f\"‚úì Loaded {len(similarities):,} similarity pairs\")\n",
    "        \n",
    "        host_pages = pd.read_csv(required_files['host_pages'])\n",
    "        print(f\"‚úì Loaded {len(host_pages):,} host pages\")\n",
    "        \n",
    "        stale_pages = pd.read_csv(required_files['stale_pages'])\n",
    "        print(f\"‚úì Loaded {len(stale_pages):,} stale pages\")\n",
    "        \n",
    "        # Add index columns\n",
    "        host_pages['host_idx'] = range(len(host_pages))\n",
    "        stale_pages['stale_idx'] = range(len(stale_pages))\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to load data: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 2: Normalize Scores\n",
    "    # ========================================\n",
    "    print(\"\\n[2/6] Normalizing authority and crawl metrics...\")\n",
    "    \n",
    "    try:\n",
    "        host_pages['authority_norm'] = host_pages['crawl_count'] / host_pages['crawl_count'].max()\n",
    "        host_pages['crawl_norm'] = host_pages['crawl_frequency'] / host_pages['crawl_frequency'].max()\n",
    "        \n",
    "        print(f\"‚úì Authority range: [{host_pages['authority_norm'].min():.4f}, {host_pages['authority_norm'].max():.4f}]\")\n",
    "        print(f\"‚úì Crawl frequency range: [{host_pages['crawl_norm'].min():.4f}, {host_pages['crawl_norm'].max():.4f}]\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to normalize scores: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 3: Calculate Urgency Score\n",
    "    # ========================================\n",
    "    print(\"\\n[3/6] Calculating urgency scores...\")\n",
    "    \n",
    "    def calculate_urgency(row):\n",
    "        \"\"\"Calculate urgency (0-1) based on status_code, page_type, days_since_modified\"\"\"\n",
    "        urgency = 0.0\n",
    "        \n",
    "        status = row.get('status_code', 0)\n",
    "        if status in [404, 410]:\n",
    "            urgency += 0.4\n",
    "        elif status in [301, 302, 500, 502, 503]:\n",
    "            urgency += 0.3\n",
    "        elif status == 200:\n",
    "            urgency += 0.05\n",
    "        elif status == 0:\n",
    "            urgency += 0.35\n",
    "        \n",
    "        if row.get('page_type') == 'Orphan':\n",
    "            urgency += 0.4\n",
    "        elif row.get('crawl_count', 0) <= 1:\n",
    "            urgency += 0.2\n",
    "        \n",
    "        days = row.get('days_since_modified', 0)\n",
    "        if days > 730:\n",
    "            urgency += 0.2\n",
    "        elif days > 365:\n",
    "            urgency += 0.15\n",
    "        elif days > 180:\n",
    "            urgency += 0.1\n",
    "        else:\n",
    "            urgency += 0.05\n",
    "        \n",
    "        return min(urgency, 1.0)\n",
    "    \n",
    "    try:\n",
    "        stale_pages['urgency_score'] = stale_pages.apply(calculate_urgency, axis=1)\n",
    "        print(f\"‚úì Urgency range: [{stale_pages['urgency_score'].min():.4f}, {stale_pages['urgency_score'].max():.4f}]\")\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to calculate urgency: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 4: Merge & Score\n",
    "    # ========================================\n",
    "    print(\"\\n[4/6] Computing final scores...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"  ‚îú‚îÄ Merging with host pages...\")\n",
    "        results = similarities.merge(\n",
    "            host_pages[['host_idx', 'url', 'authority_norm', 'crawl_norm', 'status_code']],\n",
    "            left_on='host_idx',\n",
    "            right_on='host_idx',\n",
    "            how='left'\n",
    "        )\n",
    "        results = results.rename(columns={\n",
    "            'url': 'host_url',\n",
    "            'status_code': 'host_status_code'\n",
    "        })\n",
    "        \n",
    "        print(\"  ‚îú‚îÄ Merging with stale pages...\")\n",
    "        results = results.merge(\n",
    "            stale_pages[['stale_idx', 'url', 'urgency_score', 'page_type', 'status_code', 'days_since_modified']],\n",
    "            left_on='stale_idx',\n",
    "            right_on='stale_idx',\n",
    "            how='left'\n",
    "        )\n",
    "        results = results.rename(columns={\n",
    "            'url': 'stale_url',\n",
    "            'status_code': 'stale_status_code'\n",
    "        })\n",
    "        \n",
    "        print(\"  ‚îî‚îÄ Calculating scores...\")\n",
    "        \n",
    "        results['relevance_score'] = results['similarity_score']\n",
    "        results['authority_score'] = results['authority_norm']\n",
    "        results['crawl_weight_score'] = results['crawl_norm']\n",
    "        \n",
    "        results['final_score'] = (\n",
    "            results['relevance_score'] * weight_relevance +\n",
    "            results['authority_score'] * weight_authority +\n",
    "            results['crawl_weight_score'] * weight_crawl +\n",
    "            results['urgency_score'] * weight_urgency\n",
    "        ) * 100\n",
    "        \n",
    "        results['priority_score'] = (\n",
    "            results['relevance_score'] * 0.70 +\n",
    "            results['authority_score'] * 0.30\n",
    "        ) * 100\n",
    "        \n",
    "        print(f\"‚úì Final score range: [{results['final_score'].min():.2f}, {results['final_score'].max():.2f}]\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'error': f'Failed to compute scores: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 5: Assign Placement Zones\n",
    "    # ========================================\n",
    "    print(\"\\n[5/6] Assigning placement zones...\")\n",
    "    \n",
    "    def assign_placement_zone(relevance):\n",
    "        if relevance >= 0.8:\n",
    "            return 'content_body'\n",
    "        elif relevance >= 0.6:\n",
    "            return 'sidebar'\n",
    "        else:\n",
    "            return 'footer'\n",
    "    \n",
    "    results['placement_zone'] = results['relevance_score'].apply(assign_placement_zone)\n",
    "    \n",
    "    print(\"  Placement zone distribution:\")\n",
    "    print(results['placement_zone'].value_counts())\n",
    "    \n",
    "    # ========================================\n",
    "    # Step 6: Generate Anchor Text\n",
    "    # ========================================\n",
    "    print(\"\\n[6/6] Generating anchor text...\")\n",
    "    \n",
    "    def extract_anchor_text(url):\n",
    "        try:\n",
    "            from urllib.parse import urlparse\n",
    "            path = urlparse(url).path.strip('/')\n",
    "            path = path.replace('.html', '').replace('.php', '')\n",
    "            text = path.replace('/', ' ').replace('-', ' ').replace('_', ' ')\n",
    "            text = ' '.join(text.split())\n",
    "            return text[:100] if text else 'related content'\n",
    "        except:\n",
    "            return 'related content'\n",
    "    \n",
    "    results['anchor_text'] = results['stale_url'].apply(extract_anchor_text)\n",
    "    \n",
    "    # ========================================\n",
    "    # Save Deliverables\n",
    "    # ========================================\n",
    "    deliverables = []\n",
    "    \n",
    "    # DELIVERABLE 1: Stale Page Reasons\n",
    "    print(\"\\nüíæ Saving Deliverable 1: stale_page_reasons.csv...\")\n",
    "    \n",
    "    try:\n",
    "        def categorize_reason(row):\n",
    "            if row['status_code'] in [404, 410]:\n",
    "                return 'GHOST'\n",
    "            elif row['page_type'] == 'Orphan':\n",
    "                return 'ORPHAN'\n",
    "            elif row['days_since_modified'] > 90:\n",
    "                return 'DECAY'\n",
    "            else:\n",
    "                return 'LOW_ACTIVITY'\n",
    "        \n",
    "        stale_reasons = stale_pages.copy()\n",
    "        stale_reasons['reason_category'] = stale_reasons.apply(categorize_reason, axis=1)\n",
    "        stale_reasons['days_since_crawl'] = stale_reasons['days_since_modified']\n",
    "        stale_reasons['action_taken'] = 'Linked'\n",
    "        \n",
    "        link_counts = results.groupby('stale_idx').size().reset_index(name='recommended_links_count')\n",
    "        stale_reasons = stale_reasons.merge(link_counts, on='stale_idx', how='left')\n",
    "        stale_reasons['recommended_links_count'] = stale_reasons['recommended_links_count'].fillna(0).astype(int)\n",
    "        \n",
    "        stale_reasons = stale_reasons.rename(columns={'url': 'stale_url', 'status_code': 'stale_status_code'})\n",
    "        \n",
    "        stale_reasons_output = stale_reasons[['stale_url', 'stale_status_code', 'days_since_crawl', \n",
    "                                              'days_since_modified', 'reason_category', 'action_taken', \n",
    "                                              'recommended_links_count']]\n",
    "        \n",
    "        if len(stale_reasons_output) <= max_rows_per_file:\n",
    "            output_path = os.path.join(output_folder, 'stale_page_reasons.csv')\n",
    "            stale_reasons_output.to_csv(output_path, index=False)\n",
    "            deliverables.append('stale_page_reasons.csv')\n",
    "            print(f\"‚úì Saved: stale_page_reasons.csv ({len(stale_reasons_output):,} rows)\")\n",
    "        else:\n",
    "            num_parts = (len(stale_reasons_output) + max_rows_per_file - 1) // max_rows_per_file\n",
    "            for i in range(num_parts):\n",
    "                start_idx = i * max_rows_per_file\n",
    "                end_idx = min(start_idx + max_rows_per_file, len(stale_reasons_output))\n",
    "                part_df = stale_reasons_output.iloc[start_idx:end_idx]\n",
    "                output_path = os.path.join(output_folder, f'stale_page_reasons_part{i+1}.csv')\n",
    "                part_df.to_csv(output_path, index=False)\n",
    "                deliverables.append(f'stale_page_reasons_part{i+1}.csv')\n",
    "            print(f\"‚úì Saved {num_parts} files: stale_page_reasons_part*.csv\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Failed to save stale_page_reasons: {str(e)}\")\n",
    "    \n",
    "    # DELIVERABLE 2: Master Link Graph\n",
    "    print(\"\\nüíæ Saving Deliverable 2: master_link_graph (CSV + Parquet)...\")\n",
    "    \n",
    "    try:\n",
    "        master_cols = [\n",
    "            'host_url', 'stale_url', 'anchor_text', 'placement_zone',\n",
    "            'final_score', 'priority_score', 'relevance_score', 'authority_score',\n",
    "            'stale_status_code', 'host_status_code'\n",
    "        ]\n",
    "        master_graph = results[master_cols].sort_values('final_score', ascending=False)\n",
    "        \n",
    "        if len(master_graph) <= max_rows_per_file:\n",
    "            csv_path = os.path.join(output_folder, 'master_link_graph.csv')\n",
    "            master_graph.to_csv(csv_path, index=False)\n",
    "            deliverables.append('master_link_graph.csv')\n",
    "            print(f\"‚úì Saved: master_link_graph.csv ({len(master_graph):,} rows)\")\n",
    "        else:\n",
    "            num_parts = (len(master_graph) + max_rows_per_file - 1) // max_rows_per_file\n",
    "            for i in range(num_parts):\n",
    "                start_idx = i * max_rows_per_file\n",
    "                end_idx = min(start_idx + max_rows_per_file, len(master_graph))\n",
    "                part_df = master_graph.iloc[start_idx:end_idx]\n",
    "                csv_path = os.path.join(output_folder, f'master_link_graph_part{i+1}.csv')\n",
    "                part_df.to_csv(csv_path, index=False)\n",
    "                deliverables.append(f'master_link_graph_part{i+1}.csv')\n",
    "            print(f\"‚úì Saved {num_parts} files: master_link_graph_part*.csv\")\n",
    "        \n",
    "        parquet_path = os.path.join(output_folder, 'master_link_graph.parquet')\n",
    "        master_graph.to_parquet(parquet_path, index=False, compression='snappy')\n",
    "        parquet_size = os.path.getsize(parquet_path) / 1024**2\n",
    "        deliverables.append('master_link_graph.parquet')\n",
    "        print(f\"‚úì Saved: master_link_graph.parquet ({parquet_size:.2f} MB)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Failed to save master_link_graph: {str(e)}\")\n",
    "    \n",
    "    # DELIVERABLE 3: Link Injection Manifest\n",
    "    print(\"\\nüíæ Saving Deliverable 3: link_injection_manifest.json...\")\n",
    "    \n",
    "    try:\n",
    "        manifest_data = {\n",
    "            \"metadata\": {\n",
    "                \"generated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"total_host_pages\": int(len(host_pages)),\n",
    "                \"total_stale_pages\": int(len(stale_pages)),\n",
    "                \"total_links\": int(len(results)),\n",
    "                \"weights\": {\n",
    "                    \"relevance\": float(weight_relevance),\n",
    "                    \"authority\": float(weight_authority),\n",
    "                    \"crawl\": float(weight_crawl),\n",
    "                    \"urgency\": float(weight_urgency)\n",
    "                }\n",
    "            },\n",
    "            \"host_pages\": []\n",
    "        }\n",
    "        \n",
    "        print(\"  Grouping links by host page...\")\n",
    "        grouped = results.groupby('host_url')\n",
    "        \n",
    "        stale_lookup = stale_pages.set_index('url')[['status_code', 'page_type', 'days_since_modified']].to_dict('index')\n",
    "        \n",
    "        for host_url, group in tqdm(grouped, desc=\"  Building manifest\"):\n",
    "            host_row = host_pages[host_pages['url'] == host_url].iloc[0]\n",
    "            \n",
    "            host_entry = {\n",
    "                \"host_url\": host_url,\n",
    "                \"host_crawl_count\": int(host_row['crawl_count']),\n",
    "                \"host_status_code\": int(host_row['status_code']),\n",
    "                \"links\": []\n",
    "            }\n",
    "            \n",
    "            for _, link in group.nlargest(10, 'final_score').iterrows():\n",
    "                stale_info = stale_lookup.get(link['stale_url'], {})\n",
    "                \n",
    "                status = stale_info.get('status_code', 0)\n",
    "                page_type = stale_info.get('page_type', 'Unknown')\n",
    "                days = stale_info.get('days_since_modified', 0)\n",
    "                \n",
    "                if status in [404, 410]:\n",
    "                    reason = 'GHOST'\n",
    "                elif page_type == 'Orphan':\n",
    "                    reason = 'ORPHAN'\n",
    "                elif days > 90:\n",
    "                    reason = 'DECAY'\n",
    "                else:\n",
    "                    reason = 'LOW_ACTIVITY'\n",
    "                \n",
    "                host_entry[\"links\"].append({\n",
    "                    \"target_url\": link['stale_url'],\n",
    "                    \"anchor_text\": link['anchor_text'],\n",
    "                    \"placement_zone\": link['placement_zone'],\n",
    "                    \"final_score\": round(float(link['final_score']), 2),\n",
    "                    \"relevance\": round(float(link['relevance_score']), 4),\n",
    "                    \"stale_reason\": reason,\n",
    "                    \"stale_status_code\": int(link['stale_status_code'])\n",
    "                })\n",
    "            \n",
    "            manifest_data[\"host_pages\"].append(host_entry)\n",
    "        \n",
    "        manifest_path = os.path.join(output_folder, 'link_injection_manifest.json')\n",
    "        with open(manifest_path, 'w') as f:\n",
    "            json.dump(manifest_data, f, indent=2)\n",
    "        \n",
    "        manifest_size = os.path.getsize(manifest_path) / 1024**2\n",
    "        deliverables.append('link_injection_manifest.json')\n",
    "        print(f\"‚úì Saved: link_injection_manifest.json ({manifest_size:.2f} MB)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Failed to save manifest: {str(e)}\")\n",
    "    \n",
    "    # DELIVERABLE 4: Scoring Metadata\n",
    "    print(\"\\nüíæ Saving Deliverable 4: scoring_metadata.json...\")\n",
    "    \n",
    "    try:\n",
    "        metadata = {\n",
    "            \"weights\": {\n",
    "                \"relevance\": float(weight_relevance),\n",
    "                \"authority\": float(weight_authority),\n",
    "                \"crawl\": float(weight_crawl),\n",
    "                \"urgency\": float(weight_urgency)\n",
    "            },\n",
    "            \"urgency_components\": {\n",
    "                \"status_code_weight\": 0.4,\n",
    "                \"page_type_weight\": 0.4,\n",
    "                \"crawl_decay_weight\": 0.2\n",
    "            },\n",
    "            \"statistics\": {\n",
    "                \"total_recommendations\": int(len(results)),\n",
    "                \"avg_final_score\": float(results['final_score'].mean()),\n",
    "                \"high_confidence_links\": int((results['final_score'] > 70).sum()),\n",
    "                \"placement_zones\": {\n",
    "                    \"content_body\": int((results['placement_zone'] == 'content_body').sum()),\n",
    "                    \"sidebar\": int((results['placement_zone'] == 'sidebar').sum()),\n",
    "                    \"footer\": int((results['placement_zone'] == 'footer').sum())\n",
    "                }\n",
    "            },\n",
    "            \"generated_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        metadata_path = os.path.join(output_folder, 'scoring_metadata.json')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        deliverables.append('scoring_metadata.json')\n",
    "        print(f\"‚úì Saved: scoring_metadata.json\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Warning: Failed to save metadata: {str(e)}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Phase 4.3 Complete - All Deliverables Generated\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"  Total link recommendations: {len(results):,}\")\n",
    "    print(f\"  Average final score: {results['final_score'].mean():.2f}\")\n",
    "    print(f\"  High confidence links (>70): {(results['final_score'] > 70).sum():,}\")\n",
    "    print(f\"\\nüìÅ Output Location: {output_folder}\")\n",
    "    print(\"\\n‚úÖ All deliverables ready for deployment!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'total_recommendations': int(len(results)),\n",
    "        'avg_score': float(results['final_score'].mean()),\n",
    "        'high_confidence': int((results['final_score'] > 70).sum()),\n",
    "        'deliverables': deliverables,\n",
    "        'output_folder': os.path.abspath(output_folder)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266f831-4851-429d-be14-9640d8d4c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ============================================\n",
    "# Global State\n",
    "# ============================================\n",
    "global_state = {\n",
    "    'log_files': [],\n",
    "    'host_pages_df': None,\n",
    "    'stale_pages_df': None,\n",
    "    'embeddings_ready': False\n",
    "}\n",
    "\n",
    "# Output widgets for each phase\n",
    "phase1_output = widgets.Output()\n",
    "phase2_output = widgets.Output()\n",
    "phase3_2_output = widgets.Output()\n",
    "phase3_4_output = widgets.Output()\n",
    "phase4_1_output = widgets.Output()\n",
    "phase4_2_output = widgets.Output()\n",
    "phase4_3_output = widgets.Output()\n",
    "\n",
    "# ============================================\n",
    "# Configuration Widgets\n",
    "# ============================================\n",
    "config_title = widgets.HTML(\"<h2>‚öôÔ∏è Project Configuration</h2>\")\n",
    "\n",
    "log_folder_path = widgets.Text(\n",
    "    value='D:\\\\Task-3 complete\\\\pre-process files phase 1',\n",
    "    description='Log Folder:',\n",
    "    style={'description_width': '140px'},\n",
    "    layout=widgets.Layout(width='550px')\n",
    ")\n",
    "\n",
    "sitemap_url = widgets.Textarea(\n",
    "    value='''https://www.alamy.com/sitemaps/image_daily_index_s_100000001_110000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_10000001_20000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_110000001_120000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_120000001_130000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_130000001_140000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_140000001_150000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_150000001_160000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_160000001_170000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_170000001_180000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_180000001_190000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_200000001_210000000.xml\n",
    "https://www.alamy.com/sitemaps/image_daily_index_s_210000001_220000000.xml''',\n",
    "    description='Sitemap URLs:',\n",
    "    placeholder='Enter sitemap URLs (one per line or comma-separated)',\n",
    "    style={'description_width': '140px'},\n",
    "    layout=widgets.Layout(width='550px', height='120px')\n",
    ")\n",
    "\n",
    "phase3_output_folder = widgets.Text(\n",
    "    value='D:\\\\Task-3 complete\\\\phase 3 host&stale pages output',\n",
    "    description='Phase 3 Output:',\n",
    "    style={'description_width': '140px'},\n",
    "    layout=widgets.Layout(width='550px')\n",
    ")\n",
    "\n",
    "phase4_output_folder = widgets.Text(\n",
    "    value='D:\\\\Task-3 complete\\\\Phase 4 output_folder analysis',\n",
    "    description='Phase 4.1-4.2 Output:',  # ‚Üê CHANGED description\n",
    "    style={'description_width': '140px'},\n",
    "    layout=widgets.Layout(width='550px')\n",
    ")\n",
    "\n",
    "# NEW: Phase 4.3 deliverables output folder\n",
    "phase4_3_output_folder = widgets.Text(\n",
    "    value='D:\\Task-3 complete\\phase4_final 2',\n",
    "    description='Phase 4.3 Output:',\n",
    "    style={'description_width': '140px'},\n",
    "    layout=widgets.Layout(width='550px')\n",
    ")\n",
    "\n",
    "# Phase 3.2 Config\n",
    "crawl_days_threshold = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Crawl Freq (days):',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "# Phase 3.4 Config\n",
    "stale_days_threshold = widgets.IntText(\n",
    "    value=30,\n",
    "    description='Stale Threshold:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "low_activity_percentile = widgets.FloatSlider(\n",
    "    value=10.0,\n",
    "    min=5.0,\n",
    "    max=20.0,\n",
    "    step=1.0,\n",
    "    description='Low Activity %:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "# Phase 4 Config\n",
    "model_choice = widgets.Dropdown(\n",
    "    options=['all-MiniLM-L6-v2', 'all-mpnet-base-v2'],\n",
    "    value='all-MiniLM-L6-v2',\n",
    "    description='Model:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "batch_size_embeddings = widgets.IntText(\n",
    "    value=32,\n",
    "    description='Embedding Batch:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "top_k = widgets.IntText(\n",
    "    value=10,\n",
    "    description='Top-K:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "batch_size_similarity = widgets.IntText(\n",
    "    value=1000,\n",
    "    description='Similarity Batch:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "weight_relevance = widgets.FloatSlider(\n",
    "    value=0.60,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='W Relevance:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "weight_authority = widgets.FloatSlider(\n",
    "    value=0.25,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='W Authority:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "weight_urgency = widgets.FloatSlider(\n",
    "    value=0.15,\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.05,\n",
    "    description='W Urgency:',\n",
    "    style={'description_width': '140px'}\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Phase Buttons\n",
    "# ============================================\n",
    "btn_phase1 = widgets.Button(\n",
    "    description=\"Phase 1: Data Loading\",\n",
    "    button_style='info',\n",
    "    icon='database',\n",
    "    layout=widgets.Layout(width='350px', height='45px')\n",
    ")\n",
    "\n",
    "btn_phase2 = widgets.Button(\n",
    "    description=\"Phase 2: Log Verification\",\n",
    "    button_style='success',\n",
    "    icon='check',\n",
    "    layout=widgets.Layout(width='350px', height='45px')\n",
    ")\n",
    "\n",
    "btn_phase3_2 = widgets.Button(\n",
    "    description=\"Phase 3.2: Host Page Counting\",\n",
    "    button_style='warning',\n",
    "    icon='chart-bar',\n",
    "    layout=widgets.Layout(width='350px', height='45px')\n",
    ")\n",
    "\n",
    "btn_phase3_4 = widgets.Button(\n",
    "    description=\"Phase 3.4: Stale Page Finder\",\n",
    "    button_style='warning',\n",
    "    icon='exclamation-triangle',\n",
    "    layout=widgets.Layout(width='350px', height='45px')\n",
    ")\n",
    "\n",
    "btn_phase4_1 = widgets.Button(\n",
    "    description=\"Phase 4.1: Generate Embeddings\",\n",
    "    button_style='primary',\n",
    "    icon='brain',\n",
    "    layout=widgets.Layout(width='350px', height='45px')\n",
    ")\n",
    "\n",
    "btn_phase4_2 = widgets.Button(\n",
    "    description=\"Phase 4.2: Compute Similarities\",\n",
    "    button_style='primary',\n",
    "    icon='calculator',\n",
    "    layout=widgets.Layout(width='350px', height='45px')\n",
    ")\n",
    "\n",
    "btn_phase4_3 = widgets.Button(\n",
    "    description=\"Phase 4.3: Score & Rank Links\",\n",
    "    button_style='primary',\n",
    "    icon='trophy',\n",
    "    layout=widgets.Layout(width='350px', height='45px')\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# Event Handlers - CALL YOUR FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def run_phase1(b):\n",
    "    \"\"\"Phase 1: Validate data sources\"\"\"\n",
    "    with phase1_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÑ Phase 1: Validating data sources...\")\n",
    "        \n",
    "        try:\n",
    "            if not os.path.exists(log_folder_path.value):\n",
    "                print(f\"‚ùå Log folder not found: {log_folder_path.value}\")\n",
    "                return\n",
    "            \n",
    "            log_files = [f for f in os.listdir(log_folder_path.value) if f.endswith('.csv') or f.endswith('.log') or f.endswith('.log.gz')]\n",
    "            global_state['log_files'] = log_files\n",
    "            print(f\"‚úÖ Found {len(log_files)} log files\")\n",
    "            \n",
    "            if sitemap_url.value.startswith('http'):\n",
    "                print(f\"‚úÖ Sitemap URL: {sitemap_url.value.split()[0]}\")  # Show first URL\n",
    "            \n",
    "            print(\"‚úÖ Phase 1 Complete\")\n",
    "            btn_phase2.disabled = False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "\n",
    "def run_phase2(b):\n",
    "    \"\"\"Phase 2: Log file verification and processing\"\"\"\n",
    "    with phase2_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÑ Phase 2: Verifying and processing log files...\")\n",
    "        \n",
    "        try:\n",
    "            results = execute_phase2(\n",
    "                input_folder=log_folder_path.value,\n",
    "                output_folder=phase3_output_folder.value\n",
    "            )\n",
    "            \n",
    "            if results and results.get('status') == 'success':\n",
    "                print(f\"\\n‚úÖ Phase 2 Complete!\")\n",
    "                print(f\"   Final records: {results.get('final_records', 0):,}\")\n",
    "                print(f\"   Output: {results.get('output_folder', '')}\")\n",
    "                btn_phase3_2.disabled = False\n",
    "            else:\n",
    "                print(f\"‚ùå Phase 2 failed: {results.get('error', 'Unknown error')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "\n",
    "def run_phase3_2(b):\n",
    "    \"\"\"Phase 3.2: Host page counting\"\"\"\n",
    "    with phase3_2_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÑ Phase 3.2: Counting host pages...\")\n",
    "        print(f\"   Crawl frequency threshold: {crawl_days_threshold.value} days\")\n",
    "        \n",
    "        try:\n",
    "            results = execute_phase3_2(\n",
    "                input_folder=phase3_output_folder.value,\n",
    "                output_folder=phase3_output_folder.value,\n",
    "                crawl_threshold=crawl_days_threshold.value\n",
    "            )\n",
    "            \n",
    "            if results and results.get('status') == 'success':\n",
    "                host_count = results.get('host_pages', 0)\n",
    "                print(f\"‚úÖ Phase 3.2 Complete!\")\n",
    "                print(f\"   Host pages identified: {host_count:,}\")\n",
    "                print(f\"   Output: {results.get('output_file', '')}\")\n",
    "                global_state['host_pages_df'] = results.get('dataframe')\n",
    "                btn_phase3_4.disabled = False\n",
    "            else:\n",
    "                print(f\"‚ùå Phase 3.2 failed: {results.get('error', 'Unknown error')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            \n",
    "\n",
    "def run_phase3_4(b):\n",
    "    \"\"\"Phase 3.4: Stale page finder (Party Bouncer)\"\"\"\n",
    "    with phase3_4_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÑ Phase 3.4: Detecting stale pages (Party Bouncer)...\")\n",
    "        print(f\"   Input: {phase3_output_folder.value}\")\n",
    "        print(f\"   Output: {phase3_output_folder.value}\")\n",
    "        print(f\"   Sitemap URLs: {len(sitemap_url.value.strip().split())} indexes\")\n",
    "        print(f\"   Stale threshold: {stale_days_threshold.value} days\")\n",
    "        print(f\"   Low activity percentile: {low_activity_percentile.value}%\")\n",
    "        \n",
    "        try:\n",
    "            results = execute_phase3_4(\n",
    "                input_folder=phase3_output_folder.value,\n",
    "                output_folder=phase3_output_folder.value,\n",
    "                sitemap_urls=sitemap_url.value,\n",
    "                stale_threshold=stale_days_threshold.value,\n",
    "                percentile=low_activity_percentile.value,\n",
    "                workers=10,\n",
    "                log_batch_size=5,\n",
    "                test_mode=False\n",
    "            )\n",
    "            \n",
    "            if results and results.get('status') == 'success':\n",
    "                print(f\"\\nüìä Stale Page Summary:\")\n",
    "                print(f\"   Orphans: {results.get('orphans', 0):,}\")\n",
    "                print(f\"   Low Activity: {results.get('low_activity', 0):,}\")\n",
    "                print(f\"   Total Stale Pages: {results.get('total_stale', 0):,}\")\n",
    "                print(f\"   Output files: {len(results.get('output_files', []))}\")\n",
    "                print(f\"\\n‚úÖ Phase 3.4 Complete!\")\n",
    "                \n",
    "                global_state['stale_pages_df'] = results.get('dataframe')\n",
    "                btn_phase4_1.disabled = False\n",
    "            else:\n",
    "                print(f\"‚ùå Phase 3.4 failed: {results.get('error', 'Unknown error')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def run_phase4_1(b):\n",
    "    \"\"\"Phase 4.1: Generate embeddings\"\"\"\n",
    "    with phase4_1_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÑ Phase 4.1: Generating embeddings...\")\n",
    "        print(f\"   Input: {phase3_output_folder.value}\")\n",
    "        print(f\"   Output: {phase4_output_folder.value}\")\n",
    "        print(f\"   Model: {model_choice.value}\")\n",
    "        print(f\"   Batch size: {batch_size_embeddings.value}\")\n",
    "        \n",
    "        try:\n",
    "            results = execute_phase4_1(\n",
    "                input_folder=phase3_output_folder.value,\n",
    "                output_folder=phase4_output_folder.value,\n",
    "                model_name=model_choice.value,\n",
    "                batch_size=batch_size_embeddings.value\n",
    "            )\n",
    "            \n",
    "            if results and results.get('status') == 'success':\n",
    "                print(f\"\\n‚úÖ Phase 4.1 Complete!\")\n",
    "                print(f\"   Host embeddings: {results.get('host_count', 0):,}\")\n",
    "                print(f\"   Stale embeddings: {results.get('stale_count', 0):,}\")\n",
    "                print(f\"   Embedding dimensions: {results.get('embedding_dimensions', 0)}\")\n",
    "                print(f\"   Output: {results.get('output_folder', '')}\")\n",
    "                \n",
    "                global_state['embeddings_ready'] = True\n",
    "                btn_phase4_2.disabled = False\n",
    "            else:\n",
    "                print(f\"‚ùå Phase 4.1 failed: {results.get('error', 'Unknown error')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def run_phase4_2(b):\n",
    "    \"\"\"Phase 4.2: Compute similarities\"\"\"\n",
    "    with phase4_2_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÑ Phase 4.2: Computing similarities...\")\n",
    "        print(f\"   Input: {phase4_output_folder.value}\")\n",
    "        print(f\"   Output: {phase4_output_folder.value}\")\n",
    "        print(f\"   Top-K: {top_k.value}\")\n",
    "        print(f\"   Batch size: {batch_size_similarity.value}\")\n",
    "        \n",
    "        try:\n",
    "            results = execute_phase4_2(\n",
    "                input_folder=phase4_output_folder.value,\n",
    "                output_folder=phase4_output_folder.value,\n",
    "                top_k=top_k.value,\n",
    "                batch_size=batch_size_similarity.value\n",
    "            )\n",
    "            \n",
    "            if results and results.get('status') == 'success':\n",
    "                print(f\"\\n‚úÖ Phase 4.2 Complete!\")\n",
    "                print(f\"   Similarity pairs: {results.get('similarity_pairs', 0):,}\")\n",
    "                print(f\"   Average similarity: {results.get('avg_similarity', 0):.4f}\")\n",
    "                \n",
    "                sim_range = results.get('similarity_range', {})\n",
    "                print(f\"   Similarity range: [{sim_range.get('min', 0):.4f}, {sim_range.get('max', 0):.4f}]\")\n",
    "                print(f\"   Output: {results.get('output_file', '')}\")\n",
    "                \n",
    "                btn_phase4_3.disabled = False\n",
    "            else:\n",
    "                print(f\"‚ùå Phase 4.2 failed: {results.get('error', 'Unknown error')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def run_phase4_3(b):\n",
    "    \"\"\"Phase 4.3: Score, rank and generate recommendations\"\"\"\n",
    "    with phase4_3_output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîÑ Phase 4.3: Scoring and ranking...\")\n",
    "        print(f\"   Input: Phase 3 ({phase3_output_folder.value})\")\n",
    "        print(f\"          Phase 4 ({phase4_output_folder.value})\")\n",
    "        print(f\"   Output: {phase4_3_output_folder.value}\")  # ‚Üê UPDATED\n",
    "        \n",
    "        # Validate and normalize weights\n",
    "        total = weight_relevance.value + weight_authority.value + weight_urgency.value\n",
    "        if abs(total - 1.0) > 0.01:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Weights sum to {total:.2f}, normalizing...\")\n",
    "            norm_factor = 1.0 / total\n",
    "            w_rel = weight_relevance.value * norm_factor\n",
    "            w_auth = weight_authority.value * norm_factor\n",
    "            w_urg = weight_urgency.value * norm_factor\n",
    "        else:\n",
    "            w_rel = weight_relevance.value\n",
    "            w_auth = weight_authority.value\n",
    "            w_urg = weight_urgency.value\n",
    "        \n",
    "        w_crawl = 1.0 - (w_rel + w_auth + w_urg)\n",
    "        \n",
    "        print(f\"   Weights: Relevance={w_rel:.2f}, Authority={w_auth:.2f}, Crawl={w_crawl:.2f}, Urgency={w_urg:.2f}\")\n",
    "        \n",
    "        try:\n",
    "            # Call Phase 4.3 wrapper function\n",
    "            results = execute_phase4_3(\n",
    "                phase3_folder=phase3_output_folder.value,\n",
    "                phase4_folder=phase4_output_folder.value,\n",
    "                output_folder=phase4_3_output_folder.value,  # ‚Üê UPDATED to use separate folder\n",
    "                weight_relevance=w_rel,\n",
    "                weight_authority=w_auth,\n",
    "                weight_crawl=w_crawl,\n",
    "                weight_urgency=w_urg\n",
    "            )\n",
    "            \n",
    "            if results and results.get('status') == 'success':\n",
    "                print(f\"\\nüìä Deliverables Generated:\")\n",
    "                for deliverable in results.get('deliverables', []):\n",
    "                    print(f\"   ‚úì {deliverable}\")\n",
    "                \n",
    "                print(f\"\\n‚úÖ Phase 4.3 Complete!\")\n",
    "                print(f\"   Total recommendations: {results.get('total_recommendations', 0):,}\")\n",
    "                print(f\"   Average score: {results.get('avg_score', 0):.2f}\")\n",
    "                print(f\"   High confidence (>70): {results.get('high_confidence', 0):,}\")\n",
    "                print(f\"   Output: {results.get('output_folder', '')}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Phase 4.3 failed: {results.get('error', 'Unknown error')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Attach Event Handlers\n",
    "# ============================================\n",
    "btn_phase1.on_click(run_phase1)\n",
    "btn_phase2.on_click(run_phase2)\n",
    "btn_phase3_2.on_click(run_phase3_2)\n",
    "btn_phase3_4.on_click(run_phase3_4)\n",
    "btn_phase4_1.on_click(run_phase4_1)\n",
    "btn_phase4_2.on_click(run_phase4_2)\n",
    "btn_phase4_3.on_click(run_phase4_3)\n",
    "\n",
    "# ============================================\n",
    "# Dashboard Layout\n",
    "# ============================================\n",
    "config_section = widgets.VBox([\n",
    "    config_title,\n",
    "    widgets.HTML(\"<h4>üìÇ Data Sources</h4>\"),\n",
    "    log_folder_path,\n",
    "    sitemap_url,\n",
    "    widgets.HTML(\"<h4>üìÅ Output Folders</h4>\"),\n",
    "    phase3_output_folder,\n",
    "    phase4_output_folder,\n",
    "    phase4_3_output_folder,  # ‚Üê ADDED\n",
    "    widgets.HTML(\"<h4>Phase 3.2 Config</h4>\"),\n",
    "    crawl_days_threshold,\n",
    "    widgets.HTML(\"<h4>Phase 3.4 Config</h4>\"),\n",
    "    stale_days_threshold,\n",
    "    low_activity_percentile,\n",
    "    widgets.HTML(\"<h4>Phase 4 Config</h4>\"),\n",
    "    model_choice,\n",
    "    batch_size_embeddings,\n",
    "    top_k,\n",
    "    batch_size_similarity,\n",
    "    weight_relevance,\n",
    "    weight_authority,\n",
    "    weight_urgency\n",
    "], layout=widgets.Layout(padding='15px', width='600px'))\n",
    "\n",
    "phases_section = widgets.VBox([\n",
    "    widgets.HTML(\"<h1>üîó Internal Linking Analysis Pipeline</h1>\"),\n",
    "    \n",
    "    widgets.HTML(\"<h3>Phase 1-2: Data Preparation</h3>\"),\n",
    "    btn_phase1,\n",
    "    phase1_output,\n",
    "    btn_phase2,\n",
    "    phase2_output,\n",
    "    \n",
    "    widgets.HTML(\"<h3>Phase 3: Host & Stale Page Identification</h3>\"),\n",
    "    btn_phase3_2,\n",
    "    phase3_2_output,\n",
    "    btn_phase3_4,\n",
    "    phase3_4_output,\n",
    "    \n",
    "    widgets.HTML(\"<h3>Phase 4: Link Strategy Generation</h3>\"),\n",
    "    btn_phase4_1,\n",
    "    phase4_1_output,\n",
    "    btn_phase4_2,\n",
    "    phase4_2_output,\n",
    "    btn_phase4_3,\n",
    "    phase4_3_output\n",
    "], layout=widgets.Layout(padding='15px', width='750px'))\n",
    "\n",
    "dashboard = widgets.HBox([config_section, phases_section])\n",
    "display(dashboard)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
