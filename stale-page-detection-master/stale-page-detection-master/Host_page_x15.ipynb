{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707b05e8-517e-4d15-a5a3-a0f636e1b55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6858872f124c12bebc4138fe7f92f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Processready-test.xlsx', description='Input Path:', placeholder='Enter file or foldâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def detect_host_pages_with_viz(input_path, output_dir='host_page_output', max_records_per_file=500000):\n",
    "    \"\"\"\n",
    "    Phase 3.2: Host Page Detection (Memory-Optimized for 16M+ records)\n",
    "    Host Page = â‰¥3 crawls + Max 10-day gap (NO percentile filtering)\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"ðŸ“– Loading crawl data...\")\n",
    "    dfs = []\n",
    "    \n",
    "    if os.path.isdir(input_path):\n",
    "        files = [f for f in os.listdir(input_path) \n",
    "                if f.endswith(('.csv', '.xlsx', '.xls')) \n",
    "                and not f.startswith('~$')\n",
    "                and not f.startswith('.')]\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"âŒ No CSV/Excel files found in folder: {input_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        print(f\"ðŸ“ Found {len(files)} file(s) in folder\")\n",
    "        \n",
    "        for file in tqdm(files, desc=\"Reading files\"):\n",
    "            file_path = os.path.join(input_path, file)\n",
    "            try:\n",
    "                if file.endswith('.csv'):\n",
    "                    temp_df = pd.read_csv(file_path, encoding='utf-8-sig', low_memory=False)\n",
    "                else:\n",
    "                    temp_df = pd.read_excel(file_path)\n",
    "                dfs.append(temp_df)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error reading {file}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            print(\"âŒ No data loaded from folder\")\n",
    "            return None, None\n",
    "            \n",
    "        print(\"   â”œâ”€ Concatenating DataFrames...\")\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"âœ… Loaded {len(df):,} records from {len(dfs)} file(s)\")\n",
    "        \n",
    "    else:\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"âŒ File/folder not found: {input_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        df = pd.read_excel(input_path) if input_path.endswith('.xlsx') else pd.read_csv(input_path)\n",
    "        print(f\"âœ… Loaded {len(df):,} records from single file\")\n",
    "    \n",
    "    # Memory optimization - UPDATED: Add 'status' column\n",
    "    print(\"âš¡ Optimizing memory...\")\n",
    "    progress_mem = widgets.IntProgress(value=0, min=0, max=100, description='Memory:', bar_style='info')\n",
    "    display(progress_mem)\n",
    "    \n",
    "    progress_mem.value = 50\n",
    "    df = df[['request_uri', 'time_iso8601', 'http_user_agent', 'status']].copy()  # ADDED status\n",
    "    progress_mem.value = 100\n",
    "    \n",
    "    # Vectorized URL cleaning\n",
    "    print(\"âš¡ Processing URLs...\")\n",
    "    progress_url = widgets.IntProgress(value=0, min=0, max=100, description='URLs:', bar_style='info')\n",
    "    display(progress_url)\n",
    "    \n",
    "    progress_url.value = 33\n",
    "    df['timestamp'] = pd.to_datetime(df['time_iso8601'])\n",
    "    progress_url.value = 66\n",
    "    df['url_clean'] = df['request_uri'].str.split('?').str[0]\n",
    "    df['url'] = 'https://www.alamy.com' + df['url_clean'].fillna('')\n",
    "    progress_url.value = 100\n",
    "    \n",
    "    # Drop intermediate columns - UPDATED: Keep status\n",
    "    df = df[['url', 'timestamp', 'http_user_agent', 'status']].copy()\n",
    "    \n",
    "    min_date, max_date = df['timestamp'].min(), df['timestamp'].max()\n",
    "    period_days = (max_date - min_date).days + 1\n",
    "    total_pages = df['url'].nunique()\n",
    "    \n",
    "    print(f\"ðŸ“… Period: {min_date.date()} to {max_date.date()} ({period_days} days)\")\n",
    "    print(f\"ðŸ“Š Total unique pages: {total_pages:,}\")\n",
    "    \n",
    "    print(\"\\nâš™ï¸ Calculating page statistics...\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # MEMORY-OPTIMIZED GROUPING (Split into stages)\n",
    "    # ============================================================================\n",
    "    \n",
    "    # Step 1: First groupby - counts only (no lists, saves memory)\n",
    "    print(\"   â”œâ”€ Step 1/6: Aggregating crawl counts (lightweight)...\")\n",
    "    progress_group1 = widgets.IntProgress(value=0, min=0, max=100, description='Counts:', bar_style='info')\n",
    "    display(progress_group1)\n",
    "    \n",
    "    progress_group1.value = 30\n",
    "    url_stats = df.groupby('url', sort=False).agg({\n",
    "        'timestamp': ['count', 'min', 'max']\n",
    "    }).reset_index()\n",
    "    url_stats.columns = ['url', 'crawl_count', 'first_crawl', 'last_crawl']\n",
    "    progress_group1.value = 100\n",
    "    print(f\"   â”‚     â””â”€ âœ… Aggregated {len(url_stats):,} unique URLs\")\n",
    "    \n",
    "    # Step 2: Filter to only URLs with â‰¥3 crawls (reduces dataset before expensive operations)\n",
    "    print(\"   â”œâ”€ Step 2/6: Pre-filtering URLs (â‰¥3 crawls)...\")\n",
    "    progress_filter = widgets.IntProgress(value=0, min=0, max=100, description='Filtering:', bar_style='info')\n",
    "    display(progress_filter)\n",
    "    \n",
    "    progress_filter.value = 50\n",
    "    candidate_urls = url_stats[url_stats['crawl_count'] >= 3]['url'].tolist()\n",
    "    df_candidates = df[df['url'].isin(candidate_urls)].copy()\n",
    "    progress_filter.value = 100\n",
    "    print(f\"   â”‚     â””â”€ âœ… Filtered to {len(candidate_urls):,} candidates ({len(df_candidates):,} records)\")\n",
    "    \n",
    "    # Step 3: Create timestamp lists ONLY for candidate URLs (much smaller dataset)\n",
    "    print(\"   â”œâ”€ Step 3/6: Grouping timestamps (filtered set)...\")\n",
    "    progress_group2 = widgets.IntProgress(value=0, min=0, max=100, description='Timestamps:', bar_style='info')\n",
    "    display(progress_group2)\n",
    "    \n",
    "    progress_group2.value = 30\n",
    "    timestamp_groups = df_candidates.groupby('url', sort=False)['timestamp'].apply(list).reset_index()\n",
    "    timestamp_groups.columns = ['url', 'timestamps']\n",
    "    progress_group2.value = 100\n",
    "    print(f\"   â”‚     â””â”€ âœ… Created timestamp lists for {len(timestamp_groups):,} URLs\")\n",
    "    \n",
    "    # Step 4: Get user agents\n",
    "    print(\"   â”œâ”€ Step 4/6: Extracting user agents...\")\n",
    "    progress_ua = widgets.IntProgress(value=0, min=0, max=100, description='User Agents:', bar_style='info')\n",
    "    display(progress_ua)\n",
    "    \n",
    "    progress_ua.value = 50\n",
    "    user_agent_groups = df_candidates.groupby('url', sort=False)['http_user_agent'].apply(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "    ).reset_index()\n",
    "    user_agent_groups.columns = ['url', 'user_agent']\n",
    "    progress_ua.value = 100\n",
    "    print(f\"   â”‚     â””â”€ âœ… Extracted user agents for {len(user_agent_groups):,} URLs\")\n",
    "    \n",
    "    # Step 5: Extract status codes - NEW\n",
    "    print(\"   â”œâ”€ Step 5/6: Extracting status codes...\")\n",
    "    progress_status = widgets.IntProgress(value=0, min=0, max=100, description='Status Codes:', bar_style='info')\n",
    "    display(progress_status)\n",
    "    \n",
    "    progress_status.value = 50\n",
    "    status_groups = df_candidates.groupby('url', sort=False)['status'].apply(\n",
    "        lambda x: x.mode()[0] if len(x.mode()) > 0 else 200\n",
    "    ).reset_index()\n",
    "    status_groups.columns = ['url', 'status_code']\n",
    "    progress_status.value = 100\n",
    "    print(f\"   â”‚     â””â”€ âœ… Extracted status codes for {len(status_groups):,} URLs\")\n",
    "    \n",
    "    # Merge all stats\n",
    "    url_groups = url_stats.merge(timestamp_groups, on='url', how='left')\n",
    "    url_groups = url_groups.merge(user_agent_groups, on='url', how='left')\n",
    "    url_groups = url_groups.merge(status_groups, on='url', how='left')\n",
    "    url_groups['status_code'] = url_groups['status_code'].fillna(200)  # Default 200 for crawled pages\n",
    "    \n",
    "    url_groups['crawl_frequency'] = url_groups['crawl_count'] / period_days\n",
    "    \n",
    "    # Step 6: Host eligibility check (only on candidates with timestamps)\n",
    "    print(\"   â””â”€ Step 6/6: Checking host eligibility (â‰¥3 crawls + max 10-day gap)...\")\n",
    "    \n",
    "    def check_host_eligibility_fast(timestamps, min_crawls=3, max_gap_days=10):\n",
    "        if timestamps is None or not isinstance(timestamps, list) or len(timestamps) < min_crawls:\n",
    "            return False\n",
    "        sorted_ts = sorted(timestamps)\n",
    "        gaps = [(sorted_ts[i+1] - sorted_ts[i]).days for i in range(len(sorted_ts) - 1)]\n",
    "        return all(gap <= max_gap_days for gap in gaps)\n",
    "    \n",
    "    tqdm.pandas(desc=\"   â”‚     â””â”€ Validating intervals\")\n",
    "    url_groups['meets_interval'] = url_groups['timestamps'].progress_apply(check_host_eligibility_fast)\n",
    "    \n",
    "    # Classification\n",
    "    print(\"\\nâœ… Classifying host pages...\")\n",
    "    url_groups['is_host'] = (url_groups['crawl_count'] >= 3) & (url_groups['meets_interval'])\n",
    "    \n",
    "    # Keep authority_score for visualization ONLY\n",
    "    url_groups['authority_score'] = url_groups['crawl_count']  # Simplified: just use crawl_count\n",
    "    \n",
    "    host_pages = url_groups[url_groups['is_host']].copy()\n",
    "    host_pages = host_pages.sort_values('authority_score', ascending=False)\n",
    "    \n",
    "    print(f\"   â””â”€ âœ… Identified {len(host_pages):,} host pages ({len(host_pages)/total_pages*100:.2f}% of total)\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ANALYZE GOOGLEBOT TYPES\n",
    "    # ============================================================================\n",
    "    print(\"\\nðŸ¤– Analyzing Googlebot types...\")\n",
    "    \n",
    "    def extract_bot_type(user_agent):\n",
    "        if pd.isna(user_agent):\n",
    "            return 'Unknown'\n",
    "        \n",
    "        ua_str = str(user_agent).lower()\n",
    "        \n",
    "        if 'smartphone' in ua_str and 'googlebot' in ua_str:\n",
    "            return 'Googlebot-Smartphone'\n",
    "        elif 'googlebot-image' in ua_str:\n",
    "            return 'Googlebot-Image'\n",
    "        elif 'googlebot-video' in ua_str:\n",
    "            return 'Googlebot-Video'\n",
    "        elif 'googlebot-news' in ua_str:\n",
    "            return 'Googlebot-News'\n",
    "        elif 'adsbot' in ua_str:\n",
    "            return 'AdsBot-Google'\n",
    "        elif 'mediapartners' in ua_str:\n",
    "            return 'Mediapartners-Google'\n",
    "        elif 'mobile' in ua_str and 'googlebot' in ua_str:\n",
    "            return 'Googlebot-Mobile-Legacy' \n",
    "        elif 'googlebot' in ua_str:\n",
    "            return 'Googlebot-Desktop'\n",
    "        elif 'google' in ua_str:\n",
    "            return 'Other-Google-Bot'\n",
    "        else:\n",
    "            return 'Non-Google'\n",
    "    \n",
    "    df['bot_type'] = df['http_user_agent'].apply(extract_bot_type)\n",
    "    bot_stats = df.groupby('bot_type')['url'].nunique().reset_index()\n",
    "    bot_stats.columns = ['bot_type', 'unique_urls']\n",
    "    bot_stats = bot_stats.sort_values('unique_urls', ascending=False)\n",
    "    \n",
    "    print(f\"   â””â”€ âœ… Analyzed {len(bot_stats)} bot types\")\n",
    "    \n",
    "    # VISUALIZATION CODE (unchanged)\n",
    "    print(\"\\nðŸ“Š Generating visualizations...\")\n",
    "    progress_viz = widgets.IntProgress(value=0, min=0, max=100, description='Visuals:', bar_style='warning')\n",
    "    display(progress_viz)\n",
    "    \n",
    "    progress_viz.value = 20\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    host_urls = set(host_pages['url'].tolist())\n",
    "    df['is_host_crawl'] = df['url'].isin(host_urls)\n",
    "    \n",
    "    progress_viz.value = 40\n",
    "    daily_stats = df.groupby('date').agg({\n",
    "        'url': 'nunique',\n",
    "        'is_host_crawl': 'sum'\n",
    "    }).reset_index()\n",
    "    daily_stats.columns = ['date', 'total_pages_crawled', 'host_pages_crawled']\n",
    "    progress_viz.value = 60\n",
    "    \n",
    "    output1 = widgets.Output()\n",
    "    \n",
    "    with output1:\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        ax[0].plot(daily_stats['date'], daily_stats['total_pages_crawled'], \n",
    "                   label='Total Pages Crawled', linewidth=2, color='steelblue', alpha=0.7)\n",
    "        ax[0].plot(daily_stats['date'], daily_stats['host_pages_crawled'], \n",
    "                   label='Host Pages Crawled', linewidth=2, color='orangered')\n",
    "        ax[0].set_xlabel('Date', fontsize=12)\n",
    "        ax[0].set_ylabel('Page Count', fontsize=12)\n",
    "        ax[0].set_title('Host Pages vs Total Pages Crawled Over Time', fontsize=14, fontweight='bold')\n",
    "        ax[0].legend(loc='upper left', fontsize=10)\n",
    "        ax[0].grid(True, alpha=0.3)\n",
    "        plt.setp(ax[0].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        daily_stats['host_pct'] = (daily_stats['host_pages_crawled'] / daily_stats['total_pages_crawled']) * 100\n",
    "        ax[1].fill_between(daily_stats['date'], daily_stats['host_pct'], \n",
    "                           color='mediumseagreen', alpha=0.5, label='Host Page %')\n",
    "        ax[1].plot(daily_stats['date'], daily_stats['host_pct'], \n",
    "                   color='darkgreen', linewidth=2)\n",
    "        ax[1].axhline(y=daily_stats['host_pct'].mean(), color='red', \n",
    "                      linestyle='--', label=f'Avg: {daily_stats[\"host_pct\"].mean():.1f}%')\n",
    "        ax[1].set_xlabel('Date', fontsize=12)\n",
    "        ax[1].set_ylabel('Host Page %', fontsize=12)\n",
    "        ax[1].set_title('Host Page Crawl Percentage Over Time', fontsize=14, fontweight='bold')\n",
    "        ax[1].legend(loc='upper left', fontsize=10)\n",
    "        ax[1].grid(True, alpha=0.3)\n",
    "        plt.setp(ax[1].xaxis.get_majorticklabels(), rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    output2 = widgets.Output()\n",
    "    \n",
    "    with output2:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        top_10 = host_pages.head(10)\n",
    "        colors_gradient = plt.cm.Reds(np.linspace(0.4, 0.9, len(top_10)))\n",
    "        \n",
    "        ax[0].barh(range(len(top_10)), top_10['crawl_count'].values, color=colors_gradient)  # CHANGED to crawl_count\n",
    "        ax[0].set_yticks(range(len(top_10)))\n",
    "        ax[0].set_yticklabels([f\"{i+1}. {url.split('/')[-1][:30]}...\" for i, url in enumerate(top_10['url'].values)], fontsize=9)\n",
    "        ax[0].set_xlabel('Crawl Count', fontsize=12, fontweight='bold')  # CHANGED label\n",
    "        ax[0].set_title('Top 10 Host Pages by Crawl Count', fontsize=14, fontweight='bold')  # CHANGED title\n",
    "        ax[0].invert_yaxis()\n",
    "        ax[0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(top_10['crawl_count'].values):  # CHANGED to crawl_count\n",
    "            ax[0].text(v + max(top_10['crawl_count'])*0.01, i, f'{v:.0f}',   # CHANGED format\n",
    "                      va='center', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        colors_bot = plt.cm.Blues(np.linspace(0.4, 0.9, len(bot_stats)))\n",
    "        \n",
    "        ax[1].bar(range(len(bot_stats)), bot_stats['unique_urls'].values, color=colors_bot, edgecolor='black', linewidth=1)\n",
    "        ax[1].set_xticks(range(len(bot_stats)))\n",
    "        ax[1].set_xticklabels(bot_stats['bot_type'].values, rotation=45, ha='right', fontsize=9)\n",
    "        ax[1].set_ylabel('Unique URLs Crawled', fontsize=12, fontweight='bold')\n",
    "        ax[1].set_title('Googlebot Types - URL Coverage', fontsize=14, fontweight='bold')\n",
    "        ax[1].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for i, v in enumerate(bot_stats['unique_urls'].values):\n",
    "            ax[1].text(i, v + max(bot_stats['unique_urls'])*0.01, f'{v:,}', \n",
    "                      ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    progress_viz.value = 80\n",
    "    \n",
    "    output3 = widgets.Output()\n",
    "    \n",
    "    with output3:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "        categories = ['Host Pages', 'Regular Pages']\n",
    "        counts = [len(host_pages), total_pages - len(host_pages)]\n",
    "        colors = ['#ff6b6b', '#4ecdc4']\n",
    "        \n",
    "        ax.bar(categories, counts, color=colors, edgecolor='black', linewidth=1.5, width=0.6)\n",
    "        ax.set_ylabel('Page Count', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Host Pages vs Regular Pages Distribution', fontsize=14, fontweight='bold')\n",
    "        ax.text(0, counts[0] + max(counts)*0.02, f'{counts[0]:,}', \n",
    "                   ha='center', fontsize=12, fontweight='bold')\n",
    "        ax.text(1, counts[1] + max(counts)*0.02, f'{counts[1]:,}', \n",
    "                   ha='center', fontsize=12, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    progress_viz.value = 100\n",
    "    print(\"   â””â”€ âœ… Visualizations complete\")\n",
    "    \n",
    "    # SAVE OUTPUT - UPDATED\n",
    "    print(\"\\nðŸ’¾ Saving output files...\")\n",
    "    progress_save = widgets.IntProgress(value=0, min=0, max=100, description='Saving:', bar_style='success')\n",
    "    display(progress_save)\n",
    "    \n",
    "    # UPDATED: Remove authority_score, add status_code\n",
    "    output_cols = ['url', 'crawl_count', 'crawl_frequency', 'user_agent', 'status_code']\n",
    "    \n",
    "    if len(host_pages) <= max_records_per_file:\n",
    "        progress_save.value = 50\n",
    "        output_path = os.path.join(output_dir, 'host_pages.csv')\n",
    "        host_pages[output_cols].to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        progress_save.value = 100\n",
    "        print(f\"   â””â”€ Saved: {output_path}\")\n",
    "    else:\n",
    "        num_parts = (len(host_pages) // max_records_per_file) + 1\n",
    "        print(f\"   â”œâ”€ Splitting into {num_parts} parts...\")\n",
    "        for i in tqdm(range(num_parts), desc=\"   â”‚  â””â”€ Writing files\"):\n",
    "            start_idx = i * max_records_per_file\n",
    "            end_idx = min((i + 1) * max_records_per_file, len(host_pages))\n",
    "            part_df = host_pages.iloc[start_idx:end_idx]\n",
    "            output_path = os.path.join(output_dir, f'host_pages_part{i+1}.csv')\n",
    "            part_df[output_cols].to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            progress_save.value = int((i+1) / num_parts * 100)\n",
    "        print(f\"   â””â”€ Saved {num_parts} files\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š PHASE 3.2 COMPLETE: HOST PAGE DETECTION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"âœ… Host Page Criteria: â‰¥3 crawls + max 10-day gap\")\n",
    "    print(f\"âœ… Output: url, crawl_count, crawl_frequency, user_agent, status_code\")\n",
    "    print(f\"âœ… Analyzed {len(bot_stats)} different bot types\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    display(output1)\n",
    "    display(output2)\n",
    "    display(output3)\n",
    "    \n",
    "    return host_pages, daily_stats, bot_stats\n",
    "\n",
    "\n",
    "# Execution widgets\n",
    "file_input = widgets.Text(\n",
    "    value='Processready-test.xlsx',\n",
    "    placeholder='Enter file or folder path',\n",
    "    description='Input Path:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Analysis',\n",
    "    button_style='success',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        detect_host_pages_with_viz(file_input.value)\n",
    "\n",
    "run_button.on_click(on_run_clicked)\n",
    "\n",
    "display(widgets.VBox([file_input, run_button, output_area]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9dc1ff-d319-4ab7-8899-2e1ea31dc525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
