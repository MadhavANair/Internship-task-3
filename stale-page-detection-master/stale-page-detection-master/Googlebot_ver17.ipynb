{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25679fde-7903-4980-9e4f-6d421f9768c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cache file and re-run\n",
    "import os\n",
    "if os.path.exists('google_ip_ranges_cache.json'):\n",
    "    os.remove('google_ip_ranges_cache.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bcb7809-ff83-4d78-b274-baac81e588ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ ENHANCED 3-PHASE GOOGLEBOT EXTRACTION (FIXED)\n",
      "   Phase 1: UA | Phase 2: CIDR + IMPROVED Lenient | Phase 3: FcrDNS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Enter folder path (or empty for file):  D:\\Task-3 complete\\dayfiles_9thoct2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ ENHANCED 3-PHASE GOOGLEBOT EXTRACTION (FIXED)\n",
      "================================================================================\n",
      "   Lenient Mode: ENABLED\n",
      "   FcrDNS Fallback: ENABLED\n",
      "\n",
      "================================================================================\n",
      "LOADING GOOGLE IP RANGES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Using cached IP ranges (age: 18.7 hours)\n",
      "   ‚Ä¢ IPv4 ranges: 982\n",
      "   ‚Ä¢ IPv6 ranges: 959\n",
      "‚úÖ Ready with 1941 official IP ranges\n",
      "\n",
      "üìÅ Input files: 24\n",
      "\n",
      "üîç ENHANCED 3-PHASE STRATEGY:\n",
      "   Phase 1: Filter by Googlebot UA ‚Üí ~18M\n",
      "   Phase 2: CIDR + IMPROVED Lenient mode ‚Üí ~15M+\n",
      "   Phase 3: FcrDNS fallback ‚Üí ~14.9M (TARGET!)\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: FILTER BY GOOGLEBOT UA\n",
      "================================================================================\n",
      "\n",
      "üìÇ [1/24] nginx-20251008_000000.log (4538.5 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2551183 recs [03:22, 12605.47 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 773,128 records with Googlebot UA\n",
      "\n",
      "üìÇ [2/24] nginx-20251008_010000.log (4529.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2592414 recs [03:00, 14370.12 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 838,754 records with Googlebot UA\n",
      "\n",
      "üìÇ [3/24] nginx-20251008_020000.log (4470.6 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2510059 recs [03:01, 13836.41 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 793,650 records with Googlebot UA\n",
      "\n",
      "üìÇ [4/24] nginx-20251008_030000.log (5056.7 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2814062 recs [02:17, 20463.02 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 1,116,118 records with Googlebot UA\n",
      "\n",
      "üìÇ [5/24] nginx-20251008_040000.log (4221.4 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2355701 recs [01:10, 33489.70 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 683,930 records with Googlebot UA\n",
      "\n",
      "üìÇ [6/24] nginx-20251008_050000.log (4567.6 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2529240 recs [01:10, 35891.12 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 838,401 records with Googlebot UA\n",
      "\n",
      "üìÇ [7/24] nginx-20251008_060000.log (4626.4 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2579965 recs [01:16, 33687.01 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 748,817 records with Googlebot UA\n",
      "\n",
      "üìÇ [8/24] nginx-20251008_070000.log (4469.5 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2488497 recs [01:43, 24080.69 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 599,958 records with Googlebot UA\n",
      "\n",
      "üìÇ [9/24] nginx-20251008_080000.log (4562.6 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2547138 recs [01:56, 21819.30 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 583,639 records with Googlebot UA\n",
      "\n",
      "üìÇ [10/24] nginx-20251008_090000.log (4901.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2740636 recs [02:07, 21443.27 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 642,632 records with Googlebot UA\n",
      "\n",
      "üìÇ [11/24] nginx-20251008_100000.log (4980.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2758098 recs [01:47, 25584.23 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 646,519 records with Googlebot UA\n",
      "\n",
      "üìÇ [12/24] nginx-20251008_110000.log (4699.6 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2564335 recs [01:23, 30565.39 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 708,278 records with Googlebot UA\n",
      "\n",
      "üìÇ [13/24] nginx-20251008_120000.log (4820.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2661686 recs [02:04, 21320.98 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 679,412 records with Googlebot UA\n",
      "\n",
      "üìÇ [14/24] nginx-20251008_130000.log (5193.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2868575 recs [02:01, 23616.13 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 732,245 records with Googlebot UA\n",
      "\n",
      "üìÇ [15/24] nginx-20251008_140000.log (5348.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2958048 recs [01:48, 27290.86 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 761,075 records with Googlebot UA\n",
      "\n",
      "üìÇ [16/24] nginx-20251008_150000.log (5062.9 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2772859 recs [01:44, 26587.07 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 623,403 records with Googlebot UA\n",
      "\n",
      "üìÇ [17/24] nginx-20251008_160000.log (5103.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2846096 recs [02:09, 22006.44 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 689,801 records with Googlebot UA\n",
      "\n",
      "üìÇ [18/24] nginx-20251008_170000.log (5273.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2973756 recs [01:53, 26216.51 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 928,435 records with Googlebot UA\n",
      "\n",
      "üìÇ [19/24] nginx-20251008_180000.log (5122.5 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2876436 recs [01:17, 37086.93 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 844,440 records with Googlebot UA\n",
      "\n",
      "üìÇ [20/24] nginx-20251008_190000.log (5016.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2823036 recs [01:37, 28896.28 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 807,499 records with Googlebot UA\n",
      "\n",
      "üìÇ [21/24] nginx-20251008_200000.log (4889.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2742944 recs [01:26, 31850.90 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 781,287 records with Googlebot UA\n",
      "\n",
      "üìÇ [22/24] nginx-20251008_210000.log (4510.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2531968 recs [01:26, 29214.17 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 652,574 records with Googlebot UA\n",
      "\n",
      "üìÇ [23/24] nginx-20251008_220000.log (4340.7 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2466274 recs [01:32, 26762.45 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 686,518 records with Googlebot UA\n",
      "\n",
      "üìÇ [24/24] nginx-20251008_230000.log (4300.3 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   Filtering UA: 2479327 recs [01:19, 31035.53 recs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Accepted: 769,099 records with Googlebot UA\n",
      "\n",
      "================================================================================\n",
      "‚úÖ PHASE 1 COMPLETE\n",
      "================================================================================\n",
      "   Total processed: 63,990,539\n",
      "   Googlebot UA filtered: 17,929,612\n",
      "   Unique IPs: 2,023\n",
      "   Time: 2685.5s (44.8 min)\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: CHECK IPS AGAINST CIDR RANGES (IMPROVED LENIENT MODE)\n",
      "================================================================================\n",
      "\n",
      "üîç Checking 2,023 IPs against CIDR ranges...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking IPs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2023/2023 [00:00<00:00, 4876.70 IPs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ IPs in CIDR ranges: 1,629/2,023\n",
      "   ‚ö†Ô∏è IPs outside ranges: 394 (will check with FcrDNS in Phase 3)\n",
      "\n",
      "üìù Writing CIDR-filtered records (IMPROVED LENIENT MODE)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Phase 1 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [06:09<00:00,  7.55s/ files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ PHASE 2 COMPLETE\n",
      "================================================================================\n",
      "   Total records: 17,929,612\n",
      "   - In CIDR ranges: 1,629 unique IPs\n",
      "   - Missing IP (lenient): 0\n",
      "   - Invalid IP (lenient): 0\n",
      "   - Outside CIDR (needs FcrDNS): 394 unique IPs\n",
      "   Phase 2 files: 49\n",
      "   Time: 369.8s (6.2 min)\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: FcrDNS VERIFICATION (FOR NON-CIDR IPs)\n",
      "================================================================================\n",
      "\n",
      "üåê Starting FcrDNS verification for 394 IPs outside CIDR ranges...\n",
      "   Using 150 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FcrDNS Verification: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:05<00:00, 66.26 IPs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ FcrDNS Verification Results:\n",
      "   Passed: 0/394\n",
      "   Time: 6.3s (0.1 min)\n",
      "   Speed: 62 IPs/sec\n",
      "\n",
      "üìù Writing final verified records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Phase 2 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [03:41<00:00,  4.53s/ files]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úÖ PHASE 3 COMPLETE\n",
      "================================================================================\n",
      "   Final records: 14,259,928\n",
      "   Breakdown:\n",
      "   - CIDR verified: 14,259,928\n",
      "   - Missing IP (lenient): 0\n",
      "   - Invalid IP (lenient): 0\n",
      "   - FcrDNS passed: 0\n",
      "   - FcrDNS failed (rejected): 3,669,684\n",
      "   Output files: 49\n",
      "\n",
      "================================================================================\n",
      "‚úÖ COMPLETE: ENHANCED 3-PHASE EXTRACTION (FIXED)\n",
      "================================================================================\n",
      "\n",
      "‚è±Ô∏è  PERFORMANCE:\n",
      "   Phase 1 (UA Filter): 2685.5s (44.8 min)\n",
      "   Phase 2 (CIDR + Lenient): 369.8s (6.2 min)\n",
      "   Phase 3 (FcrDNS): 228.1s (3.8 min)\n",
      "   TOTAL: 3283.5s (54.7 min)\n",
      "\n",
      "üìä RESULTS:\n",
      "   Phase 1: 17,929,612 (Googlebot UA)\n",
      "   Phase 2: 17,929,612 (CIDR + Lenient)\n",
      "   Phase 3: 14,259,928 (Final Verified)\n",
      "\n",
      "ü§ñ GOOGLEBOT VARIANTS:\n",
      "   ‚Ä¢ Googlebot (standard): 16,523,241\n",
      "   ‚Ä¢ Adsbot-Google: 1,240,612\n",
      "   ‚Ä¢ Googlebot-Image: 2,789\n",
      "   ‚Ä¢ Mediapartners-Google: 1,152\n",
      "\n",
      "üéØ GSC COMPARISON:\n",
      "   GSC: 14,913,024\n",
      "   Output: 14,259,928\n",
      "   Difference: 653,096\n",
      "   Match: 95.6%\n",
      "   ‚úÖ EXCELLENT MATCH!\n",
      "\n",
      "üì¶ Creating ZIP archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Archiving: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [02:12<00:00,  2.71s/ files]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ZIP created: googlebot_final_20251113_100009.zip (964.91 MB)\n",
      "\n",
      "üéâ DONE!\n",
      "üìÇ C:\\Users\\MadhavN\\cleaned_logs_enhanced_fixed\n",
      "üìã Final: 14,259,928 records\n",
      "üéØ Match: 95.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import gzip\n",
    "import socket\n",
    "import time\n",
    "import zipfile\n",
    "import ipaddress\n",
    "import requests\n",
    "import dns.resolver\n",
    "import dns.reversename\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MAX_DNS_WORKERS = 150\n",
    "DNS_TIMEOUT = 2\n",
    "CACHE_FILE = \"verified_ips_cache.txt\"\n",
    "IP_RANGES_CACHE_FILE = \"google_ip_ranges_cache.json\"\n",
    "CACHE_EXPIRY_HOURS = 24\n",
    "GOOGLE_DOMAINS = ['.googlebot.com', '.google.com', '.googleusercontent.com']\n",
    "MAX_RECORDS_PER_FILE = 500000\n",
    "\n",
    "# NEW: Lenient mode and FcrDNS fallback\n",
    "LENIENT_MODE = True  # Accept records with UA but missing/invalid IP\n",
    "ENABLE_FCRDNS_FALLBACK = True  # Verify non-CIDR IPs with FcrDNS\n",
    "\n",
    "GOOGLE_IP_RANGE_URLS = {\n",
    "    'googlebot': 'https://developers.google.com/static/search/apis/ipranges/googlebot.json',\n",
    "    'special_crawlers': 'https://developers.google.com/static/search/apis/ipranges/special-crawlers.json',\n",
    "    'user_triggered_fetchers': 'https://developers.google.com/static/search/apis/ipranges/user-triggered-fetchers.json',\n",
    "    'user_triggered_fetchers_google': 'https://developers.google.com/static/search/apis/ipranges/user-triggered-fetchers-google.json',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# FETCH DYNAMIC IP RANGES\n",
    "# ============================================================================\n",
    "\n",
    "def fetch_google_ip_ranges():\n",
    "    \"\"\"Fetch Google's official IP ranges (IPv4 + IPv6) from JSON files\"\"\"\n",
    "    print(\"\\nüåê Fetching Google IP ranges from official sources...\")\n",
    "    \n",
    "    all_prefixes = []\n",
    "    \n",
    "    for source_name, url in GOOGLE_IP_RANGE_URLS.items():\n",
    "        try:\n",
    "            print(f\"   ‚Ä¢ Fetching {source_name}...\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'prefixes' in data:\n",
    "                for prefix in data['prefixes']:\n",
    "                    if 'ipv4Prefix' in prefix:\n",
    "                        all_prefixes.append(prefix['ipv4Prefix'])\n",
    "                    if 'ipv6Prefix' in prefix:\n",
    "                        all_prefixes.append(prefix['ipv6Prefix'])\n",
    "            \n",
    "            print(f\"     ‚úÖ Retrieved {len(data.get('prefixes', []))} prefixes\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è Warning: Failed to fetch {source_name}: {str(e)}\")\n",
    "    \n",
    "    ip_networks = []\n",
    "    ipv4_count = 0\n",
    "    ipv6_count = 0\n",
    "    \n",
    "    for prefix in all_prefixes:\n",
    "        try:\n",
    "            network = ipaddress.ip_network(prefix)\n",
    "            ip_networks.append(network)\n",
    "            \n",
    "            if network.version == 4:\n",
    "                ipv4_count += 1\n",
    "            else:\n",
    "                ipv6_count += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total IP ranges loaded: {len(ip_networks)} CIDR blocks\")\n",
    "    print(f\"   ‚Ä¢ IPv4 ranges: {ipv4_count}\")\n",
    "    print(f\"   ‚Ä¢ IPv6 ranges: {ipv6_count}\")\n",
    "    return ip_networks\n",
    "\n",
    "def load_or_fetch_ip_ranges(cache_file=IP_RANGES_CACHE_FILE):\n",
    "    \"\"\"Load IP ranges from cache or fetch from Google if expired\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, 'r') as f:\n",
    "                cache_data = json.load(f)\n",
    "            \n",
    "            cache_time = cache_data.get('timestamp', 0)\n",
    "            cache_age_hours = (time.time() - cache_time) / 3600\n",
    "            \n",
    "            if cache_age_hours < CACHE_EXPIRY_HOURS:\n",
    "                print(f\"\\n‚úÖ Using cached IP ranges (age: {cache_age_hours:.1f} hours)\")\n",
    "                ip_networks = [ipaddress.ip_network(cidr) for cidr in cache_data['prefixes']]\n",
    "                \n",
    "                ipv4_count = sum(1 for n in ip_networks if n.version == 4)\n",
    "                ipv6_count = sum(1 for n in ip_networks if n.version == 6)\n",
    "                print(f\"   ‚Ä¢ IPv4 ranges: {ipv4_count}\")\n",
    "                print(f\"   ‚Ä¢ IPv6 ranges: {ipv6_count}\")\n",
    "                return ip_networks\n",
    "            else:\n",
    "                print(f\"\\n‚è∞ Cache expired, fetching fresh data...\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Cache error, fetching fresh data...\")\n",
    "    \n",
    "    ip_networks = fetch_google_ip_ranges()\n",
    "    \n",
    "    try:\n",
    "        cache_data = {\n",
    "            'timestamp': time.time(),\n",
    "            'prefixes': [str(network) for network in ip_networks]\n",
    "        }\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(cache_data, f, indent=2)\n",
    "        print(f\"üíæ IP ranges cached\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to save cache\")\n",
    "    \n",
    "    return ip_networks\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_full_timestamp_from_filename(filename):\n",
    "    \"\"\"Extract unique timestamp from filename\"\"\"\n",
    "    base_name = filename\n",
    "    if base_name.endswith('.gz'):\n",
    "        base_name = base_name[:-3]\n",
    "    if base_name.endswith('.log'):\n",
    "        base_name = base_name[:-4]\n",
    "    if base_name.startswith('nginx-'):\n",
    "        base_name = base_name[6:]\n",
    "    return base_name if base_name else None\n",
    "\n",
    "def extract_first_ip(ip_string):\n",
    "    \"\"\"Extract first IP (IPv4 or IPv6) from X-Forwarded-For field\"\"\"\n",
    "    if not ip_string:\n",
    "        return None\n",
    "    \n",
    "    ip = str(ip_string).split(',')[0].strip()\n",
    "    \n",
    "    if ip.startswith('[') and ip.endswith(']'):\n",
    "        ip = ip[1:-1]\n",
    "    \n",
    "    return ip if ip else None\n",
    "\n",
    "def is_googlebot(user_agent_str):\n",
    "    \"\"\"UA validation - Check if contains googlebot patterns\"\"\"\n",
    "    if not user_agent_str:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ua_lower = str(user_agent_str).lower()\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    googlebot_patterns = [\n",
    "        'googlebot', 'google-inspectiontool', 'googlebot-image', 'googlebot-news',\n",
    "        'googlebot-video', 'adsbot-google', 'mediapartners-google', 'apis-google',\n",
    "        'google favicon', 'feedfetcher-google', 'google-read-aloud', 'duplichecker',\n",
    "        'google web preview', 'google-site-verification', 'google-smartphone'\n",
    "    ]\n",
    "    \n",
    "    for pattern in googlebot_patterns:\n",
    "        if pattern in ua_lower:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def create_zip_archive(file_list, output_folder, archive_name=\"googlebot_data\"):\n",
    "    \"\"\"Create ZIP archive\"\"\"\n",
    "    zip_filename = os.path.join(output_folder, f\"{archive_name}_{time.strftime('%Y%m%d_%H%M%S')}.zip\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Creating ZIP archive...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file_path in tqdm(file_list, desc=\"Archiving\", unit=\" files\"):\n",
    "            zipf.write(file_path, arcname=os.path.basename(file_path))\n",
    "    \n",
    "    zip_size_mb = os.path.getsize(zip_filename) / (1024**2)\n",
    "    print(f\"‚úÖ ZIP created: {os.path.basename(zip_filename)} ({zip_size_mb:.2f} MB)\")\n",
    "    return zip_filename\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION METHODS\n",
    "# ============================================================================\n",
    "\n",
    "def verify_ip_in_range(ip, ip_networks):\n",
    "    \"\"\"Check if IP is in Google's CIDR ranges\"\"\"\n",
    "    try:\n",
    "        ip_addr = ipaddress.ip_address(ip)\n",
    "        for network in ip_networks:\n",
    "            if ip_addr in network:\n",
    "                return True\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def verify_ip_method2_dns_fast(ip):\n",
    "    \"\"\"FcrDNS Verification - ULTRA FAST using dnspython\"\"\"\n",
    "    try:\n",
    "        rev_name = dns.reversename.from_address(ip)\n",
    "        \n",
    "        resolver = dns.resolver.Resolver()\n",
    "        resolver.timeout = DNS_TIMEOUT\n",
    "        resolver.lifetime = DNS_TIMEOUT\n",
    "        \n",
    "        try:\n",
    "            reverse_answers = resolver.resolve(rev_name, 'PTR')\n",
    "            hostname = str(reverse_answers[0]).rstrip('.')\n",
    "            hostname_lower = hostname.lower()\n",
    "        except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.Timeout, dns.exception.DNSException):\n",
    "            return False\n",
    "        \n",
    "        is_google_domain = any(hostname_lower.endswith(domain) for domain in GOOGLE_DOMAINS)\n",
    "        \n",
    "        if not is_google_domain:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            forward_ips = []\n",
    "            \n",
    "            try:\n",
    "                forward_answers = resolver.resolve(hostname, 'A')\n",
    "                forward_ips.extend([str(rdata) for rdata in forward_answers])\n",
    "            except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                forward_answers = resolver.resolve(hostname, 'AAAA')\n",
    "                forward_ips.extend([str(rdata) for rdata in forward_answers])\n",
    "            except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n",
    "                pass\n",
    "            \n",
    "            if not forward_ips:\n",
    "                return False\n",
    "            \n",
    "            if ip in forward_ips:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        except (dns.resolver.Timeout, dns.exception.DNSException):\n",
    "            return False\n",
    "    \n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def verify_ips_parallel_dns(ips_to_verify, desc=\"DNS Verification\"):\n",
    "    \"\"\"Verify IPs using FcrDNS in parallel\"\"\"\n",
    "    verified_ips = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_DNS_WORKERS) as executor:\n",
    "        future_to_ip = {\n",
    "            executor.submit(verify_ip_method2_dns_fast, ip): ip \n",
    "            for ip in ips_to_verify\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=len(ips_to_verify), desc=desc, unit=\" IPs\") as pbar:\n",
    "            for future in as_completed(future_to_ip):\n",
    "                ip = future_to_ip[future]\n",
    "                try:\n",
    "                    is_verified = future.result(timeout=DNS_TIMEOUT + 1)\n",
    "                    verified_ips[ip] = is_verified\n",
    "                except Exception:\n",
    "                    verified_ips[ip] = False\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "    \n",
    "    return verified_ips\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PROCESSING - ENHANCED 3-PHASE WITH FIXED LENIENT MODE\n",
    "# ============================================================================\n",
    "\n",
    "def process_log_files_enhanced(input_folder=None, input_file=None, output_folder=\"cleaned_logs_enhanced\", \n",
    "                               max_records_per_file=500000, create_zip=True):\n",
    "    \"\"\"\n",
    "    ENHANCED 3-PHASE GOOGLEBOT EXTRACTION (FIXED LENIENT MODE)\n",
    "    \n",
    "    Phase 1: Filter by Googlebot UA ‚Üí ~18M records\n",
    "    Phase 2: CIDR check + IMPROVED Lenient mode ‚Üí ~15M+ records\n",
    "    Phase 3: FcrDNS fallback for non-CIDR IPs ‚Üí Final ~14.9M (99-100% match!)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéØ ENHANCED 3-PHASE GOOGLEBOT EXTRACTION (FIXED)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"   Lenient Mode: {'ENABLED' if LENIENT_MODE else 'DISABLED'}\")\n",
    "    print(f\"   FcrDNS Fallback: {'ENABLED' if ENABLE_FCRDNS_FALLBACK else 'DISABLED'}\")\n",
    "    \n",
    "    # Load IP ranges\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"LOADING GOOGLE IP RANGES\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    try:\n",
    "        google_ip_networks = load_or_fetch_ip_ranges()\n",
    "        print(f\"‚úÖ Ready with {len(google_ip_networks)} official IP ranges\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load IP ranges: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Get input files\n",
    "    log_files = []\n",
    "    if input_file:\n",
    "        if os.path.exists(input_file):\n",
    "            log_files = [input_file]\n",
    "        else:\n",
    "            print(f\"‚ùå File not found: {input_file}\")\n",
    "            return None\n",
    "    elif input_folder:\n",
    "        if os.path.exists(input_folder):\n",
    "            log_files = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) \n",
    "                        if f.endswith('.log') or f.endswith('.log.gz')])\n",
    "            if not log_files:\n",
    "                print(f\"‚ùå No .log files found in: {input_folder}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"‚ùå Folder not found: {input_folder}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"‚ùå Please provide input_folder or input_file\")\n",
    "        return None\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    abs_output_folder = os.path.abspath(output_folder)\n",
    "    \n",
    "    # Create subfolders\n",
    "    phase1_folder = os.path.join(output_folder, \"phase1_ua_filtered\")\n",
    "    phase2_folder = os.path.join(output_folder, \"phase2_cidr_filtered\")\n",
    "    phase3_folder = os.path.join(output_folder, \"phase3_final_verified\")\n",
    "    os.makedirs(phase1_folder, exist_ok=True)\n",
    "    os.makedirs(phase2_folder, exist_ok=True)\n",
    "    os.makedirs(phase3_folder, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüìÅ Input files: {len(log_files)}\")\n",
    "    \n",
    "    print(f\"\\nüîç ENHANCED 3-PHASE STRATEGY:\")\n",
    "    print(f\"   Phase 1: Filter by Googlebot UA ‚Üí ~18M\")\n",
    "    print(f\"   Phase 2: CIDR + IMPROVED Lenient mode ‚Üí ~15M+\")\n",
    "    print(f\"   Phase 3: FcrDNS fallback ‚Üí ~14.9M (TARGET!)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 1: FILTER BY GOOGLEBOT UA ONLY\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 1: FILTER BY GOOGLEBOT UA\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    phase1_files = []\n",
    "    total_phase1_records = 0\n",
    "    unique_ips_phase1 = set()\n",
    "    googlebot_variants = {}\n",
    "    \n",
    "    stats_phase1 = {\n",
    "        'total_processed': 0,\n",
    "        'has_googlebot_ua': 0,\n",
    "    }\n",
    "    \n",
    "    start_phase1 = time.time()\n",
    "    \n",
    "    for file_idx, log_file in enumerate(log_files, 1):\n",
    "        filename = os.path.basename(log_file)\n",
    "        file_timestamp = extract_full_timestamp_from_filename(filename)\n",
    "        \n",
    "        if file_timestamp:\n",
    "            base_filename = f\"phase1_{file_timestamp}\"\n",
    "        else:\n",
    "            base_filename = f\"phase1_file{file_idx:04d}\"\n",
    "        \n",
    "        file_size_mb = os.path.getsize(log_file) / (1024**2)\n",
    "        print(f\"\\nüìÇ [{file_idx}/{len(log_files)}] {filename} ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        current_file_index = 1\n",
    "        current_file_records = 0\n",
    "        current_writer = None\n",
    "        current_csvfile = None\n",
    "        \n",
    "        def create_new_csv_file(file_index):\n",
    "            if file_index == 1:\n",
    "                csv_filename = os.path.join(phase1_folder, f\"{base_filename}.csv\")\n",
    "            else:\n",
    "                csv_filename = os.path.join(phase1_folder, f\"{base_filename}_part{file_index}.csv\")\n",
    "            \n",
    "            csvfile = open(csv_filename, 'w', newline='', encoding='utf-8-sig')\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                'timestamp', 'request_uri', 'http_x_forwarded_for', \n",
    "                'bytes_sent', 'upstream_response_time', 'user_agent'\n",
    "            ], quoting=csv.QUOTE_ALL)\n",
    "            writer.writeheader()\n",
    "            phase1_files.append(csv_filename)\n",
    "            return csvfile, writer\n",
    "        \n",
    "        current_csvfile, current_writer = create_new_csv_file(current_file_index)\n",
    "        \n",
    "        file_accepted = 0\n",
    "        \n",
    "        try:\n",
    "            if log_file.endswith('.gz'):\n",
    "                file_handle = gzip.open(log_file, 'rt', encoding='utf-8', errors='ignore')\n",
    "            else:\n",
    "                file_handle = open(log_file, 'r', encoding='utf-8', errors='ignore')\n",
    "            \n",
    "            with file_handle as log_reader:\n",
    "                pbar = tqdm(log_reader, desc=\"   Filtering UA\", unit=\" recs\", ncols=100, mininterval=0.5)\n",
    "                \n",
    "                for line in pbar:\n",
    "                    try:\n",
    "                        if not line.strip():\n",
    "                            continue\n",
    "                        \n",
    "                        log_entry = json.loads(line.strip())\n",
    "                        \n",
    "                        stats_phase1['total_processed'] += 1\n",
    "                        \n",
    "                        user_agent = log_entry.get('http_user_agent', '')\n",
    "                        if user_agent:\n",
    "                            user_agent = str(user_agent).strip()\n",
    "                        \n",
    "                        # ONLY filter by UA in Phase 1\n",
    "                        if not is_googlebot(user_agent):\n",
    "                            continue\n",
    "                        \n",
    "                        stats_phase1['has_googlebot_ua'] += 1\n",
    "                        \n",
    "                        # Track variant\n",
    "                        ua_lower = user_agent.lower()\n",
    "                        for variant in ['googlebot-image', 'googlebot-news', 'googlebot-video', \n",
    "                                       'google-inspectiontool', 'adsbot-google', 'mediapartners-google']:\n",
    "                            if variant in ua_lower:\n",
    "                                key = variant.replace('-', ' ').title().replace(' ', '-')\n",
    "                                googlebot_variants[key] = googlebot_variants.get(key, 0) + 1\n",
    "                                break\n",
    "                        else:\n",
    "                            if 'googlebot' in ua_lower:\n",
    "                                googlebot_variants['Googlebot (standard)'] = googlebot_variants.get('Googlebot (standard)', 0) + 1\n",
    "                        \n",
    "                        # Extract fields\n",
    "                        request_uri = log_entry.get('request_uri', '')\n",
    "                        if request_uri:\n",
    "                            request_uri = str(request_uri).strip()\n",
    "                        \n",
    "                        timestamp = log_entry.get('time_iso8601', '')\n",
    "                        if timestamp:\n",
    "                            timestamp = str(timestamp).strip()\n",
    "                        \n",
    "                        ip = extract_first_ip(log_entry.get('http_x_forwarded_for', ''))\n",
    "                        if ip:\n",
    "                            unique_ips_phase1.add(ip)\n",
    "                        \n",
    "                        bytes_sent = log_entry.get('bytes_sent', '')\n",
    "                        upstream_response_time = log_entry.get('upstream_response_time', '')\n",
    "                        \n",
    "                        # Partition check\n",
    "                        if current_file_records >= max_records_per_file:\n",
    "                            current_csvfile.close()\n",
    "                            current_file_index += 1\n",
    "                            current_csvfile, current_writer = create_new_csv_file(current_file_index)\n",
    "                            current_file_records = 0\n",
    "                        \n",
    "                        csv_record = {\n",
    "                            'timestamp': timestamp if timestamp else '',\n",
    "                            'request_uri': request_uri if request_uri else '',\n",
    "                            'http_x_forwarded_for': ip if ip else '',\n",
    "                            'bytes_sent': bytes_sent if bytes_sent else '',\n",
    "                            'upstream_response_time': upstream_response_time if upstream_response_time else '',\n",
    "                            'user_agent': user_agent if user_agent else ''\n",
    "                        }\n",
    "                        \n",
    "                        current_writer.writerow(csv_record)\n",
    "                        current_file_records += 1\n",
    "                        file_accepted += 1\n",
    "                        total_phase1_records += 1\n",
    "                    \n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                pbar.close()\n",
    "            \n",
    "            if current_csvfile and not current_csvfile.closed:\n",
    "                current_csvfile.close()\n",
    "            \n",
    "            print(f\"   ‚úÖ Accepted: {file_accepted:,} records with Googlebot UA\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {str(e)}\")\n",
    "            if current_csvfile and not current_csvfile.closed:\n",
    "                current_csvfile.close()\n",
    "            continue\n",
    "    \n",
    "    elapsed_phase1 = time.time() - start_phase1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 1 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Total processed: {stats_phase1['total_processed']:,}\")\n",
    "    print(f\"   Googlebot UA filtered: {total_phase1_records:,}\")\n",
    "    print(f\"   Unique IPs: {len(unique_ips_phase1):,}\")\n",
    "    print(f\"   Time: {elapsed_phase1:.1f}s ({elapsed_phase1/60:.1f} min)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 2: CHECK IPS IN CIDR RANGES (IMPROVED LENIENT MODE)\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 2: CHECK IPS AGAINST CIDR RANGES (IMPROVED LENIENT MODE)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\nüîç Checking {len(unique_ips_phase1):,} IPs against CIDR ranges...\")\n",
    "    verified_ips_cidr = {}\n",
    "    for ip in tqdm(unique_ips_phase1, desc=\"Checking IPs\", unit=\" IPs\"):\n",
    "        verified_ips_cidr[ip] = verify_ip_in_range(ip, google_ip_networks)\n",
    "    \n",
    "    ips_in_range = sum(1 for v in verified_ips_cidr.values() if v)\n",
    "    ips_outside_range = len(unique_ips_phase1) - ips_in_range\n",
    "    print(f\"   ‚úÖ IPs in CIDR ranges: {ips_in_range:,}/{len(unique_ips_phase1):,}\")\n",
    "    print(f\"   ‚ö†Ô∏è IPs outside ranges: {ips_outside_range:,} (will check with FcrDNS in Phase 3)\")\n",
    "    \n",
    "    print(f\"\\nüìù Writing CIDR-filtered records (IMPROVED LENIENT MODE)...\")\n",
    "    \n",
    "    phase2_files = []\n",
    "    total_phase2_records = 0\n",
    "    unique_ips_phase2 = set()\n",
    "    unique_ips_outside_cidr = set()\n",
    "    records_missing_ip = 0\n",
    "    records_invalid_ip = 0  # NEW: Track invalid IPs\n",
    "    \n",
    "    start_phase2 = time.time()\n",
    "    \n",
    "    for phase1_file in tqdm(phase1_files, desc=\"Processing Phase 1 files\", unit=\" files\"):\n",
    "        base_name = os.path.basename(phase1_file).replace('phase1_', 'phase2_')\n",
    "        \n",
    "        current_file_index = 1\n",
    "        current_file_records = 0\n",
    "        current_writer = None\n",
    "        current_csvfile = None\n",
    "        \n",
    "        def create_phase2_file(file_index):\n",
    "            if file_index == 1:\n",
    "                csv_filename = os.path.join(phase2_folder, base_name)\n",
    "            else:\n",
    "                name_parts = base_name.rsplit('.', 1)\n",
    "                csv_filename = os.path.join(phase2_folder, f\"{name_parts[0]}_part{file_index}.{name_parts[1]}\")\n",
    "            \n",
    "            csvfile = open(csv_filename, 'w', newline='', encoding='utf-8-sig')\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                'timestamp', 'request_uri', 'http_x_forwarded_for', \n",
    "                'bytes_sent', 'upstream_response_time', 'ip_status'\n",
    "            ], quoting=csv.QUOTE_ALL)\n",
    "            writer.writeheader()\n",
    "            phase2_files.append(csv_filename)\n",
    "            return csvfile, writer\n",
    "        \n",
    "        current_csvfile, current_writer = create_phase2_file(current_file_index)\n",
    "        \n",
    "        try:\n",
    "            with open(phase1_file, 'r', encoding='utf-8-sig') as infile:\n",
    "                reader = csv.DictReader(infile)\n",
    "                \n",
    "                for row in reader:\n",
    "                    ip = row.get('http_x_forwarded_for', '').strip()  # FIXED: Added .strip()\n",
    "                    \n",
    "                    # Determine IP status\n",
    "                    ip_status = 'unknown'\n",
    "                    accept_record = False\n",
    "                    \n",
    "                    # IMPROVED: Check for various \"missing\" states\n",
    "                    if not ip or ip == '' or ip.lower() in ['none', 'null', '-', 'unknown']:\n",
    "                        # Missing or invalid IP but has Googlebot UA (lenient)\n",
    "                        if LENIENT_MODE:\n",
    "                            accept_record = True\n",
    "                            ip_status = 'missing_ip_lenient'\n",
    "                            records_missing_ip += 1\n",
    "                    \n",
    "                    # Check if IP is in our verified list\n",
    "                    elif ip in verified_ips_cidr:\n",
    "                        if verified_ips_cidr[ip]:\n",
    "                            # IP in CIDR range\n",
    "                            accept_record = True\n",
    "                            ip_status = 'in_cidr'\n",
    "                            unique_ips_phase2.add(ip)\n",
    "                        else:\n",
    "                            # IP NOT in CIDR - will verify with FcrDNS later\n",
    "                            accept_record = True\n",
    "                            ip_status = 'needs_fcrdns'\n",
    "                            unique_ips_outside_cidr.add(ip)\n",
    "                            unique_ips_phase2.add(ip)\n",
    "                    \n",
    "                    else:\n",
    "                        # IP not in Phase 1 unique list - might be malformed\n",
    "                        if LENIENT_MODE:\n",
    "                            accept_record = True\n",
    "                            ip_status = 'invalid_ip_lenient'\n",
    "                            records_invalid_ip += 1\n",
    "                    \n",
    "                    if accept_record:\n",
    "                        if current_file_records >= max_records_per_file:\n",
    "                            current_csvfile.close()\n",
    "                            current_file_index += 1\n",
    "                            current_csvfile, current_writer = create_phase2_file(current_file_index)\n",
    "                            current_file_records = 0\n",
    "                        \n",
    "                        current_writer.writerow({\n",
    "                            'timestamp': row.get('timestamp', ''),\n",
    "                            'request_uri': row.get('request_uri', ''),\n",
    "                            'http_x_forwarded_for': ip if ip else '',\n",
    "                            'bytes_sent': row.get('bytes_sent', ''),\n",
    "                            'upstream_response_time': row.get('upstream_response_time', ''),\n",
    "                            'ip_status': ip_status\n",
    "                        })\n",
    "                        current_file_records += 1\n",
    "                        total_phase2_records += 1\n",
    "            \n",
    "            if current_csvfile and not current_csvfile.closed:\n",
    "                current_csvfile.close()\n",
    "        \n",
    "        except Exception:\n",
    "            if current_csvfile and not current_csvfile.closed:\n",
    "                current_csvfile.close()\n",
    "            continue\n",
    "    \n",
    "    elapsed_phase2 = time.time() - start_phase2\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 2 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Total records: {total_phase2_records:,}\")\n",
    "    print(f\"   - In CIDR ranges: {ips_in_range:,} unique IPs\")\n",
    "    print(f\"   - Missing IP (lenient): {records_missing_ip:,}\")\n",
    "    print(f\"   - Invalid IP (lenient): {records_invalid_ip:,}\")\n",
    "    print(f\"   - Outside CIDR (needs FcrDNS): {len(unique_ips_outside_cidr):,} unique IPs\")\n",
    "    print(f\"   Phase 2 files: {len(phase2_files)}\")\n",
    "    print(f\"   Time: {elapsed_phase2:.1f}s ({elapsed_phase2/60:.1f} min)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 3: FcrDNS VERIFICATION FOR NON-CIDR IPs\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 3: FcrDNS VERIFICATION (FOR NON-CIDR IPs)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    dns_verified_ips = {}\n",
    "    \n",
    "    if ENABLE_FCRDNS_FALLBACK and len(unique_ips_outside_cidr) > 0:\n",
    "        print(f\"\\nüåê Starting FcrDNS verification for {len(unique_ips_outside_cidr):,} IPs outside CIDR ranges...\")\n",
    "        print(f\"   Using {MAX_DNS_WORKERS} workers\")\n",
    "        \n",
    "        start_phase3_dns = time.time()\n",
    "        \n",
    "        dns_verified_ips = verify_ips_parallel_dns(unique_ips_outside_cidr, desc=\"FcrDNS Verification\")\n",
    "        \n",
    "        dns_pass = sum(1 for v in dns_verified_ips.values() if v)\n",
    "        elapsed_dns = time.time() - start_phase3_dns\n",
    "        \n",
    "        print(f\"\\n‚úÖ FcrDNS Verification Results:\")\n",
    "        print(f\"   Passed: {dns_pass:,}/{len(unique_ips_outside_cidr):,}\")\n",
    "        print(f\"   Time: {elapsed_dns:.1f}s ({elapsed_dns/60:.1f} min)\")\n",
    "        if len(unique_ips_outside_cidr) > 0:\n",
    "            print(f\"   Speed: {len(unique_ips_outside_cidr)/elapsed_dns:.0f} IPs/sec\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è FcrDNS verification skipped\")\n",
    "    \n",
    "    print(f\"\\nüìù Writing final verified records...\")\n",
    "    \n",
    "    phase3_files = []\n",
    "    total_phase3_records = 0\n",
    "    rejection_stats = {\n",
    "        'in_cidr': 0,\n",
    "        'missing_ip_accepted': 0,\n",
    "        'invalid_ip_accepted': 0,  # NEW\n",
    "        'fcrdns_passed': 0,\n",
    "        'fcrdns_failed_rejected': 0\n",
    "    }\n",
    "    \n",
    "    start_phase3_write = time.time()\n",
    "    \n",
    "    for phase2_file in tqdm(phase2_files, desc=\"Processing Phase 2 files\", unit=\" files\"):\n",
    "        base_name = os.path.basename(phase2_file).replace('phase2_', 'phase3_')\n",
    "        phase3_file = os.path.join(phase3_folder, base_name)\n",
    "        \n",
    "        try:\n",
    "            with open(phase2_file, 'r', encoding='utf-8-sig') as infile:\n",
    "                reader = csv.DictReader(infile)\n",
    "                \n",
    "                with open(phase3_file, 'w', newline='', encoding='utf-8-sig') as outfile:\n",
    "                    writer = csv.DictWriter(outfile, fieldnames=[\n",
    "                        'timestamp', 'request_uri', 'http_x_forwarded_for', \n",
    "                        'bytes_sent', 'upstream_response_time'\n",
    "                    ], quoting=csv.QUOTE_ALL)\n",
    "                    writer.writeheader()\n",
    "                    \n",
    "                    for row in reader:\n",
    "                        ip = row.get('http_x_forwarded_for', '').strip()  # FIXED: Added .strip()\n",
    "                        ip_status = row.get('ip_status', '')\n",
    "                        \n",
    "                        accept_record = False\n",
    "                        \n",
    "                        if ip_status == 'in_cidr':\n",
    "                            accept_record = True\n",
    "                            rejection_stats['in_cidr'] += 1\n",
    "                        \n",
    "                        elif ip_status == 'missing_ip_lenient':\n",
    "                            accept_record = True\n",
    "                            rejection_stats['missing_ip_accepted'] += 1\n",
    "                        \n",
    "                        elif ip_status == 'invalid_ip_lenient':  # NEW: Accept invalid/malformed IPs\n",
    "                            accept_record = True\n",
    "                            rejection_stats['invalid_ip_accepted'] += 1\n",
    "                        \n",
    "                        elif ip_status == 'needs_fcrdns':\n",
    "                            if dns_verified_ips.get(ip, False):\n",
    "                                accept_record = True\n",
    "                                rejection_stats['fcrdns_passed'] += 1\n",
    "                            else:\n",
    "                                accept_record = False\n",
    "                                rejection_stats['fcrdns_failed_rejected'] += 1\n",
    "                        \n",
    "                        if accept_record:\n",
    "                            writer.writerow({\n",
    "                                'timestamp': row.get('timestamp', ''),\n",
    "                                'request_uri': row.get('request_uri', ''),\n",
    "                                'http_x_forwarded_for': ip,\n",
    "                                'bytes_sent': row.get('bytes_sent', ''),\n",
    "                                'upstream_response_time': row.get('upstream_response_time', '')\n",
    "                            })\n",
    "                            total_phase3_records += 1\n",
    "            \n",
    "            phase3_files.append(phase3_file)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    elapsed_phase3_write = time.time() - start_phase3_write\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 3 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Final records: {total_phase3_records:,}\")\n",
    "    print(f\"   Breakdown:\")\n",
    "    print(f\"   - CIDR verified: {rejection_stats['in_cidr']:,}\")\n",
    "    print(f\"   - Missing IP (lenient): {rejection_stats['missing_ip_accepted']:,}\")\n",
    "    print(f\"   - Invalid IP (lenient): {rejection_stats['invalid_ip_accepted']:,}\")\n",
    "    print(f\"   - FcrDNS passed: {rejection_stats['fcrdns_passed']:,}\")\n",
    "    print(f\"   - FcrDNS failed (rejected): {rejection_stats['fcrdns_failed_rejected']:,}\")\n",
    "    print(f\"   Output files: {len(phase3_files)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SUMMARY\n",
    "    # ========================================================================\n",
    "    total_time = elapsed_phase1 + elapsed_phase2\n",
    "    if ENABLE_FCRDNS_FALLBACK and len(unique_ips_outside_cidr) > 0:\n",
    "        total_time += elapsed_dns + elapsed_phase3_write\n",
    "    \n",
    "    gsc_expected = 14913024\n",
    "    match_rate = (total_phase3_records / gsc_expected * 100) if total_phase3_records > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ COMPLETE: ENHANCED 3-PHASE EXTRACTION (FIXED)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  PERFORMANCE:\")\n",
    "    print(f\"   Phase 1 (UA Filter): {elapsed_phase1:.1f}s ({elapsed_phase1/60:.1f} min)\")\n",
    "    print(f\"   Phase 2 (CIDR + Lenient): {elapsed_phase2:.1f}s ({elapsed_phase2/60:.1f} min)\")\n",
    "    if ENABLE_FCRDNS_FALLBACK and len(unique_ips_outside_cidr) > 0:\n",
    "        print(f\"   Phase 3 (FcrDNS): {(elapsed_dns + elapsed_phase3_write):.1f}s ({(elapsed_dns + elapsed_phase3_write)/60:.1f} min)\")\n",
    "    print(f\"   TOTAL: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS:\")\n",
    "    print(f\"   Phase 1: {total_phase1_records:,} (Googlebot UA)\")\n",
    "    print(f\"   Phase 2: {total_phase2_records:,} (CIDR + Lenient)\")\n",
    "    print(f\"   Phase 3: {total_phase3_records:,} (Final Verified)\")\n",
    "    \n",
    "    print(f\"\\nü§ñ GOOGLEBOT VARIANTS:\")\n",
    "    for variant, count in sorted(googlebot_variants.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   ‚Ä¢ {variant}: {count:,}\")\n",
    "    \n",
    "    print(f\"\\nüéØ GSC COMPARISON:\")\n",
    "    print(f\"   GSC: {gsc_expected:,}\")\n",
    "    print(f\"   Output: {total_phase3_records:,}\")\n",
    "    print(f\"   Difference: {abs(gsc_expected - total_phase3_records):,}\")\n",
    "    print(f\"   Match: {match_rate:.1f}%\")\n",
    "    \n",
    "    if match_rate >= 98:\n",
    "        print(f\"   üèÜ NEAR-PERFECT!\")\n",
    "    elif match_rate >= 95:\n",
    "        print(f\"   ‚úÖ EXCELLENT MATCH!\")\n",
    "    elif match_rate >= 90:\n",
    "        print(f\"   ‚úÖ VERY GOOD!\")\n",
    "    \n",
    "    # Create ZIP\n",
    "    if create_zip:\n",
    "        zip_filename = create_zip_archive(phase3_files, output_folder, \"googlebot_final\")\n",
    "    \n",
    "    return {\n",
    "        'output_folder': abs_output_folder,\n",
    "        'phase3_files': phase3_files,\n",
    "        'final_records': total_phase3_records,\n",
    "        'match_rate': match_rate,\n",
    "        'rejection_stats': rejection_stats\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ ENHANCED 3-PHASE GOOGLEBOT EXTRACTION (FIXED)\")\n",
    "print(\"   Phase 1: UA | Phase 2: CIDR + IMPROVED Lenient | Phase 3: FcrDNS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "INPUT_FOLDER = input(\"\\nüìÇ Enter folder path (or empty for file): \").strip()\n",
    "\n",
    "if not INPUT_FOLDER:\n",
    "    INPUT_FILE = input(\"üìÑ Enter file path: \").strip()\n",
    "    results = process_log_files_enhanced(\n",
    "        input_file=INPUT_FILE,\n",
    "        output_folder=\"cleaned_logs_enhanced_fixed\",\n",
    "        max_records_per_file=500000,\n",
    "        create_zip=True\n",
    "    )\n",
    "else:\n",
    "    results = process_log_files_enhanced(\n",
    "        input_folder=INPUT_FOLDER,\n",
    "        output_folder=\"cleaned_logs_enhanced_fixed\",\n",
    "        max_records_per_file=500000,\n",
    "        create_zip=True\n",
    "    )\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nüéâ DONE!\")\n",
    "    print(f\"üìÇ {results['output_folder']}\")\n",
    "    print(f\"üìã Final: {results['final_records']:,} records\")\n",
    "    print(f\"üéØ Match: {results['match_rate']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b495e6-a38f-48f4-9f49-7eb245753004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
