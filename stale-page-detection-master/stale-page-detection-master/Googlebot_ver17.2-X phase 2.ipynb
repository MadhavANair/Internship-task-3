{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b67e84-b150-4710-bc2e-611d558e4500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üßπ CONDA CACHE CLEANUP\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Running 'conda clean --all'...\n",
      "There are no unused tarball(s) to remove.\n",
      "There are no index cache(s) to remove.\n",
      "There are no unused package(s) to remove.\n",
      "There are no tempfile(s) to remove.\n",
      "There are no logfile(s) to remove.\n",
      "\n",
      "‚úÖ Conda clean completed\n",
      "\n",
      "2Ô∏è‚É£ Locating Conda directories...\n",
      "   Conda base: D:\\anaconda\n",
      "\n",
      "3Ô∏è‚É£ Cache locations:\n",
      "   üìÅ D:\\anaconda\\pkgs (4639.96 MB)\n",
      "   üìÅ D:\\anaconda\\conda-meta (11.18 MB)\n",
      "\n",
      "4Ô∏è‚É£ Clearing pip cache...\n",
      "‚úÖ Pip cache cleared\n",
      "\n",
      "============================================================\n",
      "‚úÖ CLEANUP COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_conda_cache():\n",
    "    \"\"\"Comprehensive Conda cache cleanup\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üßπ CONDA CACHE CLEANUP\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Run conda clean --all\n",
    "    print(\"\\n1Ô∏è‚É£ Running 'conda clean --all'...\")\n",
    "    try:\n",
    "        result = subprocess.run(['conda', 'clean', '--all', '-y'], \n",
    "                              capture_output=True, text=True)\n",
    "        print(result.stdout)\n",
    "        print(\"‚úÖ Conda clean completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error running conda clean: {e}\")\n",
    "    \n",
    "    # 2. Find and show conda directories\n",
    "    print(\"\\n2Ô∏è‚É£ Locating Conda directories...\")\n",
    "    \n",
    "    try:\n",
    "        # Get conda info\n",
    "        result = subprocess.run(['conda', 'info', '--base'], \n",
    "                              capture_output=True, text=True)\n",
    "        conda_base = result.stdout.strip()\n",
    "        print(f\"   Conda base: {conda_base}\")\n",
    "        \n",
    "        # Common cache locations\n",
    "        cache_locations = [\n",
    "            os.path.join(conda_base, 'pkgs'),\n",
    "            os.path.join(conda_base, 'conda-meta'),\n",
    "            os.path.expanduser('~/.conda/pkgs'),\n",
    "            os.path.expanduser('~/anaconda3/pkgs'),\n",
    "            os.path.expanduser('~/miniconda3/pkgs'),\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n3Ô∏è‚É£ Cache locations:\")\n",
    "        for location in cache_locations:\n",
    "            if os.path.exists(location):\n",
    "                size = sum(f.stat().st_size for f in Path(location).rglob('*') if f.is_file())\n",
    "                size_mb = size / (1024**2)\n",
    "                print(f\"   üìÅ {location} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "    \n",
    "    # 3. Clear pip cache\n",
    "    print(\"\\n4Ô∏è‚É£ Clearing pip cache...\")\n",
    "    try:\n",
    "        subprocess.run(['pip', 'cache', 'purge'], capture_output=True)\n",
    "        print(\"‚úÖ Pip cache cleared\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ CLEANUP COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clear_conda_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25679fde-7903-4980-9e4f-6d421f9768c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cache file and re-run\n",
    "import os\n",
    "if os.path.exists('google_ip_ranges_cache.json'):\n",
    "    os.remove('google_ip_ranges_cache.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83689f42-9ecb-4180-a157-2b6c26a42a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: dnspython in d:\\anaconda\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries in Jupyter\n",
    "!pip install requests dnspython tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb7809-ff83-4d78-b274-baac81e588ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import gzip\n",
    "import socket\n",
    "import time\n",
    "import zipfile\n",
    "import ipaddress\n",
    "import requests\n",
    "import dns.resolver\n",
    "import dns.reversename\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MAX_DNS_WORKERS = 150\n",
    "DNS_TIMEOUT = 2\n",
    "IP_RANGES_CACHE_FILE = \"google_ip_ranges_cache.json\"\n",
    "CACHE_EXPIRY_HOURS = 24\n",
    "GOOGLE_DOMAINS = ['.googlebot.com', '.google.com', '.googleusercontent.com']\n",
    "MAX_RECORDS_PER_FILE = 500000\n",
    "\n",
    "# NEW: Lenient mode and FcrDNS fallback\n",
    "LENIENT_MODE = True  # Accept records with UA but missing/invalid IP\n",
    "ENABLE_FCRDNS_FALLBACK = True  # Verify non-CIDR IPs with FcrDNS\n",
    "\n",
    "GOOGLE_IP_RANGE_URLS = {\n",
    "    'googlebot': 'https://developers.google.com/static/search/apis/ipranges/googlebot.json',\n",
    "    'special_crawlers': 'https://developers.google.com/static/search/apis/ipranges/special-crawlers.json',\n",
    "    'user_triggered_fetchers': 'https://developers.google.com/static/search/apis/ipranges/user-triggered-fetchers.json',\n",
    "    'user_triggered_fetchers_google': 'https://developers.google.com/static/search/apis/ipranges/user-triggered-fetchers-google.json',\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# FETCH DYNAMIC IP RANGES\n",
    "# ============================================================================\n",
    "\n",
    "def fetch_google_ip_ranges():\n",
    "    \"\"\"Fetch Google's official IP ranges (IPv4 + IPv6) from JSON files\"\"\"\n",
    "    print(\"\\nüåê Fetching Google IP ranges from official sources...\")\n",
    "    \n",
    "    all_prefixes = []\n",
    "    \n",
    "    for source_name, url in GOOGLE_IP_RANGE_URLS.items():\n",
    "        try:\n",
    "            print(f\"   ‚Ä¢ Fetching {source_name}...\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if 'prefixes' in data:\n",
    "                for prefix in data['prefixes']:\n",
    "                    if 'ipv4Prefix' in prefix:\n",
    "                        all_prefixes.append(prefix['ipv4Prefix'])\n",
    "                    if 'ipv6Prefix' in prefix:\n",
    "                        all_prefixes.append(prefix['ipv6Prefix'])\n",
    "            \n",
    "            print(f\"     ‚úÖ Retrieved {len(data.get('prefixes', []))} prefixes\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ö†Ô∏è Warning: Failed to fetch {source_name}: {str(e)}\")\n",
    "    \n",
    "    ip_networks = []\n",
    "    ipv4_count = 0\n",
    "    ipv6_count = 0\n",
    "    \n",
    "    for prefix in all_prefixes:\n",
    "        try:\n",
    "            network = ipaddress.ip_network(prefix)\n",
    "            ip_networks.append(network)\n",
    "            \n",
    "            if network.version == 4:\n",
    "                ipv4_count += 1\n",
    "            else:\n",
    "                ipv6_count += 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total IP ranges loaded: {len(ip_networks)} CIDR blocks\")\n",
    "    print(f\"   ‚Ä¢ IPv4 ranges: {ipv4_count}\")\n",
    "    print(f\"   ‚Ä¢ IPv6 ranges: {ipv6_count}\")\n",
    "    return ip_networks\n",
    "\n",
    "def load_or_fetch_ip_ranges(cache_file=IP_RANGES_CACHE_FILE):\n",
    "    \"\"\"Load IP ranges from cache or fetch from Google if expired\"\"\"\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            with open(cache_file, 'r') as f:\n",
    "                cache_data = json.load(f)\n",
    "            \n",
    "            cache_time = cache_data.get('timestamp', 0)\n",
    "            cache_age_hours = (time.time() - cache_time) / 3600\n",
    "            \n",
    "            if cache_age_hours < CACHE_EXPIRY_HOURS:\n",
    "                print(f\"\\n‚úÖ Using cached IP ranges (age: {cache_age_hours:.1f} hours)\")\n",
    "                ip_networks = [ipaddress.ip_network(cidr) for cidr in cache_data['prefixes']]\n",
    "                \n",
    "                ipv4_count = sum(1 for n in ip_networks if n.version == 4)\n",
    "                ipv6_count = sum(1 for n in ip_networks if n.version == 6)\n",
    "                print(f\"   ‚Ä¢ IPv4 ranges: {ipv4_count}\")\n",
    "                print(f\"   ‚Ä¢ IPv6 ranges: {ipv6_count}\")\n",
    "                return ip_networks\n",
    "            else:\n",
    "                print(f\"\\n‚è∞ Cache expired, fetching fresh data...\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Cache error, fetching fresh data...\")\n",
    "    \n",
    "    ip_networks = fetch_google_ip_ranges()\n",
    "    \n",
    "    try:\n",
    "        cache_data = {\n",
    "            'timestamp': time.time(),\n",
    "            'prefixes': [str(network) for network in ip_networks]\n",
    "        }\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(cache_data, f, indent=2)\n",
    "        print(f\"üíæ IP ranges cached\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to save cache\")\n",
    "    \n",
    "    return ip_networks\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_full_timestamp_from_filename(filename):\n",
    "    \"\"\"Extract unique timestamp from filename\"\"\"\n",
    "    base_name = filename\n",
    "    if base_name.endswith('.gz'):\n",
    "        base_name = base_name[:-3]\n",
    "    if base_name.endswith('.log'):\n",
    "        base_name = base_name[:-4]\n",
    "    if base_name.startswith('nginx-'):\n",
    "        base_name = base_name[6:]\n",
    "    return base_name if base_name else None\n",
    "\n",
    "def extract_first_ip(ip_string):\n",
    "    \"\"\"Extract first IP (IPv4 or IPv6) from X-Forwarded-For field\"\"\"\n",
    "    if not ip_string:\n",
    "        return None\n",
    "    \n",
    "    ip = str(ip_string).split(',')[0].strip()\n",
    "    \n",
    "    if ip.startswith('[') and ip.endswith(']'):\n",
    "        ip = ip[1:-1]\n",
    "    \n",
    "    return ip if ip else None\n",
    "\n",
    "def is_googlebot(user_agent_str):\n",
    "    \"\"\"UA validation - Check if contains googlebot patterns\"\"\"\n",
    "    if not user_agent_str:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        ua_lower = str(user_agent_str).lower()\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    googlebot_patterns = [\n",
    "        'googlebot', 'google-inspectiontool', 'googlebot-image', 'googlebot-news',\n",
    "        'googlebot-video', 'adsbot-google', 'mediapartners-google', 'apis-google',\n",
    "        'google favicon', 'feedfetcher-google', 'google-read-aloud', 'duplichecker',\n",
    "        'google web preview', 'google-site-verification', 'google-smartphone'\n",
    "    ]\n",
    "    \n",
    "    for pattern in googlebot_patterns:\n",
    "        if pattern in ua_lower:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_search_api_url(url):\n",
    "    \"\"\"Check if URL contains search-api pattern\"\"\"\n",
    "    if not url:\n",
    "        return False\n",
    "    return '/search-api/v1/search/' in str(url).lower()\n",
    "\n",
    "def create_zip_archive(file_list, output_folder, archive_name=\"googlebot_data\"):\n",
    "    \"\"\"Create ZIP archive\"\"\"\n",
    "    zip_filename = os.path.join(output_folder, f\"{archive_name}_{time.strftime('%Y%m%d_%H%M%S')}.zip\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Creating ZIP archive...\")\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file_path in tqdm(file_list, desc=\"Archiving\", unit=\" files\"):\n",
    "            zipf.write(file_path, arcname=os.path.basename(file_path))\n",
    "    \n",
    "    zip_size_mb = os.path.getsize(zip_filename) / (1024**2)\n",
    "    print(f\"‚úÖ ZIP created: {os.path.basename(zip_filename)} ({zip_size_mb:.2f} MB)\")\n",
    "    return zip_filename\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION METHODS\n",
    "# ============================================================================\n",
    "\n",
    "def verify_ip_in_range(ip, ip_networks):\n",
    "    \"\"\"Check if IP is in Google's CIDR ranges\"\"\"\n",
    "    try:\n",
    "        ip_addr = ipaddress.ip_address(ip)\n",
    "        for network in ip_networks:\n",
    "            if ip_addr in network:\n",
    "                return True\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def verify_ip_method2_dns_fast(ip):\n",
    "    \"\"\"FcrDNS Verification - ULTRA FAST using dnspython\"\"\"\n",
    "    try:\n",
    "        rev_name = dns.reversename.from_address(ip)\n",
    "        \n",
    "        resolver = dns.resolver.Resolver()\n",
    "        resolver.timeout = DNS_TIMEOUT\n",
    "        resolver.lifetime = DNS_TIMEOUT\n",
    "        \n",
    "        try:\n",
    "            reverse_answers = resolver.resolve(rev_name, 'PTR')\n",
    "            hostname = str(reverse_answers[0]).rstrip('.')\n",
    "            hostname_lower = hostname.lower()\n",
    "        except (dns.resolver.NXDOMAIN, dns.resolver.NoAnswer, dns.resolver.Timeout, dns.exception.DNSException):\n",
    "            return False\n",
    "        \n",
    "        is_google_domain = any(hostname_lower.endswith(domain) for domain in GOOGLE_DOMAINS)\n",
    "        \n",
    "        if not is_google_domain:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            forward_ips = []\n",
    "            \n",
    "            try:\n",
    "                forward_answers = resolver.resolve(hostname, 'A')\n",
    "                forward_ips.extend([str(rdata) for rdata in forward_answers])\n",
    "            except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                forward_answers = resolver.resolve(hostname, 'AAAA')\n",
    "                forward_ips.extend([str(rdata) for rdata in forward_answers])\n",
    "            except (dns.resolver.NoAnswer, dns.resolver.NXDOMAIN):\n",
    "                pass\n",
    "            \n",
    "            if not forward_ips:\n",
    "                return False\n",
    "            \n",
    "            if ip in forward_ips:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        except (dns.resolver.Timeout, dns.exception.DNSException):\n",
    "            return False\n",
    "    \n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def verify_ips_parallel_dns(ips_to_verify, desc=\"DNS Verification\"):\n",
    "    \"\"\"Verify IPs using FcrDNS in parallel\"\"\"\n",
    "    verified_ips = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_DNS_WORKERS) as executor:\n",
    "        future_to_ip = {\n",
    "            executor.submit(verify_ip_method2_dns_fast, ip): ip \n",
    "            for ip in ips_to_verify\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=len(ips_to_verify), desc=desc, unit=\" IPs\") as pbar:\n",
    "            for future in as_completed(future_to_ip):\n",
    "                ip = future_to_ip[future]\n",
    "                try:\n",
    "                    is_verified = future.result(timeout=DNS_TIMEOUT + 1)\n",
    "                    verified_ips[ip] = is_verified\n",
    "                except Exception:\n",
    "                    verified_ips[ip] = False\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "    \n",
    "    return verified_ips\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PROCESSING - IN-MEMORY 3-PHASE (NO INTERMEDIATE FILES)\n",
    "# ============================================================================\n",
    "\n",
    "def process_log_files_final(input_folder=None, input_file=None, output_folder=\"googlebot_final_only\", \n",
    "                            max_records_per_file=500000, create_zip=True):\n",
    "    \"\"\"\n",
    "    IN-MEMORY 3-PHASE GOOGLEBOT EXTRACTION\n",
    "    Only outputs final Phase 3 files with search-API filtering\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéØ IN-MEMORY 3-PHASE GOOGLEBOT EXTRACTION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"   Lenient Mode: {'ENABLED' if LENIENT_MODE else 'DISABLED'}\")\n",
    "    print(f\"   FcrDNS Fallback: {'ENABLED' if ENABLE_FCRDNS_FALLBACK else 'DISABLED'}\")\n",
    "    print(f\"   Search-API Filter: ENABLED\")\n",
    "    \n",
    "    # Load IP ranges\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"LOADING GOOGLE IP RANGES\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    try:\n",
    "        google_ip_networks = load_or_fetch_ip_ranges()\n",
    "        print(f\"‚úÖ Ready with {len(google_ip_networks)} official IP ranges\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load IP ranges: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Get input files\n",
    "    log_files = []\n",
    "    if input_file:\n",
    "        if os.path.exists(input_file):\n",
    "            log_files = [input_file]\n",
    "        else:\n",
    "            print(f\"‚ùå File not found: {input_file}\")\n",
    "            return None\n",
    "    elif input_folder:\n",
    "        if os.path.exists(input_folder):\n",
    "            log_files = sorted([os.path.join(input_folder, f) for f in os.listdir(input_folder) \n",
    "                        if f.endswith('.log') or f.endswith('.log.gz')])\n",
    "            if not log_files:\n",
    "                print(f\"‚ùå No .log files found in: {input_folder}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"‚ùå Folder not found: {input_folder}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"‚ùå Please provide input_folder or input_file\")\n",
    "        return None\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    abs_output_folder = os.path.abspath(output_folder)\n",
    "    \n",
    "    print(f\"\\nüìÅ Input files: {len(log_files)}\")\n",
    "    \n",
    "    print(f\"\\nüîç IN-MEMORY 3-PHASE STRATEGY:\")\n",
    "    print(f\"   Phase 1: Filter by Googlebot UA ‚Üí Store in memory\")\n",
    "    print(f\"   Phase 2: Check IPs in CIDR ‚Üí Store in memory\")\n",
    "    print(f\"   Phase 3: FcrDNS verification ‚Üí Write final output (no intermediate files)\")\n",
    "    print(f\"   Phase 4: Search-API filter ‚Üí Final clean dataset\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 1: IN-MEMORY UA FILTERING\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 1: FILTER BY GOOGLEBOT UA (IN-MEMORY)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    googlebot_records = []  # Store all googlebot records in memory\n",
    "    unique_ips_phase1 = set()\n",
    "    googlebot_variants = {}\n",
    "    \n",
    "    stats_phase1 = {\n",
    "        'total_processed': 0,\n",
    "        'has_googlebot_ua': 0,\n",
    "    }\n",
    "    \n",
    "    start_phase1 = time.time()\n",
    "    \n",
    "    for file_idx, log_file in enumerate(log_files, 1):\n",
    "        filename = os.path.basename(log_file)\n",
    "        file_size_mb = os.path.getsize(log_file) / (1024**2)\n",
    "        print(f\"\\nüìÇ [{file_idx}/{len(log_files)}] {filename} ({file_size_mb:.1f} MB)\")\n",
    "        \n",
    "        file_accepted = 0\n",
    "        \n",
    "        try:\n",
    "            if log_file.endswith('.gz'):\n",
    "                file_handle = gzip.open(log_file, 'rt', encoding='utf-8', errors='ignore')\n",
    "            else:\n",
    "                file_handle = open(log_file, 'r', encoding='utf-8', errors='ignore')\n",
    "            \n",
    "            with file_handle as log_reader:\n",
    "                pbar = tqdm(log_reader, desc=\"   Filtering UA\", unit=\" recs\", ncols=100, mininterval=0.5)\n",
    "                \n",
    "                for line in pbar:\n",
    "                    try:\n",
    "                        if not line.strip():\n",
    "                            continue\n",
    "                        \n",
    "                        log_entry = json.loads(line.strip())\n",
    "                        \n",
    "                        stats_phase1['total_processed'] += 1\n",
    "                        \n",
    "                        user_agent = log_entry.get('http_user_agent', '')\n",
    "                        if user_agent:\n",
    "                            user_agent = str(user_agent).strip()\n",
    "                        \n",
    "                        if not is_googlebot(user_agent):\n",
    "                            continue\n",
    "                        \n",
    "                        stats_phase1['has_googlebot_ua'] += 1\n",
    "                        \n",
    "                        # Track variant\n",
    "                        ua_lower = user_agent.lower()\n",
    "                        for variant in ['googlebot-image', 'googlebot-news', 'googlebot-video', \n",
    "                                       'google-inspectiontool', 'adsbot-google', 'mediapartners-google']:\n",
    "                            if variant in ua_lower:\n",
    "                                key = variant.replace('-', ' ').title().replace(' ', '-')\n",
    "                                googlebot_variants[key] = googlebot_variants.get(key, 0) + 1\n",
    "                                break\n",
    "                        else:\n",
    "                            if 'googlebot' in ua_lower:\n",
    "                                googlebot_variants['Googlebot (standard)'] = googlebot_variants.get('Googlebot (standard)', 0) + 1\n",
    "                        \n",
    "                        # Extract all required fields\n",
    "                        ip = extract_first_ip(log_entry.get('http_x_forwarded_for', ''))\n",
    "                        if ip:\n",
    "                            unique_ips_phase1.add(ip)\n",
    "                        \n",
    "                        # Store record with all fields\n",
    "                        record = {\n",
    "                            'time_iso8601': str(log_entry.get('time_iso8601', '')).strip(),\n",
    "                            'request_uri': str(log_entry.get('request_uri', '')).strip(),\n",
    "                            'status': str(log_entry.get('status', '')).strip(),\n",
    "                            'http_user_agent': user_agent,\n",
    "                            'http_x_forwarded_for': ip if ip else '',\n",
    "                            'geoip_country_code': str(log_entry.get('geoip_country_code', '')).strip(),\n",
    "                            'upstream_response_time': str(log_entry.get('upstream_response_time', '')).strip(),\n",
    "                            'bytes_sent': str(log_entry.get('bytes_sent', log_entry.get('body_bytes_sent', ''))).strip(),\n",
    "                            'source_file': filename\n",
    "                        }\n",
    "                        \n",
    "                        googlebot_records.append(record)\n",
    "                        file_accepted += 1\n",
    "                    \n",
    "                    except Exception:\n",
    "                        continue\n",
    "                \n",
    "                pbar.close()\n",
    "            \n",
    "            print(f\"   ‚úÖ Accepted: {file_accepted:,} records with Googlebot UA\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    elapsed_phase1 = time.time() - start_phase1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 1 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Total processed: {stats_phase1['total_processed']:,}\")\n",
    "    print(f\"   Googlebot UA filtered: {len(googlebot_records):,}\")\n",
    "    print(f\"   Unique IPs: {len(unique_ips_phase1):,}\")\n",
    "    print(f\"   Time: {elapsed_phase1:.1f}s ({elapsed_phase1/60:.1f} min)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 2: CIDR CHECK (IN-MEMORY)\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 2: CHECK IPS AGAINST CIDR RANGES (IN-MEMORY)\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\nüîç Checking {len(unique_ips_phase1):,} IPs against CIDR ranges...\")\n",
    "    verified_ips_cidr = {}\n",
    "    for ip in tqdm(unique_ips_phase1, desc=\"Checking IPs\", unit=\" IPs\"):\n",
    "        verified_ips_cidr[ip] = verify_ip_in_range(ip, google_ip_networks)\n",
    "    \n",
    "    ips_in_range = sum(1 for v in verified_ips_cidr.values() if v)\n",
    "    unique_ips_outside_cidr = {ip for ip, in_range in verified_ips_cidr.items() if not in_range}\n",
    "    \n",
    "    print(f\"   ‚úÖ IPs in CIDR ranges: {ips_in_range:,}/{len(unique_ips_phase1):,}\")\n",
    "    print(f\"   ‚ö†Ô∏è IPs outside ranges: {len(unique_ips_outside_cidr):,} (will check with FcrDNS in Phase 3)\")\n",
    "    \n",
    "    elapsed_phase2 = time.time() - start_phase1 - elapsed_phase1\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 2 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Time: {elapsed_phase2:.1f}s ({elapsed_phase2/60:.1f} min)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # PHASE 3: FcrDNS + WRITE FINAL OUTPUT\n",
    "    # ========================================================================\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"PHASE 3: FcrDNS VERIFICATION + WRITE FINAL OUTPUT\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    dns_verified_ips = {}\n",
    "    \n",
    "    if ENABLE_FCRDNS_FALLBACK and len(unique_ips_outside_cidr) > 0:\n",
    "        print(f\"\\nüåê Starting FcrDNS verification for {len(unique_ips_outside_cidr):,} IPs outside CIDR ranges...\")\n",
    "        print(f\"   Using {MAX_DNS_WORKERS} workers\")\n",
    "        \n",
    "        start_dns = time.time()\n",
    "        \n",
    "        dns_verified_ips = verify_ips_parallel_dns(unique_ips_outside_cidr, desc=\"FcrDNS Verification\")\n",
    "        \n",
    "        dns_pass = sum(1 for v in dns_verified_ips.values() if v)\n",
    "        elapsed_dns = time.time() - start_dns\n",
    "        \n",
    "        print(f\"\\n‚úÖ FcrDNS Verification Results:\")\n",
    "        print(f\"   Passed: {dns_pass:,}/{len(unique_ips_outside_cidr):,}\")\n",
    "        print(f\"   Time: {elapsed_dns:.1f}s ({elapsed_dns/60:.1f} min)\")\n",
    "        if len(unique_ips_outside_cidr) > 0:\n",
    "            print(f\"   Speed: {len(unique_ips_outside_cidr)/elapsed_dns:.0f} IPs/sec\")\n",
    "    \n",
    "    print(f\"\\nüìù Writing final verified records (with search-API filtering)...\")\n",
    "    \n",
    "    phase3_files = []\n",
    "    total_phase3_before_filter = 0\n",
    "    total_phase3_after_filter = 0\n",
    "    search_api_filtered = 0\n",
    "    \n",
    "    rejection_stats = {\n",
    "        'in_cidr': 0,\n",
    "        'missing_ip_accepted': 0,\n",
    "        'invalid_ip_accepted': 0,\n",
    "        'fcrdns_passed': 0,\n",
    "        'fcrdns_failed_rejected': 0\n",
    "    }\n",
    "    \n",
    "    # Group records by source file\n",
    "    records_by_file = {}\n",
    "    for record in googlebot_records:\n",
    "        source_file = record['source_file']\n",
    "        if source_file not in records_by_file:\n",
    "            records_by_file[source_file] = []\n",
    "        records_by_file[source_file].append(record)\n",
    "    \n",
    "    start_write = time.time()\n",
    "    \n",
    "    for source_file, records in tqdm(sorted(records_by_file.items()), desc=\"Writing files\", unit=\" files\"):\n",
    "        file_timestamp = extract_full_timestamp_from_filename(source_file)\n",
    "        \n",
    "        if file_timestamp:\n",
    "            base_filename = f\"googlebot_{file_timestamp}\"\n",
    "        else:\n",
    "            base_filename = f\"googlebot_file\"\n",
    "        \n",
    "        current_file_index = 1\n",
    "        current_file_records = 0\n",
    "        current_writer = None\n",
    "        current_csvfile = None\n",
    "        \n",
    "        def create_output_file(file_index):\n",
    "            if file_index == 1:\n",
    "                csv_filename = os.path.join(output_folder, f\"{base_filename}.csv\")\n",
    "            else:\n",
    "                csv_filename = os.path.join(output_folder, f\"{base_filename}_part{file_index}.csv\")\n",
    "            \n",
    "            csvfile = open(csv_filename, 'w', newline='', encoding='utf-8-sig')\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                'time_iso8601', 'request_uri', 'status', 'http_user_agent',\n",
    "                'http_x_forwarded_for', 'geoip_country_code', \n",
    "                'upstream_response_time', 'bytes_sent'\n",
    "            ], quoting=csv.QUOTE_ALL)\n",
    "            writer.writeheader()\n",
    "            phase3_files.append(csv_filename)\n",
    "            return csvfile, writer\n",
    "        \n",
    "        current_csvfile, current_writer = create_output_file(current_file_index)\n",
    "        \n",
    "        for record in records:\n",
    "            ip = record['http_x_forwarded_for'].strip()\n",
    "            \n",
    "            # Determine if record should be accepted\n",
    "            accept_record = False\n",
    "            \n",
    "            if not ip or ip == '' or ip.lower() in ['none', 'null', '-', 'unknown']:\n",
    "                if LENIENT_MODE:\n",
    "                    accept_record = True\n",
    "                    rejection_stats['missing_ip_accepted'] += 1\n",
    "            elif ip in verified_ips_cidr:\n",
    "                if verified_ips_cidr[ip]:\n",
    "                    accept_record = True\n",
    "                    rejection_stats['in_cidr'] += 1\n",
    "                else:\n",
    "                    if dns_verified_ips.get(ip, False):\n",
    "                        accept_record = True\n",
    "                        rejection_stats['fcrdns_passed'] += 1\n",
    "                    else:\n",
    "                        rejection_stats['fcrdns_failed_rejected'] += 1\n",
    "            else:\n",
    "                if LENIENT_MODE:\n",
    "                    accept_record = True\n",
    "                    rejection_stats['invalid_ip_accepted'] += 1\n",
    "            \n",
    "            if accept_record:\n",
    "                total_phase3_before_filter += 1\n",
    "                \n",
    "                # Check for search-API URL\n",
    "                if is_search_api_url(record['request_uri']):\n",
    "                    search_api_filtered += 1\n",
    "                    continue  # Skip this record\n",
    "                \n",
    "                total_phase3_after_filter += 1\n",
    "                \n",
    "                if current_file_records >= max_records_per_file:\n",
    "                    current_csvfile.close()\n",
    "                    current_file_index += 1\n",
    "                    current_csvfile, current_writer = create_output_file(current_file_index)\n",
    "                    current_file_records = 0\n",
    "                \n",
    "                current_writer.writerow({\n",
    "                    'time_iso8601': record['time_iso8601'],\n",
    "                    'request_uri': record['request_uri'],\n",
    "                    'status': record['status'],\n",
    "                    'http_user_agent': record['http_user_agent'],\n",
    "                    'http_x_forwarded_for': record['http_x_forwarded_for'],\n",
    "                    'geoip_country_code': record['geoip_country_code'],\n",
    "                    'upstream_response_time': record['upstream_response_time'],\n",
    "                    'bytes_sent': record['bytes_sent']\n",
    "                })\n",
    "                current_file_records += 1\n",
    "        \n",
    "        if current_csvfile and not current_csvfile.closed:\n",
    "            current_csvfile.close()\n",
    "    \n",
    "    elapsed_write = time.time() - start_write\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ PHASE 3 COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"   Before search-API filter: {total_phase3_before_filter:,}\")\n",
    "    print(f\"   Search-API URLs filtered: {search_api_filtered:,}\")\n",
    "    print(f\"   After search-API filter: {total_phase3_after_filter:,}\")\n",
    "    print(f\"   Breakdown:\")\n",
    "    print(f\"   - CIDR verified: {rejection_stats['in_cidr']:,}\")\n",
    "    print(f\"   - Missing IP (lenient): {rejection_stats['missing_ip_accepted']:,}\")\n",
    "    print(f\"   - Invalid IP (lenient): {rejection_stats['invalid_ip_accepted']:,}\")\n",
    "    print(f\"   - FcrDNS passed: {rejection_stats['fcrdns_passed']:,}\")\n",
    "    print(f\"   - FcrDNS failed (rejected): {rejection_stats['fcrdns_failed_rejected']:,}\")\n",
    "    print(f\"   Output files: {len(phase3_files)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # SUMMARY\n",
    "    # ========================================================================\n",
    "    total_time = time.time() - start_phase1\n",
    "    \n",
    "    gsc_expected = 14913024\n",
    "    match_rate = (total_phase3_after_filter / gsc_expected * 100) if total_phase3_after_filter > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"‚úÖ COMPLETE: IN-MEMORY 3-PHASE EXTRACTION\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  PERFORMANCE:\")\n",
    "    print(f\"   Phase 1 (UA Filter): {elapsed_phase1:.1f}s ({elapsed_phase1/60:.1f} min)\")\n",
    "    print(f\"   Phase 2 (CIDR Check): {elapsed_phase2:.1f}s ({elapsed_phase2/60:.1f} min)\")\n",
    "    print(f\"   Phase 3 (FcrDNS + Write): {elapsed_write:.1f}s ({elapsed_write/60:.1f} min)\")\n",
    "    print(f\"   TOTAL: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    \n",
    "    print(f\"\\nüìä RESULTS:\")\n",
    "    print(f\"   Phase 1: {len(googlebot_records):,} (Googlebot UA)\")\n",
    "    print(f\"   Phase 2: {len(unique_ips_phase1):,} unique IPs checked\")\n",
    "    print(f\"   Phase 3 (before search-API): {total_phase3_before_filter:,}\")\n",
    "    print(f\"   Phase 3 (after search-API): {total_phase3_after_filter:,} ‚úÖ\")\n",
    "    \n",
    "    print(f\"\\nü§ñ GOOGLEBOT VARIANTS:\")\n",
    "    for variant, count in sorted(googlebot_variants.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"   ‚Ä¢ {variant}: {count:,}\")\n",
    "    \n",
    "    print(f\"\\nüéØ GSC COMPARISON:\")\n",
    "    print(f\"   GSC: {gsc_expected:,}\")\n",
    "    print(f\"   Output: {total_phase3_after_filter:,}\")\n",
    "    print(f\"   Difference: {abs(gsc_expected - total_phase3_after_filter):,}\")\n",
    "    print(f\"   Match: {match_rate:.1f}%\")\n",
    "    \n",
    "    if match_rate >= 98:\n",
    "        print(f\"   üèÜ NEAR-PERFECT!\")\n",
    "    elif match_rate >= 95:\n",
    "        print(f\"   ‚úÖ EXCELLENT MATCH!\")\n",
    "    elif match_rate >= 90:\n",
    "        print(f\"   ‚úÖ VERY GOOD!\")\n",
    "    \n",
    "    # Create ZIP\n",
    "    if create_zip:\n",
    "        zip_filename = create_zip_archive(phase3_files, output_folder, \"googlebot_final\")\n",
    "    \n",
    "    return {\n",
    "        'output_folder': abs_output_folder,\n",
    "        'phase3_files': phase3_files,\n",
    "        'final_records': total_phase3_after_filter,\n",
    "        'search_api_filtered': search_api_filtered,\n",
    "        'match_rate': match_rate,\n",
    "        'rejection_stats': rejection_stats\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ IN-MEMORY 3-PHASE GOOGLEBOT EXTRACTION\")\n",
    "print(\"   Phase 1-2: In-Memory | Phase 3: Final Output + Search-API Filter\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "INPUT_FOLDER = input(\"\\nüìÇ Enter folder path (or empty for file): \").strip()\n",
    "\n",
    "if not INPUT_FOLDER:\n",
    "    INPUT_FILE = input(\"üìÑ Enter file path: \").strip()\n",
    "    results = process_log_files_final(\n",
    "        input_file=INPUT_FILE,\n",
    "        output_folder=\"googlebot_final_only\",\n",
    "        max_records_per_file=500000,\n",
    "        create_zip=True\n",
    "    )\n",
    "else:\n",
    "    results = process_log_files_final(\n",
    "        input_folder=INPUT_FOLDER,\n",
    "        output_folder=\"googlebot_final_only\",\n",
    "        max_records_per_file=500000,\n",
    "        create_zip=True\n",
    "    )\n",
    "\n",
    "if results:\n",
    "    print(f\"\\nüéâ DONE!\")\n",
    "    print(f\"üìÇ {results['output_folder']}\")\n",
    "    print(f\"üìã Final: {results['final_records']:,} records\")\n",
    "    print(f\"üö´ Search-API filtered: {results['search_api_filtered']:,} records\")\n",
    "    #print(f\"üéØ Match: {results['match_rate']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b495e6-a38f-48f4-9f49-7eb245753004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
