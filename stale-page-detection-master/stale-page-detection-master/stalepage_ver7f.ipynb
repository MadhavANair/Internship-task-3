{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8e5d3d-fea8-4a40-a7e5-0e3c8ba4aec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6a5f41adf948fbb430aeaff23ec05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>üï∫ Stale Page Detection: Party Bouncer (Updated)</h2>'), Text(value='', descript‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import os\n",
    "import concurrent.futures\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import csv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Worker function (Must be outside)\n",
    "def parse_gz_sitemap_worker(gz_url):\n",
    "    try:\n",
    "        response = requests.get(gz_url, timeout=60, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        with gzip.GzipFile(fileobj=BytesIO(response.content)) as gz:\n",
    "            xml_content = gz.read()\n",
    "        root = ET.fromstring(xml_content)\n",
    "        ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        urls_data = []\n",
    "        for url_elem in root.findall('sm:url', ns):\n",
    "            loc = url_elem.find('sm:loc', ns)\n",
    "            lastmod = url_elem.find('sm:lastmod', ns)\n",
    "            if loc is not None:\n",
    "                urls_data.append({\n",
    "                    'url': loc.text,\n",
    "                    'last_modified': lastmod.text if lastmod is not None else None\n",
    "                })\n",
    "        return urls_data\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def detect_stale_pages_bouncer(log_folder_path, sitemap_urls_text, output_dir='stale_pages_output', \n",
    "                               max_records_per_file=500000, max_gz_per_index=None,\n",
    "                               log_batch_size=5, workers=10):\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    sitemap_urls = [url.strip() for url in sitemap_urls_text.replace(',', '\\n').split('\\n') if url.strip()]\n",
    "    \n",
    "    if not sitemap_urls:\n",
    "        print(\"‚ùå No sitemap URLs provided\")\n",
    "        return None\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üï∫ STALE PAGE DETECTION: THE PARTY BOUNCER (Parallel: {workers} workers)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìÅ Log folder: {log_folder_path}\")\n",
    "    print(f\"üó∫Ô∏è  Sitemap indexes: {len(sitemap_urls)}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # STEP 1: BUILD THE GUEST LIST (LOGS)\n",
    "    # ============================================================================\n",
    "    print(\"\\nüìñ Step 1: Building the Guest List (Processing Logs)...\")\n",
    "    \n",
    "    if not os.path.exists(log_folder_path):\n",
    "        print(f\"‚ùå Folder not found: {log_folder_path}\")\n",
    "        return None\n",
    "\n",
    "    log_files = [f for f in os.listdir(log_folder_path) if f.endswith('.csv')]\n",
    "    \n",
    "    if not log_files:\n",
    "        print(\"‚ùå No CSV files found.\")\n",
    "        return None\n",
    "\n",
    "    all_log_stats = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(log_files), log_batch_size), desc=\"   Processing Logs\"):\n",
    "        batch_files = log_files[i:i+log_batch_size]\n",
    "        dfs = []\n",
    "        for file in batch_files:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(log_folder_path, file), \n",
    "                               usecols=['request_uri', 'http_user_agent', 'time_iso8601', 'status'], \n",
    "                               encoding='utf-8-sig', low_memory=False)\n",
    "                dfs.append(df)\n",
    "            except: continue\n",
    "            \n",
    "        if dfs:\n",
    "            batch_df = pd.concat(dfs)\n",
    "            batch_df['url'] = 'https://www.alamy.com' + batch_df['request_uri'].str.split('?').str[0].fillna('')\n",
    "            batch_df['timestamp'] = pd.to_datetime(batch_df['time_iso8601'], errors='coerce')\n",
    "            \n",
    "            min_date = batch_df['timestamp'].min()\n",
    "            max_date = batch_df['timestamp'].max()\n",
    "            days = (max_date - min_date).days + 1 if pd.notnull(max_date) else 1\n",
    "            \n",
    "            batch_stats = batch_df.groupby('url').agg({\n",
    "                'request_uri': 'count',\n",
    "                'http_user_agent': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "                'status': lambda x: x.mode()[0] if len(x.mode()) > 0 else 200\n",
    "            }).reset_index()\n",
    "            batch_stats.columns = ['url', 'crawl_count', 'user_agent', 'status_code']\n",
    "            batch_stats['days_active'] = days\n",
    "            \n",
    "            all_log_stats.append(batch_stats)\n",
    "            del batch_df, dfs\n",
    "            gc.collect()\n",
    "\n",
    "    if not all_log_stats:\n",
    "        print(\"‚ùå No log data processed.\")\n",
    "        return None\n",
    "\n",
    "    print(\"   ‚îú‚îÄ Finalizing Guest List...\")\n",
    "    full_log_stats = pd.concat(all_log_stats).groupby('url').agg({\n",
    "        'crawl_count': 'sum',\n",
    "        'user_agent': 'first',\n",
    "        'status_code': 'first',\n",
    "        'days_active': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    full_log_stats['crawl_frequency'] = full_log_stats['crawl_count'] / full_log_stats['days_active']\n",
    "    full_log_stats['authority_score'] = (full_log_stats['crawl_count'] * 0.7 + \n",
    "                                         full_log_stats['crawl_frequency'] * full_log_stats['days_active'] * 0.3)\n",
    "    \n",
    "    threshold = full_log_stats['crawl_count'].quantile(0.10)\n",
    "    print(f\"   ‚îú‚îÄ Bottom 10% Threshold: <= {threshold} crawls\")\n",
    "    \n",
    "    invited_guests = set(full_log_stats['url'])\n",
    "    wallflowers_df = full_log_stats[full_log_stats['crawl_count'] <= threshold]\n",
    "    wallflowers = set(wallflowers_df['url'])\n",
    "    \n",
    "    print(f\"‚úÖ Guest List Ready: {len(invited_guests):,} total, {len(wallflowers):,} low activity\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # STEP 2: AT THE GATE (SITEMAP PARSING)\n",
    "    # ============================================================================\n",
    "    print(\"\\nüì• Step 2: Checking Sitemaps at the Gate (Parallel)...\")\n",
    "    \n",
    "    def parse_sitemap_index(index_url):\n",
    "        try:\n",
    "            response = requests.get(index_url, timeout=60)\n",
    "            root = ET.fromstring(response.content)\n",
    "            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "            return [loc.text for loc in root.findall('sm:sitemap/sm:loc', ns)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    stale_party_list = []\n",
    "    total_processed_urls = 0\n",
    "    \n",
    "    def check_at_gate(batch_df):\n",
    "        batch_df['is_invited'] = batch_df['url'].isin(invited_guests)\n",
    "        batch_df['is_wallflower'] = batch_df['url'].isin(wallflowers)\n",
    "        \n",
    "        orphans = batch_df[~batch_df['is_invited']].copy()\n",
    "        orphans['page_type'] = 'Orphan'\n",
    "        orphans['crawl_count'] = 0\n",
    "        orphans['authority_score'] = 0.0\n",
    "        orphans['crawl_frequency'] = 0.0\n",
    "        orphans['user_agent'] = 'Not Crawled'\n",
    "        orphans['status_code'] = 0  # NEW: Orphans = never crawled\n",
    "        \n",
    "        low_act = batch_df[batch_df['is_wallflower']].copy()\n",
    "        low_act['page_type'] = 'Low Activity'\n",
    "        \n",
    "        if not low_act.empty:\n",
    "            low_act = low_act.merge(\n",
    "                full_log_stats[['url', 'crawl_count', 'authority_score', 'crawl_frequency', 'user_agent', 'status_code']], \n",
    "                on='url', how='left'\n",
    "            )\n",
    "            low_act['status_code'] = low_act['status_code'].fillna(200)  # Default 200 for crawled pages\n",
    "        \n",
    "        return pd.concat([orphans, low_act])\n",
    "\n",
    "    for idx_num, index_url in enumerate(sitemap_urls, 1):\n",
    "        print(f\"\\n   ‚îú‚îÄ Index {idx_num}/{len(sitemap_urls)}: {index_url.split('/')[-1]}\")\n",
    "        gz_urls = parse_sitemap_index(index_url)\n",
    "        if not gz_urls: continue\n",
    "        if max_gz_per_index: gz_urls = gz_urls[:max_gz_per_index]\n",
    "            \n",
    "        print(f\"   ‚îÇ  ‚îú‚îÄ Spawning {workers} workers for {len(gz_urls)} files...\")\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "            futures = {executor.submit(parse_gz_sitemap_worker, url): url for url in gz_urls}\n",
    "            batch_results = []\n",
    "            \n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(gz_urls), desc=\"   ‚îÇ  ‚îî‚îÄ Parsing\", leave=False):\n",
    "                result = future.result()\n",
    "                if result: batch_results.extend(result)\n",
    "                \n",
    "                if len(batch_results) > 200000:\n",
    "                    batch_df = pd.DataFrame(batch_results)\n",
    "                    total_processed_urls += len(batch_df)\n",
    "                    processed_batch = check_at_gate(batch_df)\n",
    "                    if not processed_batch.empty:\n",
    "                        stale_party_list.append(processed_batch)\n",
    "                    batch_results = []\n",
    "                    del batch_df\n",
    "                    gc.collect()\n",
    "            \n",
    "            if batch_results:\n",
    "                batch_df = pd.DataFrame(batch_results)\n",
    "                total_processed_urls += len(batch_df)\n",
    "                processed_batch = check_at_gate(batch_df)\n",
    "                if not processed_batch.empty:\n",
    "                    stale_party_list.append(processed_batch)\n",
    "                del batch_df\n",
    "                gc.collect()\n",
    "\n",
    "    # ============================================================================\n",
    "    # STEP 3: SAVE OUTPUT (CSV with Proper Quoting)\n",
    "    # ============================================================================\n",
    "    print(\"\\nüíæ Step 3: Consolidating and Saving...\")\n",
    "    \n",
    "    if not stale_party_list:\n",
    "        print(\"‚ùå No stale pages found.\")\n",
    "        return None\n",
    "        \n",
    "    final_df = pd.concat(stale_party_list, ignore_index=True)\n",
    "    \n",
    "    current_date = pd.Timestamp.now()\n",
    "    final_df['last_modified'] = pd.to_datetime(final_df['last_modified'], errors='coerce')\n",
    "    final_df['days_since_modified'] = (current_date - final_df['last_modified']).dt.days\n",
    "    \n",
    "    final_df['priority_score'] = 0\n",
    "    final_df.loc[final_df['days_since_modified'] > 180, 'priority_score'] = 100\n",
    "    final_df.loc[(final_df['days_since_modified'] > 90) & (final_df['days_since_modified'] <= 180), 'priority_score'] = 70\n",
    "    final_df.loc[final_df['days_since_modified'] <= 90, 'priority_score'] = 40\n",
    "    \n",
    "    final_df = final_df.sort_values(['page_type', 'priority_score'], ascending=[False, False])\n",
    "    \n",
    "    # UPDATED: Added status_code, renamed status to page_type\n",
    "    output_cols = ['url', 'crawl_count', 'authority_score', 'crawl_frequency', 'user_agent', \n",
    "                   'status_code', 'page_type', 'priority_score', 'days_since_modified', 'last_modified']\n",
    "    \n",
    "    final_cols = [c for c in output_cols if c in final_df.columns]\n",
    "    final_df = final_df[final_cols]\n",
    "    \n",
    "    if len(final_df) <= max_records_per_file:\n",
    "        output_path = os.path.join(output_dir, 'stale_pages.csv')\n",
    "        final_df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "        print(f\"   ‚îî‚îÄ Saved: {output_path}\")\n",
    "    else:\n",
    "        num_parts = (len(final_df) // max_records_per_file) + 1\n",
    "        for i in range(num_parts):\n",
    "            start_idx = i * max_records_per_file\n",
    "            end_idx = min((i + 1) * max_records_per_file, len(final_df))\n",
    "            part_df = final_df.iloc[start_idx:end_idx]\n",
    "            output_path = os.path.join(output_dir, f'stale_pages_part{i+1}.csv')\n",
    "            part_df.to_csv(output_path, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_ALL)\n",
    "        print(f\"   ‚îî‚îÄ Saved {num_parts} files\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä FINAL STATS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ Total Analyzed: {total_processed_urls:,}\")\n",
    "    print(f\"‚úÖ Stale Found: {len(final_df):,}\")\n",
    "    print(f\"   ‚Ä¢ Orphans: {len(final_df[final_df['page_type']=='Orphan']):,}\")\n",
    "    print(f\"   ‚Ä¢ Low Activity: {len(final_df[final_df['page_type']=='Low Activity']):,}\")\n",
    "    print(f\"üìÅ Output Location: {os.path.abspath(output_dir)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "# UI Widgets\n",
    "log_folder_input = widgets.Text(value='', placeholder='D:\\\\path\\\\to\\\\log\\\\files', description='Log Folder:', layout=widgets.Layout(width='600px'))\n",
    "sitemap_input = widgets.Textarea(value='https://www.alamy.com/sitemaps/image_daily_index_s_1_10000000.xml', placeholder='Enter sitemap URLs', description='Sitemap URLs:', layout=widgets.Layout(width='600px', height='150px'))\n",
    "test_mode_checkbox = widgets.Checkbox(value=True, description='Test Mode')\n",
    "log_batch_input = widgets.IntText(value=5, description='Log Batch:')\n",
    "gz_batch_input = widgets.IntText(value=20, description='Workers:')\n",
    "run_button = widgets.Button(description='üöÄ Run Party Bouncer', button_style='success', icon='check', layout=widgets.Layout(width='300px'))\n",
    "output_area = widgets.Output()\n",
    "\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        max_gz = 10 if test_mode_checkbox.value else None\n",
    "        detect_stale_pages_bouncer(\n",
    "            log_folder_input.value, \n",
    "            sitemap_input.value, \n",
    "            max_gz_per_index=max_gz, \n",
    "            log_batch_size=log_batch_input.value, \n",
    "            workers=gz_batch_input.value\n",
    "        )\n",
    "\n",
    "\n",
    "run_button.on_click(on_run_clicked)\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>üï∫ Stale Page Detection: Party Bouncer (Updated)</h2>\"), \n",
    "    log_folder_input, sitemap_input, \n",
    "    widgets.HBox([log_batch_input, gz_batch_input]), \n",
    "    test_mode_checkbox, run_button, output_area\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dece1a3-30b0-4a5b-b6c8-007d8aaabe7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ecc47-eb82-449e-a549-124bcf981da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
